 you you you you you Great Okay, so Hello, I'm Selan I'm studying computer engineering in Polytron So what is Isaac Isaac is an association that is global independent nonpartisan non-profit and youth run What is our purpose we are striving for a better future and our values are youth development people's potential and practical experiences We have four types of programs which are global talents global teacher global volunteer and Isaac member We are in Isaac member So we are in over 120 countries and territories we have over 5,000 party organizations and over 30,000 experiences every year So one of our programs global talent now today we're going to present you global talent First of all why global talent is Useful for your international network it will boost your career and you're gonna have a automation and you're gonna have our support Now imagine you're working in HR and you have ten candidates Which are in same age same course same score and same knowledge and there's one candidate with an international experience If you go to HR, who would you hire? I think the answer is obvious So Now my colleague is gonna present you some opportunities So guys as you know or maybe you don't know Isaac Have many offers in different countries in different fields but for now we have three countries and We have two kinds of opportunities Short-term opportunities and me mid-line term opportunities the short-term opportunities are something between eight to twelve weeks the cost is 420 euros plus five euros for membership for the short-term opportunities Accommodation is provided and paid so you don't need to be worried about your Modation and also about the mid-line term opportunities. It is something between 13 to 78 weeks the cost is 460 euros to five euros for membership and Also, you will get salary And Now we start with our opportunities After this is might be deal a scam barcode So you can sign up in the site and also you can see the opportunities and you can check the details about it But for now we have three opportunities in Romania Portugal and Netherlands for all of them. Of course, you need Knowledge about computer. Of course, you need to know some programming language like Python HTML5 and JavaScript But it's the programming language is different case by case and You can check it when you stand the barcode So we have opportunity in Romania Which is unpaid. It's a kind of short-term opportunity it starts from 5th of june and Accommodation is provided and paid and second one is in Portable is again Untaid but the accommodation is provided and if you're if you don't have computer or your computer is old You don't need to be worried because the computer is also provided And also one meal per day also paid, but not provided And then last opportunity is in Netherlands. You will get paid for this opportunity And the accommodation is not paid and provided, but you will have computer And that's it. That's our opportunities and now you can Expand this QR code. You can sign up in our site and see our opportunities Thank you Thank you so much professor. Yes. Do you have any questions for them? Oh So you will have access to the recording. I don't know whether I could put the the slides on the teaching material ISEC is an official students association of polytechnic. It's a Reno They contact me asking me for a short slot to present the association and some some opportunities and I agreed But there is no directly link between Me or professor mantelero vachago and the association Okay, so if there are no questions Thanks again, and we will now resume a lecture You can send me and then I will check whether it is appropriate or not Just send me and then I will check about the progress So Okay, hope you can hear me. Yes very good Just a few seconds to finish the setup You You Okay, so we will assume from last point that we We touched a couple of weeks ago As usual, I remind you that I use the virtual classroom to record the lecture, but I am not going to interact remotely Okay Last time we had a very long introduction to the course And to the premises of why Why there is a a course on that ethics at least part of the course is devoted to that ethics we We reflect on the importance of data for science for commerce of course for the economy for Policy making and for decision making which is our our our focus and we started to To give some definitions of ethics. First of all, what are the common problems? What what is the definition of artificial intelligence that we take into account and the definition of Data-driven automated decision making systems Today we will we will make a step forward and we we start Seeing together some practical issues That belong that stay under the wide umbrella or data issues uh, first Of all, let me recall the last thing that we identified the Data-driven a DM systems or also called predictive optimizations We also use these these This term that I introduced to you They are all based these systems are based on patterns recognized from From available data and the particularity is that The actions the predictions the classifications are all actions are all decisions That can be automatic or semi-automatic that they have an effect on on people being These the epistemic approach of these kind of systems that are inductive systems. They also inherit some Of course some opportunities from From inductive systems. Basically, you can have Portion of data you can infer on phenomena that are larger on data that is also be Beyond the subset of data that you use for the training that is the the most important Advantage of using this approach however, there are also some advantages some the most Notable are summarized in this slide and we will comment them shortly together So the first one is What is called the survival bias of low-hanging fruit that is you measure only what you can measure Reality is is is much much bigger than What is measurable and so this is a non limitation of Of such systems that they are based on variables and on data that are a small portion of reality The second is highly connected to the first limitation that we cannot measure At all some some aspects of our lives and of our society Why the first aspect says that You cannot That reality is so big that you cannot measure all of it The second aspect says that there are some aspects of the of reality that are not measurable at all Even if you could have them the all the technical means you cannot measure for instance happiness You cannot measure an opportunity For a for a for a person that is located in the future. You can make a prediction You can use some proxy, but I cannot measure for instance. What is the probability that you will become an engineer of success I can make the prediction, but I cannot measure it And this is a second important aspect that that we see even if it's quite a um simple, let's say Actually is present in many many systems Deployed in in reality third aspects is that Even if we have a very precise measurement system In technological system that can measure several aspects of reality if reality has some Structural inequalities your measurement will reflect these Dear these structural inequalities so if I may if as you know Computer science is a discipline that is highly polarized with respect to gender I can know all I can measure I can have data about all the individuals in the world that are studying computer science That's fine, but The data will reflect the the gender imbalance. So I will have data that is mostly related to men And for The structural inequality between the global north and the global south of the world Most of the data will probably come probably come All will be all will be more accurate probably From where from the global north that are economy societies more Advanced from from the point of view of technology deployment Final point final limitations Is the following one there is mathematical proof that the bigger the data sets And the higher Is the probability to get correlations just by chance These are the so-called confounding also confounding factors and spurious correlations spurious correlations are correlations that are just random It exists in your data confounding factors are factors that are you are not Taking into account, but they do have an impact on your on your on your data We will see in a few seconds the first one we will see several cases Most of the cases real cases that That we study Have to deal with these macro limitations. So these are these are limitations of all or any inductive inference First first example we start with two examples that are located Uh now seven years back in the past And this is a specific choice because 2016 was the year in which many Scandals or issues were uncovered by press and is Somehow the date of birth the the year of birth of the specific data ethics discipline Data and aggregate ethics disciplines the first case Today we will we will Speak a bit more about two cases. They are all from the United States And I already told you why we are taking so much into account the United States because it's the most Developed the nation with respect to the deployment of ADM systems The first case is from well known e-commerce company That is amazon and The case was Was Was discovered For the first time by by bloomberg You can check the if you're interested that you can deepen then the details about this case through this url The company at the time had already vast amount of information Historical data about purchases of their customers. So who about what when where Etc And decided to launch a very innovative service Okay, apologies, but the microphone is not working and Again at least today the screen is working Okay, let's continue like this and then I will send another warning now they made Um, what I was saying that the company was launching an innovative service that today is is used probably by most of you that is the same day delivery Is the beginning is somehow the beginning of the Of the prime amazon Opportunities that you have that today they go much beyond the fast delivery delivering the same day The company decided to use to leverage the large amount of historical data that Owned about purchases in its platform with the socio-economic data Data from the census that the u.s. Is is publicly available and other economical data Not all of this data is possible was possible the time this article was written to to know Only the the high the high level categories Of the data that was integrated with the historical data of amazon the goal was to To make a predictive optimization. That means Make predictions on when or where the probability of people of buying things and requiring goods Was very high so that it was commercially profitable to offer the same day delivery that is the test of cost because you need to have A lot of transportation means in order to to To offer this this possibility and What was the result the result was Had some Unpredictable impact that we will now comment. Let's focus on the first city, which is atlanta And for each city use you you see two maps One that is filled with green dots and the other one is filled with blue dots The one that is filled With green dots represent unit of population if I will remember there are 500 people each thought That are the so-called the white residents so they belong to the category White cow cow caesion that is an official category used in the u.s. Census The blue dot instead is Referred to unit of population of the same size About black residents that means people that in the u.s. Census is lavalet as afro-american people The gray area is where the same day delivery is offered The white area or non-field area is where the office is the office the the service is not offered Based on the statistical optimization That were available at that time as you can see In atlanta most Of the people that were taking advantage of the same day delivery Well, white people because you can see in the gray area just focusing for a moment on the gray area If you check the density of the green dots is much much Higher the not only the density, but also the the number of green dots is much much higher than In the black residents maps Where blue dots are less also, if you take if you take into account the totality of the green dots Roughly more than 95 percent are all Located in the gray area That is the area where the service is offered If you take into account only the green the blue dots Approximately three four three over four 75 70 percent of the blue dots are out Of the area where the service was offered this was a An implication an unplanned implication and impact of the statistical optimization That decided that this was the most profitable area to offer the service And in this area most of the white residents were leaving We can debate whether A company like amazon should worry about these aspects And and hope we we we will have a short debate later In any case this is a factual a fact This is an impact Of a disparity impact of the algorithm The the decision on where to offer the service or not was Impacting differently people according to a certain attribute Which is a special attribute the skin the skin of the color It's called a protected attribute Because You have to guarantee some equity towards Ethnic origin and other attributes that we will see during the the course For a certain for a certain type of services This service is not among those services that should take care of protected attributes So let's just make it this clear in any case the impact was notable and was Covered by this journalistic investigation. We can check quickly all the other five cities in boston In the small area, which is quite a rich city So most of in most of the city the service is offered unless these Small center area where Mostly blue dots are present So again The white residents they had all access to the service to the fast delivery service In chicago the situation is quite uh, I would say that the the map Is quite evident that in the in the white area Very very few green dots are available. Why are represented while in the In the corresponding black residents map Quite a remarkable number of people with dark skin could not use this service Same for dallas Same for new york city Although less clear than other cities same for washington dc But this is an area where most of the black people lives The argument was simply optimizing by the profitability. So the area where Where we're residing most of the people who could spend money on the platform Uh, we need to say that Uh, after the publication of the study The company offered the service In all other areas At least for the cities that were mentioned in the study and later on Knowledge evolved and also economical availability of amazon exploded. And so the service now is quite It's quite basic Well before saying that let me say something more We need to consider first of all that this is a Particularity of the united states where cities are Uh, sometimes even sharply devises in blocks blocks where certain Ethnic groups live and and other do not live at all because for instance, there are too much expensive blocks that are particularly poor and where Uh, black population is concentrated You know that in that country there is a quite a big problem with access to services and access to equal opportunities For a large part of population that is black And this is correlated to economic Economic conditions. So This is sharp inequalities in the way that society is organized at least urban societies organized in us And was reflected perfectly By the optimization algorithm I think say that, uh We need to check the facts. So the facts are the following ones the classifications of the systems and the disparity impact on black people And you saw on the maps, uh That this this was quite clear Other two facts the source code No, the source core not the design of the systems Were inspectable at all and this is a fact. I'm not judging this fact, but this is a fact And this is a fact that was true in 2016 and it is still true for Very recent issues that we have for instance with different technologies like like large language models that fuel a chatbot and that have particular impacts on our information system our information sphere On the reputation of certain people Because even chatbots work with the same mechanism inductive inferences And the same question is Valid today for chat gpt4. We cannot inspect the model Uh, whether we shoot or not, this is still an open question And the data used by the software was not publicly available Seven years later the data the training data for chat gpt4 Therefore gpt4 is not known as well So these are questions are related to this specific case, but are still valid Uh The recent development of the i-Act in european union is moving towards These aspects there is a new center created just one month ago Center for aglitmic transparency In uh, uh, marseille If i'm not wrong that will will be charged of Moving steps forward towards the large platforms that according to the i-Act have Further and more strict obligations than other company Companies like platforms like facebook like google they need to Show some details about the way they implement their Classifications about the labels that they give To users and so on so these Aspects that that are that were erased in 2000 and existing are very very Uh debated right now and they are object of legislation Of european legislation Uh Of course i say of course that the source code was not available This is a company that needs to preserve the capital economic capital of the shareholders So there is an industrial secret However Is it also a company that offers a service that is quite important In societies at the time And now think and the pandemic of much we used amazon For the way usc is a structure that this kind of services could be have also uh, a quite remarkable social value because You cannot find for instance Fresh foods fresh vegetable organic food. Let's say in every Corner of the city so having such a service could be useful for certain people Could be very important more than useful for people who are sick Who cannot afford buying a car and for the type of society And for the way that usc cities are structured if you don't know if you don't own a car You don't have access to a lot of services. So having the possibility to to get home Medicaments for instance Drugs foods for people who do not have a car is something that has a social value So there are some Motivations for at least making the question the following questions Should given the circumstances and given the impact Disparity impact that we have seen Even if amazon Is a private company And invested a lot On the technology That technology should the source code or the design of the system be public There are several levels and here I opted for four choices both of them only the source code Only the design of the system On none of them There are four possible level that the service is not available yet About data the same question given the circumstances are given this party Even if the company is a private company should data be public Yes with metadata Only a subset with the metadata in order to understand it Only the metadata Or not should not we should preserve The capital the economic capital that was put into the into the company by shareholders and the work of thousand employees that work in the Not in the algorithm because much less works but it's work in amazon The same questions can be Uh Can be offered in two versions So the first version to everybody. This is a meaning what public means public means to everybody So I ask you whether Source code should be open to everybody or data should be open to everybody or Imagine that We might have trusted the third parties. So institutions research centers NGOs Whatever you like but an entity that is independent is trusted and Only that entity should have access to the algorithms or to the data Two different levels. So I would like to ask you, uh, this is not an open question. This is just uh, you have only these options and I Would like to um to ask you to express your your opinion About these aspects And hopefully to To then to reason together on the motivation behind your answer. So I ask you a couple of minutes Uh to answer whoever wants to participate to this short survey That should inform a short debate You have the slides so the link in the slides so you can uh, you can also, um, use the qr code So let's wait two three minutes You You You You But at the same time Yes, uh, there is of course this is this is a simplification Uh, we our assumption is said is that the third trusted third parties is a is an entity that is trusted That is independent as no, uh, at least no declared bias or political Vision on the issue and that if a trusted third party means also that if you share something with the with the party It has the the entity has the obligation to not share further the, uh, the artifacts So The center that I I I just mentioned earlier is a new center of the european union center for algorithmic transparency and it will have exactly this this goal So this is an example trusted to part in reality that just was just built, uh, uh, a month ago Like Yeah Where are you like taking the first consideration of black and white? As in diversity, so let's say this but third party actually takes into consideration About these two these two races So at some point we could say that there are Regardless of the diversity there is majority in which they could actually Make that central for me the decision Which means that third party is also white at some point Any institution is made by humans Yes, um, the fact is that a trusted third party that should Should work according to accountable principles that do not apply to usually to private companies That are not accountable because of several reasons because of Of um, let's say industry secrets because they own and intellectual property and so on But when we have the point is that I'm happy that we can already start this discussion. The point is that whenever Some AI tools that makes decision instead of us And this case was quite simple. We will see another case in which is is a bit more complicated If you use black boxes, they are not accountable anymore. If you replace a judge If you replace a judge with an algorithm and the argument is closed You cannot you can you can you cannot Recover the chain of responsibility. This is an open question. I'm not explaining. I will never express my opinion I will always try to Let you appreciate the complexity of the issues attend Did they answer somehow? Uh, let's share now To see whether you have more insights Let's share now Your opinion on these questions Okay At the question whether they should whether the source code or design the system Be inspectable by trusted third parties A large that is really surprising to me a large majority of you answered yes 20% only design and Seven and only three people answered none of them This is quite surprising because in the past years when I made the same questions. So they were actually the the reverse Uh Before going to data, let's see also the same question, but With the public with the large possible publicity to so public to all Uh From 72 percent we are now to 5 percent only two people kept probably the idea that Uh, the source code or design this is the system should be Available so preserving the intellectual property rights for instance of the of the company Uh, the second level only the source code Was selected only by true And then only design so there is you still allow a certain level of transparency And finally 30% none of them Is there anybody who would like to share the motivation of their vote? The but it's not a vote of their answer So is sharing source code a form of transparency? I assume no from your answer and why You Can you raise your voice please So you you are saying let's uh, if I want is to you say that the design is much more useful because Then you can reproduce the software Yes, so scotter is also not democratic because uh, uh, well at least it's less democratic than design because very only technicians can understand So sport fd and in cases it is quite complex Only very few technicians can understand the source code written Thousands thousands lines of code written by by a company By the way, one month ago twitter made the public see available to all most of the source code of its recommendation system Um Any other comments on this I Can bear it to the same fact Like make it source code source code more publicly more public public access Which is like at the same point as As it was mentioned that The keep this Thanks There is no right answer and I don't have a strong opinion on all of this but this is very uh hot at the moment in the debate about Uh chat gpt. They are the same questions Looking at data Asking whether uh anonymized data used by the software should be spectable by Tracitor party. Yes 55 55 with metadata The spec for the symmetric question on the source code was higher. The answer was higher 72 percent answer The source code and design And here data and the metadata a bit less. So it's like you you value more data than the uh the algorithms Just guessing from from the percentage that you can see And uh 30 only subset with metadata. So we have a at least according to this salary You have a quite large Favorable opinion on On sharing the data because if we sum up The blue and the red is the 85 percent And for that ring it was 72 percent Uh, and then we have a minority that thinks no should be preserved Of course the percentage is decreased when we are large availability at all and Analyze data used Published should be used to be published and be available to all No, yes only two person Answered yes with metadata And 35 percent only a subset with the metadata. So the 85 percent of the first question decreased to 40 percent so half of the people with so You see a value I assume that you someone of you say sees a value on trust the third parties Then we should debate what is a trusted third party as you as you as you say that how to Be sure that is Is accountable is fair is is not the same bias as the The company all the system that is investigating Uh, with that charge epit the problem is a bit more complex because he uses a vast vast amount of data that is already publicly available So here the question is somehow reserve reversed because should the question is Should the company open a high Uh make a game public the data so because it used a large amount of data from the internet Well at least Looking at the at the technical reporter on gpt4 and looking at the technical report on gpt3.5 It is much more detailed And at least that there were some Indications of the training data and from those indications and from indications on similar large language model like c4 for google it seems that There is a quite Impressive amount of data that was used to train the system It was already public. So here we have the person is a bit more complex because we have data that is already public That it was a result of someone else Work And somehow the benefit Is privatized there was there is a Let's say a common good. Let's call it even it is if it is not a proper definition But it is a resource that is available to everybody the web is until a certain extent available to everybody a company has the uh the money to um To build a system to to grasp all this content and to to make a model privatizing the uh The benefit so The question is Should the training data be at least public since it is already Made by a lot of public data Yes, not it's difficult question But these kinds of systems like jpt that are substituting now people also on cognitive tasks probably probably Looking how the debate is going right now needs special regulation because the impact is so big on society and because the resources That are used are also public resources that probably needs a proper regulation However, there is no right question no no right answer. Sorry But it's important to to make To make questions now Because now the decisions is under development and used maybe in a few years will be too late It is also a huge contra because the nabin's To treat this huge amount of data are possible only by a few actors Microsoft invested 10 billions on um the open eye Foundation 10 billion Dollars is a lot who has this power Governments only but governments are not investing such a huge amount of data Of money, sorry So we have a concentration of power to a few actors That's Are not accountable by definition. They do not Have to apply to the same obligations that might have for instance Let's say a public research center that Use makes the same technology A given the impact of this technology Should we make this company more accountable and one way to make the company more accountable is to require transparency on the training data on the model parameters and so on Open question Second case compass do you already seen seen this case in other courses? Sorry Not anybody else beyond Montanero discourse, of course But this surprised me because this is very important Okay, is that inspired then Hundreds of scientific studies in the machine learning communities Compas is an acronymus Correctional offender management profiling for alternative functions. What does mean? It means that it's a software that recommends judges about whether to give an alternative function To a person who is charged for a crime So instead of staying to prison I can have a limited freedom limited freedom. Oh, I can pay Yeah We are in the justice system and terms are very technical. So I will use a general terms There is there are more more precise terms, but they do not belong to to my Domain of knowledge so for paying to be free as a specific Technical term in in the justice realm. However, the idea is the following one to support judges and make somehow judgments more fair so recommending them What a risk Indicator for each person. So imagine that you are a judge You need to decide about a case in front of you. You have a person And you need to decide whether this person that was stopped for Either a crime or a suspected crime should stay in prison or can have a certain level of alternative functions at a level of freedom The The score that the person can receive is from one to ten ten being the highest possible It means high risk high risk of what? A risk of reoffending in the future So the system is makes a probability A probability based prediction or whether a certain person will commit a will make a crime again In the near future if yes, if the probability is high then his risk score will be n nine eight high risk Otherwise we make a will output lower risk scores from one to four low risk or five six seven medium medium risk This is the idea how the system works. It's not Substituting judges. It is it is helping them. It is supporting them in this decision This is an example really simplified of output About with real data from the system data that was Taken from ProPublica that is a NGO organization A journalistic organization that found The data on an unsecured amazon web service And obtained the data and then integrated With other available data to reconstruct the The output of the system Venor Prater as I was saying Was when was stopped he had in his history two armed robberies Plus one uptempt of robbery. This was his criminal history He was given a free that is a low risk score that is a suggestion to give him An alternative sanction basically Later two years within two years He offended again. He made a grand theft Again trying to stealing a stolen he stole some something else A pressure board and instead Had in her For you and I miss the minors crimes that are Uh, not so relevant like stealing a bike There was the reason why it was Lettally stopped when she got this output that was eight high risk And she was never then she didn't offend again At least she wasn't caught This is brish aboard and and this is vernal prater Another similar case is the one of dialing fudget And the burnon parker dialing fudget had in his history one attempt to burglary Uh, it was given a low risk of free And later in the following two years He was stopped again for drug possession I don't know what type of drug so much was severe but because this is change from Count to count in the united states, but uh, this is uh, this enters the criminal record Burnon parker had one nine nonviolent resistance to arrest in his uh, criminal history He got a 10 he never offend again. And this is the picture I think that's you now you have clear what's was the problem The problem in numbers was that At least in the data set That was available to a propabrica The 24 percent of of Of white people were labeled the higher risk But did not reoffend. These are the false positive So I receive a high risk But then I will not make a crime in the next two years But then I will not make a crime in the next two years This is not this is not a measure Of how much you are dangerous. This is just Just the prediction whether you will enter again the criminal record What about black african-american the double The false positive 45 percent were labelled as a risk and then did not reoffend. This was the disparate the measure of the disparate impact Other way around your label that's low risk, but then you reoffend so you can be considered A false negative in terms of prediction and this this error is somehow an advantage because You are labelled as low risk. You get a certain level of freedom if you follow the recommendation by the system And this was the reverse 48 white and 28 african-american The system was developed by north point Then changed name equivalent And it was offered to Public authorities and is still used by some public authorities in the u.s. So here the context changes a little bit We have a private company Still that develop develops a software that is then used by a public administration for a specific case That is Helping supporting the decision about whether Whether a person should stay in prison or not we can make the same considerations before We can observe this is a fact that disparate impact By the way, the company would develop to the software never Challenged never say that the data was wrong Confirm that the water the data was the real data and never Challenged these percentages. However, they say that the software was calibrated Was well calibrated according to the several to several editing groups And both were right Both the company and the propablica But again, it was not respectable neither the source code or the designer system or and the Of the data So before the break I ask you whether you would like to To give a uRansware let me open the survey, okay, sorry Okay I Ask you again Couple of minutes to answer to this question and then A few more minutes to shortly comment and then we make the break You You You You You You Okay, shortly We have the 77 percent of you says that both designers code should be available To third part to trust the third parties And is quite similar to the amazon case And only design we have to the the remaining part here disappeared The at least they did not answer the category none of them So it seems that being the service implied in some kind of public Public functioning makes an effect on your consideration Public the largest possible publicity of course as lower Lower percentages 20 percent and Only one source code and design 60 percent so 20 percent compared to previously 5 percent So I assume that the fact that The the software is is used again in in the justice in the public justice system makes Makes a difference about data Anonymized data should be inspectable by trustor party 60 3 percent and 27 that may makes the 90 percent of you very high percentages And with respect to amazon was Well, the the blue part is higher in the public a case, but something the two of them we have very similar percentages And then of course I cannot serve that we have less Um Less blue answers with the largest possible publicity Do you have further comments on your answer or you want to underline some specific aspect? Yes, please We Leave a specific election that will be devoted specifically to compass and we will answer to this question However, I can also already anticipate that A few judges were interviewed by propablica and not all of them answered However, the one who answered confirm that they had a certain level of This tool had a certain level of influence on their final decision this kind of bias, let's call it is called Computer bias or also machine bias. That's why if you click on the If you click sorry On these url You will see that the title of the article is machine bias So that the the fact that the suggestion comes from a machine you give it much more importance Thanks for the question I would like to resume the lecture at 5 40 So now we stop here for a while Yes Okay, thanks a lot for participating to the survey your comments also during the break Of course this small short service the goal is to try to to interact a bit more than a traditional lecture I would like to So we'll have as I said, I received a couple of questions on compass during the break We will have a specific lecture on compass. Well, where we will see why what kind of data Fools the system and why this disparity impact occurs Before continuing with The next slides, I just would like to share with you Just to let you appreciate that we are speaking in this course about things that are happening now About questions that right now society is trying to address So this is the the website of the the new center that has been just set up The official communication was During in april the european center for algorithmic transparency A few lines. Well the eca t we contribute to a safer more predictable and trust online environment for people in business Our algorithm system shapes the visibility and promotion of content And its societal and ethical impact in an area of growing concern Measures adapted under the digital service act Call for algorithmic accountability and transparency audits that are Soon mandatory for whom? It depends. It is a risk approach. This is the domain of mantelera and bachargo and I'm sure they shared with you details The reason the reasoning behind the the rationale behind behind the european regulation is the the highest is the risk And the more obligations that you have the highest possible the risk you cannot deploy at all your system But these are very few exceptions and this makes this is why the the regulation probably is not strong enough But for very large online platforms And very large online search engines you have some specific obligations And this new center The goal of this new center is to enforce the obligations. There is a list of VLOPs and VLOs se Companies, but not only companies also foundations like Wikipedia enter at this list with specific obligations And so now they are recruiting experts Scientists To make operations possible to make audits possible Scientists experts working at the C80 will cooperate with industry representatives academia and civil society organizations to improve our understanding of our algorithms work They will analyze transparencies assess risk Including Discrimination risk like the one that I showed you before and propose new transparent approaches and best practices At the moment the the center is hosted by the joint research center that is a specific research center of the european union. We'll have we have one in italy as well at ispra And Yes, they are recruiting right now. Let me check if they have some More information here, so they will address Companies working on social networks online marketplace search engines Where there is a huge need for public oversight Of the processes at the core of their business So this is low and it is right now The institutions to to make this low effective are are being built In these literate in these days And the c80 important Even if it is hosted by a research the european research center is Directly connected with the director general communication networks content and technology that digi connect That is like the ministry Of let's say digital innovation telecommunication of europe It's like it's not ministry, but it's that part of the institution that is exclusively devoted to the digital world Algorithm transparency recommender systems information retrieval search engines fairness Contability and transparency A whole part of our courses is devoted to algorithmic fairness Then ethical economic legal and social impact research case assessment risk mitigation measures and fundamental rights This is from the public Offer let's say so there are institutions that need to make low applicable But now we have a market growing In usa is a bit more advanced in europe is coming for instance a new company has been Has been formed In the last couple of months it is this is an article from the 9th of mars imanes That will be will make its business on audits Because Law is going to be applied very very soon and this is the website of Of imanes, we assess digital technologies to make them ethical and responsible And as I said, it's a few years that this kind of market Has been formed and is growing in the united states here in torino. We have something At least from its beginning is similar that is clear box. I and now turned its business Into synthetic data in order to avoid to use real only real data about people And we will have a seminar also given by clear box. I here during the course and I will tell you I will tell you in the next weeks when the seminar Will be and we also have a couple of thesis proposals in the in this start up As more information is coming. I showed you these web pages just to make a concrete what we are discussing Because New institutions and new companies are growing right now just on these on these On the issues that we are studying Going back to our presentation We proceed Let me check if i'm still recording There are a multiple of Other possible questions that we could make we will not go into this now, but just To let you understand that automation Should not be a software automation In a specific context where you are trying to especially when you are trying to predict the social outcomes Brings a lot of complexity and a lot of additional questions For instance, should we make only classifications? And provide to judges just a risk score or should we Add additional information What why the risk score was 10 not one for example How the training dataset was made Etc. So should we provide contextual information in addition to to prediction of Our classification Should we build a software with a totally different goal? instead of Of Making as output risk score should we give another totally different output? For instance Social vulnerability of the person who is charged for a crime This can also be turned into a classification with some limitation But this let's say Is very different when you have a software that tells a judge a this guy according to my data analysis is Vulnerable according to his History to other is unemployed for instance since many years etc etc Instead of saying this guy is a risk for society Then the the judge can use this information from the software very very different Which stakeholders should be included when you develop such a system that has such a social impact Just the company that beat the software or should or should we Involve better civil society associations working Uh with people in prisons for instance other types of associations The category of the judges etc so participation civic participation to the deployment of predictive optimization software that has An important social impact And then we should be responsible for software misclassification When I label as a high risk a person and the judge follows that classification and is wrong Who is responsible who should pay for such an error Can the person challenge the software classification? But these are all open questions new questions that are rising from a surprising big wave of software automation In the realm of human decisions Now let's have A few other examples without going too much in the in the case in which I will just show you Examples of disparate impacts not explanations. We will have a next lecture that devoted to explanation So this is this was an image that Was quite become quite viral on twitter a few years ago about a soft dispenser that did not recognize The hand of black people Probably because it was designed in a certain way to recognize only Light skin This is a disparate impact Another example of disparate impact This is an example of images Searching of images that are given by google when you make a search with the word co only made white Here the impact Is less evident And there is not let's say a direct impact on there is not a decision attached to this This software output. However, there is a certain representation of reality that is quite polarized No women here. Well here one here one here, but one here, but very few for Of course, I cannot know what that would be in your if you make a search Now because you see is a cost an agor an aggregate that is Continuous level being and is based partially on on the data that is taken by us from us Another google images search with the word carpenter Nurse so carpenter only male nurse only female in english this term is neutral Is not is not connotated by gender Kia this is a tool software classification image classification by google that tells you what is in the picture So here there is a hand and a gun But if we change the color of the hand and we make it clear Yeah, there is a hand and a monocular Not a gun anymore another example of This part impact Here automated moderation It's very difficult to moderate so called a speech in the social network for this reason usually is made by humans By persons that are usually in the global south and usually paid Uh, not so well for working very long hours with Content that is much much more violent than the one that i'm showing you This is a tool always by by google Perspective api tool that gives you a toxicity score from zero to 100 percent saying whether uh, signaling whether a sentence has a speech Can be leveled as a speech and so subject to removal from a social network, for example If you have a sentence like this, I would disagree as a black man My experience is different the level of the the toxicity score is 46 percent If you change one word Instead of black men, you write french men the toxicity goes down to five percent And then you have all intermediate levels so black women is 38 percent of toxicity homosexual men is 14 percent Desire to women 12 percent and so on so just changing Very few terms one just one in some cases The level of toxicity changes Quite a lot and again, we have a very different impact On whether there is the the world of black or or not This is a very recent paper on dal e Output by dalin if you ask An image Of an emotional person you will get the basic images from women if you Ask an image of a poor person You have a quite clear indication of the ethnic group groups Uh A tag they are all black a terrorist They are well connotated. This is output from dal e Well, this is a scientific uh, uh, paper This can be also a game try to test this software, but imagine an AI pipeline where you use automatically components that Gives this kind of output and then they are connected automatically to other software the chain of This parity impacts will grow and and grow This is an american man in this car and this is a fracking an african man in this car This quite the stereotypes are quite strong In dal e an american man in this house and an african man in this house An ethiopian man That is represented in specific with specific characteristics As a poor man with in a poor country And i'm sure not all but this is not a good image for ethiopian men If you write a software developer they are all male and white While a flight attendant is a female A chef is a man Uh, a cook is a female a taxi driver Is a black man usually from dal e and now skipper Is a female Also with some specific ethnic groups Uh, if you remember that one of the limitations of inductive inferences was that they replicate usually structural inequalities or structural differences in society And this is a quite clear a toy example of of this Amazon in 2018 had to stop the development of a tool to automatize the screening of applications to high people Because the software was basically Suggesting only white men people And The the reason as you can imagine uh is in the training set in the training data But there was there were no ways to remove this uh, this kind of disparate Impact that this is an impact that is serious because if you have a tool that discriminate in a very strong way female And you deny job opportunities Based on gender even if it was not your intention, but if this is the impact of your software you have problems You you may incur Low problems legal issues. So amazon since technically was not able to to Improve the system decided to to stop it We have still unresolved big issues in face classification and place recognition. They do not work Uh with well with black faces, especially black women I supervised a thesis last year Uh trying out two classification commercial classification face recognition face classification systems on faces of people with disabilities All wrong So again, we have disparate impact Here that software that works well If you have certain personal characteristics that you cannot change You cannot change the the skin of your color and in certain cases you should you cannot make differences with respect to uh personal characteristics This is uh Suggestion for your own curiosity culture a video of about An hour 50 minutes you and I just an engineer the politics of AI where you have more and more examples Is given by Kate Crawford correct? Crawford is a is working above on microsoft and she is a professor in uh, we remember about New York university and University of paris Uh, she wrote a brilliant book that is called the atlas of AI in which she Show what is Uh Behind the supply chain of AI systems. So from the hidden work labor from humans to uh instructions of massive instructions of rail and natural resources Environmental cost and so on so on is a very good book and now pete crawford is well known in all over uh the world She will speak in september in polytechnic. Oh, you know that at a database conference That is organized by professor baralis and she will be Uh acd ml. I think is the name of the conference. It's a conference on machine learning. She will be among the keynote speakers Uh a few years ago and uh the problem was only known to Very specific research centers or very specific journalists Then evidence become uh to um Begin to grow and grow and the media also covered a lot that ethics issues and then politics also Right and understood that this is a problem. This is an accept from a public speaker of Margaret vestager We is in charge of competition law and digital In in europe and is also the vice president of the european commission That says us very clearly. What is the problem trained on bias data? These kind of systems automated decision-making systems They can learn to repeat those same biases and sadly our societies have such a history Of prejudice that you need to work very hard to get that bias out I don't know whether it is possible or not to remove this bias Um Because if you change the training data you lose accuracy. So it's a problem. It's an open problem for for uh businesses and research as well But the problem is now known at the highest possible level of the european institutions Now coming to an end of this Of this set of lights We are interested in understanding better these relationship the relationship between Society and digital technologies a specific type of digital technologies that we defined in terms of inequality Our question is to understand whether the predicted optimization systems that are have been deployed a lot in these years are Increasing or not are improving the inequalities of our society Unfortunately, I have to anticipate you that the question is the answer is negative They are not improving. They are Making things worse and we'll try to understand why we're trying to to have Notions of algorithm fairness to understand how to measure The fairness of algorithms And Uh, why because our discipline is quite new. It says only 70 years of history Computer science in modern terms Was born with the the first digital computer 1946 the anya can us was the result of intensive research during the world war So we can say that our discipline is quite young and uh We are still making difficulties in Understanding that the system that we you will build we are building. They are not only technical system They are social technical systems. They are systems that have some specific technical characteristics But they also have some social That have to be taken into account when designing and deploying a system In other disciplines, this is quite that are much much older This is quite obvious in our discipline. Instead it is it is quite new instead Although although In at least in research this is debated since a long time since the very first Um years of our discipline imagine to build a road an highway When you make a project of a highway you don't need only to specify and to take care of technical details like The type of materials that you are going to use The the level of inclination how many lanes etc etc How much traffic that that that highway should should be able to to um to convey But you also need to take care of what kind of territories you are dividing when you build an highway you divide Well potentially forever at the territory you cannot cross anymore easily Unless you make some bridges and some other technical artifacts You cannot cross easily the highway the highway you bring economic opportunities because of course you you let Good flow much much easier But you also have to to take care that situation like this do not happen Here in this picture there is a Apparent probably which is which is on a child that is going shopping or coming back from shopping so in a for instance in a country where It is difficult to have a car because it's very expensive when building such a huge Street or highway or or even streets urban street you need to take care of people that cannot afford a car So probably sidewalks should be a bit better than this to avoid Having a negative strong impact on the life of people So this is a metaphor, but in the discipline of urban constructions there are Techniques and awareness Of this kind of social issues and this kind of impacts in our discipline. This is growing and there is one of the goals of our Our course if you go back to the very first slides you will see that You will we hope that you will learn to Understand the impact especially especially towards vulnerable people like any other engineering discipline. So this is Trying to connect with the very first introduction Uh, what we see We will take into account and to consideration because if we are in europe the european charter of fundamental rights article 21 and on discrimination Any discrimination based on any grounds such as sex rates color Ethnic or social origin genetic genetic features Language religion or belief political or any other opinion membership of a national minority property birth disability age or sexual orientation should be provided Any any time that the software systems make predictions and classifications On social outcomes and there is a disparate impact towards any of these Protected attributes. It is a problem. We'll consider it as a problem So take this list of attributes as a reference point whenever We speak about discrimination Whenever you you will see in an exam text Explain possible discrimination issues They should all refer to protected attributes not to any type of attributes in a database But this is very important article sub article comma one of article 21 The european charter of fundamental rights Um Having said having shown what's What are the types and example of this parent impact we now proceed and explain why It occurs At least in a large majority of cases So, please allow me a few seconds to change slides and then I will also upload them So So Okay This is like set is a derived work from a book That is Almost finished. It is already available online fair ml book.com fairness and machine learning limitations and opportunities by Sar They are the most active professors in the fields Uh attention that the derived work means that I took uh, I based these slides on A specific chapter or the first chapter, but there are some Things that that I added and a few others that I removed So take as a reference the slides. However, if you go to if you want to go deeper on some concepts, you know that you have A chapter of a book that is openly available. You can consult This is the machine learning loop that is proposed by the authors of the fair ml book and it is It is made of very of a few elements so First of all The individuals as the people together make made together Taking together we make the so-called the state of the world of state of society we can That means that we can aggregate data on individuals and individuals and Elaborate some patterns on groups of individuals. So this is the the reason why we They divided the two elements by means of measurement you get data from what from the so-called the state of the world and so from individuals You learned from data and you build a model the model As an output a prediction or classification that is bound to an action This is very similar to the To the definition of artificial intelligence sergeant that I gave to you. So there is an action on individuals and this closes the Big loop and there is a shorter loop because individuals can change Their behavior according to the action of the model. Well the system that implements the model and They can provide feedback that updates the model directly. This is possible technically possible Not easy but but possible this creates a further loop inner loop This is a formalization that we will check step by step Why this parity impact may might happen The first is about the state of the world it is aggregated data on individuals as we have already said that the society is not It's not perfect. So there are some structural inequalities For instance in Italy it is well known that the economy is much much stronger in the north than in the south And if you if you collect economic data on individuals, this will be Most of the times reflected Sometimes there are not inequalities just these proportions Like the number of women in this class is Probably 10 15 percent Because this is common in the population of computer science courses And we have seen that in the amazon case that The probably the the black people were living in they were all concentrated in areas that were poorer That's why the areas were not selected by the algorithm to offer the The same day delivery service And there are others infinite possible types of demographic disparities Few very easy and well known example. Let's start from torino Our city and you know that torino has eight Administrative regions sub regions quarters make make good smart quarters And if you take from the open data of 2017 the average age By gender family average By divide by That is the administrative unit. You know that It change can changes So I used on purpose the blue for female to change the common stereotype and You can see that there is a difference of four years between circoscrizione due and say between female And male is three years difference What are two and six? two is Mira fiori nord's mirror fiori sud and satarita and six is the opposite barrier of milano legio parko barka vertola, etc Just just taking the two poles. So we have a quite relevant difference on average age Then we can also have a look at the age distribution All together and we can see that females lives longer At least according to that of 2017 But situation shouldn't have changed a lot Because you have more female In this area of the graph that is from 55 To even 100 years On the left side of distribution, we have instead that there are that males are Are you there are more younger males? So this is a just a straight example of demographic disparities that basically Can easily enter The loop the feedback loop Is the age distribution in two districts of torino Ciclos Ciclione one and two as you can see is quite different Is in at least in the level of number of people living and then in certain regions of the of the distributions Then we turn into more interesting at least from my point of view data that is the income distribution Where the richest people live in torino on the top of the hills as usual So here this is data from the tax declaration So official data that there's some limitation Especially in Italy has some known limitation But we need in data we trust no so this is the official data and we can see that There is a large Large difference in terms of income here All the north of the city is around 16,000 20,000 maximum euros per year and here we are over 50,000 So if you use this data this disproportion, this is more than disproportion is a structural Let's say difference Will enter will enter your training data for sure And here you also have In the city center you also have Some quite good salary that where is the middle class the middle class is here in the south part more or less Then you can Check this data with Election polls and you get some interesting results, but this is up to you if you want to do it In Italy what I was saying if you use this to take the same data From Italy instead. Well, the difference is average data, of course always It's quite big north Italy as you can see is the blue darker blue In south Italy is the The yellow so there is a difference between Trentino and Calabria of declared income Of about 38,000 euros on average And this is again another example of Of inequality structural inequalities that can enter easily A system that is informed with this data this train with this data We know that the reality might be a bit different. This is a declaration of tax And in Italy we have a huge problem with black market or black Let's say salaries But this is the factual data the only data that you can use if you're interested in this type of of data for our applications And this is another interesting example Then we larger a bit more our Our view and we look at income Inequality Then we cannot use euros if we enlarge our view to the entire world and we use the genie coefficient That is zero if there is a complete equality in terms of income in a country In terms of income in a country and one if there is a complete Inequality so a very Inequal state when there is very few people that are very rich and the majority is very poor will have a Genie that is very close to one This is the meaning of the genie index Where is Italy first of all? I don't see yes here We are not so good 033 At least in the among the ocd Countries And it is increased in the last two three years the the the the pandemic Increased the inequalities in all over the world And then there are countries like South Africa with a very high genie index Costa Rica and look united states that will be The home of many of our use case is quite high. It's zero four. So these are countries where There is a large inequality of of income Europe, Iceland, Norway, Belgium, they are the best country in terms of Equality of income This is another example of Structure inequality you cannot change that will be reflecting your data if you use this kind of data. Maybe you are interested in The seven seven and the six hundred million people that Lives in Europe and it's quite a quite a good amount of data that could be used In in some system and you have to take account that as countries are different Very different among the service and inside their territory What about us russia wealth inequality? Let's use this graph that is a statistic from Forbes that is Regraft by statistic the most important portal about statistics As a student political student you should have access full access to the statistic. I think So this is the average Early income of a white person in united states And we are constantly above a hundred thousands dollars The difference with black and latino is huge is a factor of 15 let's say or even 25 according to official data So I will assume from now on that some of the Structure inequalities that we are looking together Are will be the base for your reasoning index sum There will I will always refer to such kind of inequalities that you will see today and the way that we see in some other cases Yes, please With Thank you very very good observation So I should update this data because now these years are forecasted. So probably I will have The new data will probably reflect also what you watch Even more what what what you say it Also because we are witnessing a quite historical wave of inflation That we witnessed it only only in the 80s and back in the 20s That brought to the world second world world But this is cultural consideration that are important and It's important to make these kind of considerations in university classes And so you are always welcome to interrupt me and and the reasoning together with me comment For the sake of the exam It's enough that you know that there is a huge disparity between white people and other ethnic groups, especially black people and latinos in united states for instance Just to be concrete and practical Uh At least half of uk's black stuff affected by russia pay gap and new research finds so black people are paid less Uh Also with the same work with the same with the same Let's say not contrast the same duties work duties The same is happening in italy with women same duties paid less This is a graph from the world economic forum the gota of economy and Uh, it say it shows the performances in the in the time during the time of ethnic groups young adults in u.s universities And indifference indifference According to different ethnic groups As you can see black white they are So they are all somehow decreasing but there are differences between black is panic Black and white have this kind of difference. You see let's let's take one line per time. So it is easier At the 20 at the 12th year of school The similar pattern is at the 8th in math similar pattern is at the 8th Year of school in math So this is the highest observed difference between white students and black students in grade math This is the meaning of these first two lines clear Okay then Okay, the third line is similar We have less difference than if you compare his panic and white In math and then we start so math Shows the biggest differences between black and white and also partial between spanish and white Then we have black and white in reading. There are also relevant differences on the 12th year And also between other other comparisons So the graph says that education skills Are more advanced in white people in us And less advanced in black people. There is a social technical social economical social technical explanation and is based on the fact that the the the The school system and us is also based on economic opportunity or your family of origin But this is not a class of inequality Is a class of data ethics So what is important for you to know is that there is a gap of education skills us and this is a fact Between white and the other ethnic groups, especially black people This is the takeaway another takeaway from this graph Then this is what I'm I was saying earlier the gender paid up The average in ICD is 13 percent at least according to data I don't know. I didn't put the data the date of this data. I should I'll try to update soon I'll try to update soon In Italy we are around 5 percent us around 18 percent is a lot In Japan even 24 percent in korea even 34 percent is a lot So take away we have a problem in the Equality in terms of payment gender equality in terms of payments This is a statistic from 2020 2020 in Italy gender pay gap women on average 27 thousands euros per year and the men at 30 thousands average So to be taken carefully this is an average But there is a a pay gap Of about three four percent Percent according to this data that is consistent with the five percent by ocd If you want to go a bit more in details On this data, we we see that most of the difference is the in the low Lower paid jobs blue collars where the difference reaches even 13 percent So women blue collars women are paid More than 10 percent less than men At a parity of let's say category of work The defense shrinks until a minimum in the middle management and again increase again in the top management I will never never ask you about the numbers, but it's important that you know about this fact And then I leave to your curiosity the trend in the past What about us Similar research, this is one of many This data from the world economic forum The damages of pollution Are more serious for black people because they live the more the the The neighborhoods or the cities the regions that are in which there is more pollution They are more exposed to pollution. So there is a russian problem also for For us These maps show you the exposure the bar then And the two known whites that is biggest in certain Regions where that according to the census, there is a certain kind of population So latin americans 63 percent more pollution exposed to what they produce 56 black americans. So this is not a statistics But the takeaway is the same if you live If you have less economic opportunities, you live also in territories that are more exposed in terms of environmental damages Take away all these these lights And this is an interesting Report from the Lancet journal is one of the best journal in the medical sector Showing the mortality the mortality rate in Switzerland it was one of the Most advanced the economy in terms of Economic well-being Countries in europe It is showing The mortality rate also the and other statistics According to what to the socio-economic position Social economic positions are labeled in a scale from one to ten And if you look at the last graph at the deaths Relative deaths rush of deaths per 100,000 people There is quite a relevant difference between the people in the lowest social economical positions And the richest in the highest economical positions You see The sep sep group Have highest deaths Until six seven roughly then the richest and this is again is a fact So economic opportunities Can give you Obviously better Medical treatments better knowledge on how to to to take care of yourself More money to buy medicaments roughly speaking also there was a yes, uh Yes Social economical position Is that is an absolute value or if it's related to the amount of people in the Disease It's absolute No, it's it's it's not normalized is that you have number of deaths per relative to 100 divided by 100,000 people Thanks for for the question Again the important thing to know is that there might be a correlation between your socio-economical position And the level of medical treatments that you can afford Especially in countries where Where there is no public health system And you need insurance, but if you then a war you don't have a war you don't have insurance As you know medical insurance is a big problem in us again Because people who cannot afford medical insurance Cannot access certain Medicaments That's why also mama was trying to to to make the Medicare for for all And I don't remember how Until which extent he He he was able to do so, but this is again is a fact is a is a problem that we should take into account When we consider structural inequalities The people who studies structural inequalities specialist sociologists, but also economists are Elaborated these Graph that I will briefly comment to you And this is was was proposed by A black scholar patricial Collins black feminist scholar Who called it the matrix of domination or matrix of oppression, but Let's for let's take it as matrix of domination. That is what is the matrix of domination? Explains in general in the most advanced economies. What are the characteristics Of the dominating class? What is the dominating class is the social class? Who owns most of the political and economical power? And The dominant group in the world but On average and especially us is has the following characteristics is men European Anglo-Americans They are rich They are well educated. They don't have disabilities They speak fluent English They're often young and they're heterosexual This according to series of statistical studies and social and studies from sociologists. These are the main traits of The people that is in power In Societies of the world This is a sociologist view perspective It is important to understand that Wealth is russian connotated Income is russian connotated in many countries can be even a gender connotated as we have seen So this kind of structural inequalities should be taken into account when when what when when using predictive optimization for Social outcomes because social outcomes are the results of The structure of society in which you live There is a book on AI by Sasha Costanza-Ciocca That Explained Quite well this kind of Of implications for AI technologies This is an article designed just AI and escaped from the matrix of the imagination. This is the book that design justice Community led practices to be the world we we need With a view on design or software technologies We have an explosion of books on these topics Just signaling you for your own curiosity Just signaling you for your own curiosity intellectual curiosity automating inequality by Regina Eubanks one of the first books that were detailing How inequality structures were replicated in ADM systems And then another important book is math weapon distractions by Katie Neal. There will be a slides also on it Okay So it should be clear now that the world has some Demographic disparities and some structural inequalities that Can be transferred through measurement into data into the training data of Of a data driven system Um, this is actually another example of how this is then translated into data Into data by the measurement process And this is a graph from the Washington the New York Times on the Washington poster Showing how much black people are still underrepresented in the in the Trop universities the top colleges Regardless of the affirmative actions that were taken the policies that were taken in the united states Regardless of that We have that whites and isharn are still over represented with respect to the actual population in usa in the top colleges Then his panics and blacks This is also there is also an additional problem that is the following one that The multi russia category When you rolled into university was not used until 2008. So what you should what you should Should track when you roll if you are if you come from a multi russia family And this is a limitation a further limitation of Of many of many data that is used in us census, but also in other in other institutions because The race is tracked The ethylene group is tracked in those systems and it has an impact actually Then if the data is is used I consider that it is also very controversial to to use a race as an ethnic group as a Category This is because race does not exist in nature Race in the english terms. It is a social construct For example, look at the classification used in south africa It was officially used Uh in the 50s and was implemented by ibm Was europeans asiatics persons of mixed race or colored Natives or blue blue that the individuals Or the ban to race This was out of people was registered into the census of south Of south africa Very this is very also politically connotated It is a social construct. This is also a limitation of using this type of tracking this type Of data This was made to introduce you the fact that That whenever you we measure Characteristic of people Individuals and you have a variety of forms We make politics because any measurement process is not objective even though Usually we give Objectivity when we look numbers we say the numbers of objectives The measurement process and all the decisions That are well made to build a variable even a single variable Is a political process that means is a process that embeds a certain Uh visions of the world Send certain characteristics of society And I will try to convince you on next Monday So if you don't have questions make sure it's over Well trying to convince you means that I will explain you formally why it is like this. It's not an opinion Thanks. Have a nice evening Oh