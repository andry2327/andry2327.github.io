 Okay, so we are at the last few classes of this first part on the law in the court. How are you doing? And we devoted this last three lessons to the topic of AI. And with possible during the last class we also tried to make some other lessons. And then I think you will have one day without my class because I finish this part on the 17th And as far as I know my colleague is not 18 but the following week. But we see in the calendar. So as we got this part on AI, the main problem addressing the topic is the fact that we have not specific regulations for now. As you know, because I already mentioned sometimes there is a proposal that is under discussion. Brussels, the so called AI Act, is an act that we have already mentioned at the beginning that is an act that is initiative from the commission of parliament in order to regulate something that has not the seal regulation at national level. It is not an exercise of harmonizing something that already exists in the directives but in the regulation. But it is something to prevent, we can say, any future differences in the regulation of this topic among the member states. This discussion started a couple of years ago and it still opened the debate. It is expected at the end of the first vote on an agreement at the level of parliamentary EU parliament. But of course the final approval needs the agreement between the parliament, the council and the commission. So for this reason we don't know exactly. In the best scenario we have the approval at the end of this year. In the worst scenario, due to the fact that there are political elections, there is a risk that it goes to the next fund. This of course will be a nightmare because after spending a lot of time in trying to prepare draft regulation, the new parliament also considers that change that probably we have in the position of the European parliament will be at serious risk in terms of approval of this act. So for this reason we cannot comment into the phase of the provision because it is one of the several available drafts that we can consider because the draft process and the level is not transparent. So there is not an official draft but there are several drafts that are discussed at different levels. And not public available to a larger stand. So it is quite difficult to know exactly at the moment what is the text under discussion. So for this reason I don't want to dig too much into the text as it is a provision. But there are some key elements in the AI regulation that are relevant and almost will stay in the final version. Some minor changes but in terms of structure the key elements are already set. And with regard to these key elements there are some open methodological aspects that should be considered also having in mind the consideration and the analysis of the regulation of data protection that we already done in the previous class. So this is the scenario for this last part of this section of the course. So AI regulation. And let's jump. Okay, from data protection to AI regulation. So the reason that we started discussing... We missed a part on the data protection. Okay, so I have to come back. The reason that we started discussing AI regulation was about the fact that we realized that the existing framework in terms of digital regulation that in Europe was mainly based on the GDPR was no longer adequate in order to address the challenges of new kind of technologies that some years ago were called big data and then AI. Starting from big data to AI we saw a sort of a new wave of technologies that changed the traditional paradigm that we have in data protection. And because you know that data protection is based on the GDPR but GDPR is a third upgrade we can say of the freezer director. The goal is back to the end of the early 90s. The scenario that is very different from the present scenario in which we have a way of processing personal data that is quite different from the previous one. In the past scenario we had a system that was mainly based on the principle that we have ever seen that is the principle of purpose specification, the idea of consent. And in the previous framework we had the focus on purpose specification, the idea of consent as an instrument to evaluate the potential risk of processing operations. Of course when we move from this context to the context of big data analytics and AI we have important challenges with regard to this point. In what sense? If you consider for instance purpose specification, purpose specification is something that we have defined according to the GDPR at the moment in which you collect the data. But when you use big data analytics and when you use AI in many cases you have not a clear idea of the specific purpose. You have an idea of the general purpose but you have not a clear idea of the specific purpose because one of the key features of this application, to understand is the possibility to extract inferences. Inferences it means something that you already don't know about the data set but the correlation that data set are explored by this system in order to point out some connection. Then if this correlation represents causation is another matter. But in terms of purpose it's quite difficult sometimes to be very specific with regard to the purpose at the moment of data collection as I said by GDPR in general that the protection law. Because you have a general purpose for instance analyzing the illness that we have in an hospital and truth it is analysis for instance you have some inference about the cross relationship that exists between different kinds of pathologies or the effect of being closed in the hospital in some areas and all the transmission of some pathology etc. But it's something that you don't know in advance or is it a result that was in an unexpected way to understand. And so for this reason it's very difficult to provide a specific purpose according to data protection. On the other hand if we look at the other main pillars of the legal ground to understand the legal ground has been frequently based on individual concern and also in this regard the idea of concern is an instrument in which you look at the information the notice that you receive you understand what are the potential consequences of data processing. The processing is no longer fully effective we can say in order to make possible self determination with regard to the processing. Because why in the 80s I give you a card as a fidelity card that you know that the points were based on the cost of single item that you buy etc. Now if you consider profiling profiling use a dozen of variables that march your information about what you buy in the supermarket with the real last flight and many other connections. So the system that we create are much more complicated and you are not able to understand exactly in a system that use hundreds of variables what is the impact on you in terms of the final decision making process. For instance if you look at the screen scoring some research I read down some years ago show how in the US among the variables that were used in order to assess the grade scoring for each person they consider also for instance elements concerning which kind of TV program you watch during the night. Which is not exactly something that you imagine as related with the grade scoring and your attitude in order to pay something. So this approach made possible by recent development of cloud computing and analytics. The approach based on a large scale of data has stressed the focus on the quantification. On the idea that everything can be transformed in data and that the quantification is a key approach in order to analyze and to predict the phenomenon. As a consequence due to the fact that it's quite easier to increase the number of variables and it's quite easy to combine many variables and indicators. We create a system that are very very complicated to be understood by the user not to be designed but to be understood by the user. And sometimes this system also uses in terms of tech instruments and tools like neural networks or other forms of deep learning that are quite complicated also to understand in terms of their function. And so we know that they produce some outcome that are useful but we don't know exactly the process to reach this kind of outcome. And for this reason the combination of these different elements make very difficult to state as in the past that through the noticing constant system you provide a constant in which you are aware and understand the processing operation. And you decide in a free and hour way the process the decision to give or not your consent. So now it's much more difficult to have this kind approach. As we have stressed this was reflected also in the GDPR that move from the traditional approach based on the legal grounds on approach more focused on risk assessment. The data protection impact assessment that was stressed in the GDPR is a clear answer in order to emphasize the risk on the shoulder of those that created the system rather than on the shoulder of that assignment they have to decide if they want to or not provide, whether they want to provide or not the data. But in this scenario it's clear evidence has the data protection law that for many years represented a main reference for the legal system in the digital context is no longer adequate in order to address all the experts. And the case that we discussed last class about GDPR and example of this regard as we have seen focus mainly on the collection and processing operation but this is not the main concern at the end of the day. The main concern of the GDPR is about biases about potential misleading information all aspects that are not related to data protection but that right now have not a specific protection in the digital context by law. So for this reason the AI creates some concern in terms of the effectiveness of existing legal framework to address the rising issue. And for this reason we have to go back to what we have stopped during the first part that was addicts because there's a relationship existing and this was exactly what I wanted to figure out. So the first answer in order to address this challenge and this is the data set that the set those lights on the number is M2 data address. You wish you that. Why there is a connection the connection is due to the fact that in front of considering complexity of the processing operation in order to face this challenge. The first answer from also the legal environment we can say was to shift the focus on ethics. The first important initiative in this regard was the initiative of the European data protection supervisor. The European data protection supervisor is the authority that supervises all the EU bodies in order to check their compliance with data protection. It's like a data protection authority for the EU body. And so it's an authority that is very focused on data protection. But in 2016 the European data protection supervisor start with a paper and then we had a workshop and then they created a doc group start discussing about data ethics. Because the point to rise at that time by the supervisor was we have to face the challenges of big data and the rising AI and data protection is no longer in order to address these issues. Because these issues are behind data protection they include the risk in terms of discrimination or an impact on society in a broader sense. And so we have to focus on ethics. This move of the data protection supervisor was followed by similar approaches also by the commission that later created an AI group of experts on AI that deeply relay on the ethics approach involving several experts in ethics etc. So this point that is interesting in terms of approach should be understood thinking about the fact that when you have... Thinking about the fact that the law is not something that is given like in the Bible by God but the law is the consequence of some view that we have in society. So in this sense there is a strict connection between law and ethics. The legal principle reflects the values of a society and the values of society are ethical and social values. So if this is the general framework, society values, ethical and social values law in which the law is simply the codification of some values, the shaping of some of these values. The consequence is that when the law does not provide an answer you go to the upper level that is represented by the general ethical framework. So this is the logic. This is why the authorities start to focus on ethics. Because ethics is to understand the model of the law is the origin of the law and the law provision. And when the law provision is not adequate in order to address the new challenges the logic exercise is to look back to the general principle, to the general ethical principle. So for this reason in the moment in which there was no specific provision on AI and there was not a clear idea about how to regulate AI, their reaction was to look at a general picture and the general picture is represented by ethics. And for this reason in that day, in that year, we emphasize the focus on data ethics. And data ethics is only a subcategory you can say of ethical issues and ethics and technology. There is a specific focus on the use of data and the ethical use of information. In this sense what is interesting is the impact that we have in terms of what we have mentioned before, the notification of society. The notification of society has changed the relationship that we have. Because the typical relationship that we have in terms of research, in terms of ethics of technology, is a relationship between someone that done a new technology or make a carry out a research and individuals, humans that are part of these experiments, a part of this implementation. In data ethics it is a bit different because we are not the person but the data. So humans in this game are represented by their information about them. So the data ethics is different from other branches of ethics because in other branches of ethics of technology, the relationship is between technology and humans. Here we have the relationship between technology and data that represents humans. So there is something in the middle, there is a sort of mediation through the data, the information. So the data, we focus on the ethics of data, we focus on the ethical use of information, but information are not person, information are representation, is a representation of person. So in this sense is a different kind of dimension that we have in this kind of ethical framework. A second point is that this progressive datification of society including physical and relational dimension arises an approach that is not focused on the individual but is focused on the global picture and means that on the impact of society in general. If we look at data processing, the traditional idea that we have for instance in the protection law based on the individual dimension, personal data, it means information referred to an identifiable natural person is no longer the core of the processing operation because when you use big data, when you use AI in many cases you are not so interested in knowing exactly the person. Of course there are applications that are based on profiling, that are very focused on the person but we have several applications, many applications that are not focused on the person but are focused on categories. So what is important for the system is to understand if you belong from one category or another, regardless of your identity. So this is another aspect in terms of quantification and analysis of the society that is different from the previous framework. In the previous framework, in terms of regulation, the focus was on individuals, on the person. We granted the person the right to assess, we granted the person the individual consent, so everything was at the individual level. But when we use large scale data in order to detect some behavior in a city, in a region, in order to predict the behavior of customers, of user, etc. We focus on categories, no longer on individuals. And this dimension is not fully addressed by data protection because data protection is for individuals. So this is another change that is the collective dimension that also in terms of ethics is largely unexplored, we can say. We have some aspects that are considered in terms of collective dimension that is about minorities for instance or about some small group, typically indigenous group, we can say, that in some countries, for instance in Australia or in Canada, have specific protection at the level of group. But generally speaking, this group dimension is quite absent in the protection of rights, we can say. Group rights is quite limited. So these are two changes and of course the focus on ethics has also some backslash in the sense that this alert, this fact that the data protection authority stressed the importance of considering the ethical dimension was also exploited, we can say, by some companies in order to transform ethics in something that became a sort of marketing tool or something that was an easy way in order to create framework of values that is consistent with the corporate values. Recently a quite famous philosopher that is Luciano Floridi decided to change his mind at least and to adopt an approach focused on regulations saying that there was a mistake for a long time focused on ethics in order to solve the problem of the digitalization. Unfortunately, it was one of the most important points in order to stress the ethical approach as the best approach. And what was the problem? The problem was that while in the initiative of the European Data Protection Supervisor, ethics was a sort of solution in order to address a broader topic, the corporate version of ethics was an ethics wash in many cases. It means creating some ethics code or creating some ethics board in order to say whatever we are doing is legitimate because there is someone that is overlooking about that. Don't worry, there are no problems. But in this analysis in many cases this overlooking was very poor or very corporate oriented also in the values. So ethics is different from law because law is codified. Ethics is not codified and not being codified quite easy to manipulate according to your needs or your interests, you can say. So this was the first challenge. The second challenge was about the overestimation of the ethical values. Many initiatives focus on ethics and ethical values but ethics does not represent the only aspect of the non-legal value, we can say. For instance there are many aspects that concern society that are societal values but are not necessarily ethical values. In the debate the societal dimension was quite limited, what do you mean societal dimension? It means that it's not a matter of good or bad but it's a matter of societal, acceptable or not. Societal acceptability of a technology is not necessarily a matter of ethical acceptability. Ethical acceptability means that it's in line on against some values. Societal acceptability means that we want or not that kind of solution. And we can decide that we want this solution or not, not based on the fact that there is a conflict with the values. But simply this solution may represent not our scenario in terms of delimit. For instance the case of Marciz and the final case of Toronto show the fact that that was not a problem of a medical city. It was simply a tech solution for the city that was not what was expected or that had in mind the citizen in Toronto. So acceptability and ethics are not the same and for this reason we have not only ethical value but also social value that are largely not considered and as a consequence is not considered the dimensional participation because the participation is only in order to understand what is the societal meaning of the society interest. And the third point is that in this debate on ethics and AI we also create some false problem. And for instance a typical debate was about the trolley problem in self driving cars. If the AI prefer to kill the babies or the old person or kill 10 persons rather than one or very different kind of solution that were possible. This is to understand a bad way to find the problem you can say. It's not a matter of ethical decision about who wants to kill but it's about the design of the technology and the design of the environment in which this technology is used. So in this sense I think it's something that you already mentioned as from the beginning I don't remember. In this sense the problem of self driving cars is not an ethical problem about on which kind of target we have to put the risk but it's a problem of how we design the environment in which we create the risk. So exactly as when you build an infrastructure for instance a railway in the right way of course crossing the railway is a risk. But it's not a matter of ethical decision if the train have to kill the people crossing the railway or have to slope creating some damages to the people that are on the train. The fact is simply that in the railway there are means of protection and there is a legal order not to cross and to pass in that area. If you do not respect the legal order and if you don't have not stopped by the physical barriers the rest is on your risk and there is no problem of legal liability we can say. So this is an example that clearly shows how large part of the trolley problem is a false problem because if you correctly create the system if the people do not respect the rules they accept the consequences. It's no longer an ethical problem. So in this sense for this self driving car the problem is not about what can be the outcome in terms of accidents but how to prevent or how to put in all to firstly in terms of liability the potential accident. So it means as now is some case we are starting doing it means that we can create a specific area for this kind of cars and of course this protected area there are barriers there are legal notice in order not to cross and not to pass in this area etc. Then if you don't respect you and you have an accident this is not an liability of the self driving car. So this is another important point to correct the frame of the ethical dimension and to set the appropriate solution. In terms of relationship between data ethics, data protection, law etc. we can see a sort of growing spheres of interaction at the center we have in the area of law of course privacy and data protection. Data protection is broader than privacy because it doesn't cover only private and not review information but also public and private information. And then we have the law in general that includes other aspects that are not about privacy and data protection such as discrimination, freedom of opinion that can be affected by AI based content moderation or other application. And then we have the largest area represented by the social ethical users. In this sense we can figure out how it's important to have a sort of integrated approach that consider all these areas. Ok, so if we look at the data ethics and user ethics in shaping data driven technology we have to consider that there is a difference between law and ethics and we have not to misunderstand this. There are two contexts. For instance in the first report of the iLevel group of experts on AI appointed by the European Commission there was a clear confusion about ethical and legal values. They also state that the ethics values that they ensured in this document were based on human rights which is an in logic statement because human rights that are based on ethical values are not ethical values that are based on human rights. Human rights are a specific, we can say, codification of some ethical values in terms of equality, discrimination, human dignity etc. So in this sense it's very important to keep in mind that there are two areas. The ethical areas in which we have a general principle value and the legal area. And the relation is complicated because sometimes the ethical value became a legal value. For instance the idea that we have not to treat in a different manner people so not to discriminate is an ethical principle. But this ethical principle is also translated we can say in ethical values, in legal provisions in which for instance we have some provisions that say that it's not possible to discriminate in selecting the people for the job application, some provisions against some specific discrimination like racial, gender discrimination etc. So when we move from the general principle and the ethical value to the legal value or legal principle we make an exercise to codify these values and these principles. Why stress this point? Because this point is very important because we have to understand that what is in an ethics is not the same as in law. If I say in a ethical level for instance that non-discrimination is a core value but then in the implementation in the law of this principle we have many different reasons. I can't decide for instance that non-discrimination is also a legal principle limiting this principle to the discrimination against racial bias for instance. But not considering gender bias. For instance I had some tough discussions with the Russian representative because they are against all the gender discrimination and gender issues because they don't consider gender as an issue. And of course many other countries in Europe have a different approach but it means that although in Russia there is in the legal framework this idea of non-discrimination is restricted to some areas. Why in other countries is broader including also gender discrimination. The same is for instance other principles in terms of ethical principles there is of course a principle about freedom and freedom of expression. Individual freedom and individual freedom about the talk of the person and to stress the talks etc. But then this freedom of expression can be very different in the level of protection that has at the legal level. There are countries in which the freedom of expression is broad and there are countries which is limited in the interests of the state or the interests of private actors etc. So this is the point. What we have in terms of general principle ethical values is not the same that we have in the law provision. Law provision transforming the general principle in a provision necessarily shape this principle. And shaping this principle provides a specific angle we can say in terms of viewing this principle. This is an important point also to understand the dynamic that we see in a way between the ethical and the legal regulation of the eye. Another point that we have to consider from the ethics perspective in using ethical rules in order to regulate the eye is that these ethical values are different from the laws. While in the laws we have some general principles that are commonly accepted like human rights. For this reason we have international charters of human rights that are agreed almost by all the countries although in the implementation there are some differences. In ethics there is not an official global ethics. There are many different ethical approaches at group, at individual level and also at theoretical level. At theoretical level we have the Kantian approach in terms of ethics. We have the more deterministic approach in terms of ethics. We have a more socratic approach that are based on virtue ethics and so on, a different kind of shape. We have all the ethical approach in the Asian and Oriental ethics that are not discussed in the IDP about ethics. So there are many differences. So as a consequence it is very difficult to decide which is the ethical framework. When we select an ethical framework, this is a decision that limits other options, that limits other ethical views. So what was the consequence of this first stage of the debate on AI regulation looking at ethics? We have some consequences in terms of critical consequences. The first one was a sort of uncertainty. Because as I mentioned in both the guidance provided by the companies and also by some initiatives, as I mentioned one of the European Commission, the other group, there was a confusion between law and ethics. It was not clear because when you look at this ethical course or at the guidance provided by the 11 group, the list of ethical values to an understanding of a law of legal values, respect of individual dignity, respect of fundamental rights, non-discrimination, protection of environment, but all values that are already embedded in the law, there are specific provisions that already protect these kind of values. So if you shape this as an ethical principle for the development of AI, of course you create a sort of confusion. When you create a sort of confusion, because if a value is already set in the law and you recall the general principle, the risk is to have a sort of clash between the specific implementation that we have and the fact that you recall the general principle. So if I say that an ethical value is privacy, for instance, but privacy is implemented in right to privacy and data protection, what it means? It means that in ethics code I can reshape the existing law and data protection using the general category of privacy, for instance, providing a different kind of protection, broader or more restrictive. This is the problem. The law when shape a general principle and general ethical principle, gives a certain framework. If you recall the general principle, the risk is to reopen the framework and so not to have a clear position about what is needed. The second point was the variety of approaches and to many of the initial works on ethics of AI, the lack of a clear approach we can sales. Because in all these documents we adopted hundreds of ethics guidelines on AI. We can look, for instance, at the work done by the back one center in Harvard that they collect an analysis of all these charters on AI ethics. And there are many, many different kinds of documents. In all these documents there is no any statement about the ethics framework. They simply say, oh, for us these are the ethical values. Why this and on other? What is the ethical background is not mentioned. So this is another limitation of these approaches. The third point is of course that all these contexts are very focused, all these initials are very focused on the context and mainly focus on the western countries. So they reflect a very Euro or US centric approach in many cases, in which our values are presented as global values for the delp in AI. And another point is a very critical aspect that is already known in the field of research ethics, we can say, is the risk of transplant. What do you mean transplant? It means that like it happened in social science in the past, in which the values that we can say the experiments in the medical sector, the ethics for medical sector, were transplanted, so copy and pasted in social science. The risk is to see the same in digital ethics or data ethics. In several documents we see the key ethical values that are common in the magazine, for instance beneficence, normal efficiency, etc. Copy and paste in the data ethics. But the context is different. If you are a doctor and there's a patient, of course you have to take care about the patient. But if you are a soldier and you use an AI based drone, you have not to take care about the people that you are bombing, the context is quite different. So the copy and past of ethical framework, like in the past, is of course a challenge with regard to data ethics, because the context in which you use AI is a broad counter. There are many different kinds of applications that have different kinds of ethical values. AI in medicine is not in terms of values like AI in finance, AI in warfare, AI in education. The values that are behind these different cash-in-earners have different values, of course. And we cannot imagine that the same principle can work in all these sectors. Okay, so we start... Based on this discussion, emerged three different pillars in order to address the challenges of AI. The first one was relaying on existing laws, so basically data protection. The second one was to stress the role of ethics. And the third one was human rights. Human rights was at that time not very explored, but was a byproduct, if you want, of the confusion that we have in many ethical documents, in which we refer to ethics principles that in the end are not ethical principles, but are human rights principles. So we can say that in the first stage of confusion, in order to figure out how to address AI, these are the three main areas that we can identify. Some provision coming from data protection, some provision coming from ethics in general, and some coming from the human rights approach. In terms of debates, of course, as I mentioned, there was an important role played by the institutional initiatives, like the one of the European Data Protection Supervisor and Commission. But of course there was also an ethical debate, and as I already mentioned, there were ethics calls. The level of debate was, of course, different. While in the academic debate about ethics, there was the benefit of having quite an extensive background to the fact that the ethics of technology is not something new, but we have a lot of literature on this topic. So in the theoretical analysis of this topic, the academia and scholars were much more focused on the value, the framework, and the details. But when all this stuff is transformed into political arena, we miss the picture. We miss the reference, and we focus on the key points. And the key points are this list of values. So the political debate transformed the broader discussion that we had in the past about the ethics and ethics of technology that can be useful also for the ethics of AI. In a more simply version, that is very focused on the outcome. What are the key values that have to take into account when I develop AI? And so they squeeze all this debate in a list of values. And of course, providing a list of values is not enough, in many cases, to understand what is behind this value, how we reach this value, etc. And in this sense, the company decided to address this, and not address, to follow this approach, set in a list of values, and created bodies, tasks in order to implement ethics in their business, in their product. In doing that, they create expert committees that have the task to create a sort of ethical framework. Microsoft had an ethics board, Google had ethics advisors, many companies have ethics experts in order to drive their innovation. What is the problem in this approach? The first one is the fact that when you create this system, when you create this board, everything can be manipulated, we can say. It was sense that the selection of experts is not neutral. You can select an expert that, for instance, in the past received grants from your company, and of course they are more oriented to the interests of your company. You can select experts that are more in favor of some product development rather than other. So the selection is the first point. The second point is the procedure. Also, in the case of the high level group created by the commission, there was a lot of debate about the poor outcome of this group. The fact that there were a lot of people, more than 50 people, so they divided the topic in group and subgroup, and then they merged the other results. Of course, the lack of general overview, the segmentation of the work, and also the limited amount of time that in many cases these experts have in order to address this topic, impact on the final results. And this is true also at company level. If you want to create an ethics body but not 1 to 12, too much, the role of ethics body, it's enough to reduce the resource for this body. If you give a few time to analyze the decision and to make a decision, if you don't give a lot of support in terms of staff, etc., this ethics board will limit its work. So the structure is quite important. As well is important also the way in which we reach the final decision in this group. In the sense that if we need a general consensus, it's different for instance if we need a mere majority in the decision. Because if the rule is consensus, also minor opinion, also those that represent some very specific field in terms of knowledge should be on board in the final decision. While the other hand is a majority, it means that some views can be not reflected and not considered in the final decision. All these aspects are technicalities, but they are technicalities that may significantly impact on the final outcome. In order to drive the outcome more towards one result or another result. For this reason I say that there is room for manipulation in the larger sense. So you can try to understand the results of these kind of activities. As we mentioned, the problem is of the lack of clear framework in many cases. This is something that we already mentioned. Colonia is a factor that is Eurocentric of not an American or Celtic approach. In fact there is a flourishing of many charges. We already mentioned it. In order to map these approaches, we have some general studies. I mentioned one of the Bergmann Center, there is another in Germany, that have selected a large amount of available ethics charter in order to understand if there are some key values. Because what was the problem? If you want to regulate AI and there is not a legal framework, let's look what is the ethical approach. And due to the fact that in the ethical realm there are many different initiatives, let's try to aggregate, let's try to look at what are the main drivers. This was a sort of sociological approach if you want. In order to understand if it was possible to figure out, to extract some values that were generally accepted in the community with regard to the ethical regulation of AI. So we have some of the studies, the last one was also done by the Council of Europe, Yank and the Dappers. And these studies are useful in order to understand a bit more if we can clusterize the values that should shape AI. But before discussing the outcome, we have to stress some limitation of these studies. The first limitation is about the fact that all these materials are based on documents that to a larger sense are not official documents. So the mix of charters by companies, by NGOs, by institutional bodies, etc. So there is a certain kind of variety. The second point is that in many cases these analyses are affected by some constraints to do the language, for instance they do not consider some charters in some languages that are not the most common languages. And the second point is that, the last point is that many of these studies focusing on what are the key values, in terms of ethical values for AI design, we can say, they have a very quantitative approach. It means that they focus on the frequency. How many times in the chart there is a reference to non-discrimination? How many times in the chart there is a reference to human dignity, etc. But this approach is okay from a quantitative approach but it's wrong from a policy and a political studies analysis. From a political and regulatory approach, what is important is not the frequency but it is important the alternative nature of the source. What I want to say is that if some principles are listed in the 50% of the charts that you have seen, but are not listed in the charters on ethics issued by the big institutions like United Nations, Council of Europe, State Actors, etc. and they are almost provided only by small NGOs. In terms of impact regulation, this frequency of 50% is very, very limited. On the other hand, if some values are present in a few charters, but there are charters coming from big institutional actors, like international bodies, like state actors, or like big companies, or big NGOs. Of course, the fact that in terms of number, it doesn't represent the impact because in terms of impact, their impact is very high. So for this reason, this approach doesn't fit well with the ethical issues that we have to investigate. Okay, so just to mention this limit, we have to look at what are the values and if we look at the values, the first point is that the values are defined in terms of general terms. What it means, it means that in the charter they say, okay, the focus is no discrimination, but there is not any further information about what it means, no discrimination in our development. And so the fact that you outline the principle to not solve properly the problem and how to implement this principle in a concrete way in the AI development. So for this reason, many of these charters are very interesting in terms of set of values, but values per se do not solve the problem. All the AI delfas almost agree about the fact that they want a human-centric AI for the benefit of humanity, no discrimination, respect for the mental rights, etc. There is a general agreement on that, but in concrete what it means is how we can reach this goal. So this is the first limitation of these approaches that is very high level, we can say. And in terms of the values that they figure out, we can say also that there are different set of values. Some of them are already in the law, and so in this sense are not ethical values. For instance, the principle of sustainability, explainability, openness, and all the principles that are not in the list, but are strictly legal principles like protection and dignity from the right, etc. are already in the law, so are not new ethical values. Then there is another cluster of values in these charters that is no beneficence, or beneficial arm that comes from the realm of ethics. And finally there are some general values like common good, well-being, or solidarity that comes from the general social values. So this points out the variety of principles that we find in this instrument that try to figure out some ethical guidelines for AI development. There is a mix of ethical elements, societal elements, and legal elements. So of course this creates a sort of confusion in terms of regulation. This creates an improper overlap, we already mentioned. So if this was the state of the art of the debate, and this were also the limitation of this debate, what is the first assessment we can say of the role of ethics in AI regulation? We can say that the main mistake was to make a confusion between the ethical and the adopted layer. So ethics is not useful where simply repeat what already exists in the law. In this case create confusion and not solve problems. But there is an important role that ethics can play in order to cover what is not in the law. So this complementary role of ethics is very important. Because as we mentioned the origin of the problem was the lack of a full answer from the legal framework with regard to the challenges of AI. So in this regard there is room for analytical evaluation. When I developed an AI solution for a community, it is not a matter of legal compliance, conformity, but it is also a matter of ethical acceptability for that community. Consistent with the values of that community. This complementary dimension of the ethical framework is the part that is important and that should remain in this debate. And we see that also now, after the AI proposal, is the part that is still important in the future scenario. The second part that we have to point out according to this complementary approach of the ethics is that the ethical values are not like the legal values. So what it means? It means that the approach should be different. As for the legal value we have in the law and in the international principle, some frameworks that are the same for everyone. In ethics it is not the same for everyone. In ethics, ethics is contextual. So in terms of exercise, in terms of developing solutions for AI, while in the case of the law is a mere matter of consistent with the legal framework, in ethics there is not consistent with the ethical framework. We have to investigate what is the ethical framework in the specific context. So if I develop an AI approach to smart cities in Europe, I can have a common legal framework in terms of data protection for instance. But I have not a common ethical framework because the smart cities in Toulouse is different from the smart cities in Prague or in other parts of Europe because the values can be different. The way in which a citizen sees the city and their relationship in the city may be different. And so as a consequence, the ethical framework is different. This contextual nature of the ethical framework is something that we have to keep in mind because when we want to include this dimension in the development of AI, it means that we have to investigate in the real world, on the field world, what is the ethical framework. What is the point that created your such smiling comments and interest over the entire classes? Can you share your opinion so it benefits from your knowledge? I was smiling because I was watching other people doing other stuff. The problem with people that like you watch other people or make other stuff is that they are in the wrong context. Because people that come to a tender class, it's supposed that as medium clever people, are able to understand that in order to spend three hours of their life, it's much better to spend three hours outside and playing football, whatever you want, rather than staying in class and doing other things. Also because now the weather is quite good and outside also is much better than staying inside. So for you and for other people, my personal suggestion is that I don't need an audience because this is not sort of representation like in a theater. I'm not paid on the base of the number audience, but the class is for you. If you are not interested, don't understand the reason. You want to spend three hours here making something different rather than following the discussion. And this is not in the order of the things and also you waste your time and your money because you pay for education. So you are wasting your time and your money. So my suggestion in general in this class, in all the classes, if you are interested, stay and participate in the event. If you are not interested, do something better for you. It's not mandatory to attend the classes. The classes is to learn. You can learn in many other ways the topic and you can pass the exam or maybe not, it depends by your background. But it's not a matter to stay only because today we have class. The class is for those that are interested. If you are not interested, perfectly okay, you can stay outside. But stay inside and doing other things or looking around or having a strange habit is useless for you and it's also disturbing for the rest of the group. So going back to AI and ethics, the contextual dimension is a very important aspect. And the contextual dimension, we also to admit that partially affects all the new models. Because we said that at the beginning, also human rights are very contextual, which was understand. We mentioned I think the freedom of expression is recognized in China, in Russia, in Hungary, in Italy, in Spain and in the US. But it's not different. There are different ones with regard to the marginal manoeuvres that we have in terms of freedom of expression. This is the meaning of vernacularization. Vernacularization means that to a certain extent also human rights that are almost universal have a sort of contextual implementation that you have to keep in mind when we consider the human rights aspect. So this is the background as regard the ethics of AI, we can say. So what was the outcome of this debate? The outcome of this debate was quite poor, we can say. In the sense that at general level the initiatives at the institutional level by several bodies do not reach a strong practical, we can say, answer in terms of implementing ethics. They almost provide a list of principles, but remain still unclear how these principles can be translated in AI. At corporate level there was a lot of initiative, but only in few cases this initiative reached their goal. It means that in many cases they create bodies that oversight but in a very general manner and we have not enough information. So we know that in some cases we know that there are some bodies, but we don't know how they interact with the product and the development of the product in the company. In other cases we have more information, for instance the oversight body for Facebook that has the task to check the moderation in all the cases of Trump, etc. But for instance in the oversight body that is a typical social ethics as a science because the idea is to check if some behavior are okay or not according to the values that are set by the company. Here we see several limitations of the ethical approach. The first one is that the oversight body carries this evaluation based on the ethics values set by the company. So I'm not a general ethical framework, but it's the ethical framework that was set by the company. So of course this means that the company has a certain influence on the ethical framework. The second point, the oversight body can check only the consistency of the decision in terms of stopping or removal content with the ethical framework of the company but never discussing the entire structure. What it means that they never discuss the fact that for instance Facebook per se was more favorable in order to increase fake news or some kind of polarization because this was not in the task and this is a clear limitation of an ethics body in which you can analyze a subset of the problem but not the big picture that affect of course also that subset. Other entities have created advisory or ethical bodies and they perform a bit better in terms of cooperation for instance Axiom that create AMS and an ethics board that there are evidence in the document on the company that they follow the indication from the board and the board provides the general level of the product. It was a bit more concrete. But you can say that there is not a clear evidence of how this ethics board impacted and a recent outcome in the chat GPD and Bing application and the fact that they limit before adopting chat GPD in Bing they limit the role of internal ethical advisor reducing the ethical staff etc. And the other cases similar that we had in Google in the past show how there is a sort of gap between the ethical approach that is presented in terms of corporate reputation we can say and what is the effective role of these bodies in terms of shaping the product and the service of this company. So also for this reason in that ongoing debate on AR regulation in the last two years we shift from the ethical approach to the regulatory approach. So the initial discussion on ethics that was the answer in a moment in which there is not clear idea about how to address the AI then we realized that it was not so effective and that there was a lot of misrepresentation, misunderstanding and critical issues and so for this reason we shift from the ethical framework to the legal framework. Of course this shift is a challenge again because in the AI actor there is not any alliance on ethics and societal impact and this is not good because as it was not good to focus on the ethics and framing only as a ethical problem it's not good framing that only as a legal conformity and security assessment problem because we have both. We have problem in terms of legal framework and problem in terms of social ethical acceptability and in this service stage of AR regulation we move from an emphasis on ethics to an emphasis on law. So at the moment everything is focused on AI Act that is very conformity assessment approach it means very focused on security and safety and so on. So no room for ethical societal issues and we miss the first part that after being overemphasized now is forgotten. So this is the problem. We have not reached the right balance between combining a strong legal framework appropriate for the context and also taking into account the other aspects that are very important when you create a system like AI system that impact on society. There are aspects that are not only about the role conformity but about the social acceptability, ethical acceptability. At the moment we have no discombination because we start from ethics and then we move to AI regulation and we lost the first part. We hope that at the end of the AI Act we will be able to recombine and we expand again our sovietical framework because we need both in order to properly address the AI challenges. And I think we can stop for the first part for today now and then we continue.