Chunk #1    791 tokens
-------------------------

1.

 Try to find the netical sum answer to AI regulation and AI challenges. And now we move towards the other solution that is the regulatory approach that is under discussion that the AI acts. But as we have discussed the part on data protection, and now we try to figure out which relationship exists between these two legal approaches. Because to some extent the AI acts have some features that are related to the experience that we have in data protection. So it's very important also from a very practical point of view to be aware about the connection that exists between the realm of data protection regulation and AI regulation. Also because in the company I'm not like in the book of law or in the law that you have the data protection law and the AI law. But you have to combine both because you use data in order to create AI. And you have to comply both with data protection and AI regulation. So it's very important to figure out what are the connections and relationships between these two areas of data protection regulation and the future AI regulation. So in this sense as you know the data protection approach has been for many years the main reference in regulating the issues, the concern in the digitalization and the digital framework we can say. And this is still the situation that we have. The recent decision of the Italian data protection authority on chat GPT clearly point out that now we have seen data protection as the only instrument in order to address the challenges related to the impact of use of data in society. Because there is not another framework. And this of course stress the role of data protection because it's also true that we have stopped the processing operation in chat GPT but this is not the main problem processing operation. The main problem is the impact of chat GPT on society and so we use an instrument that does not fit perfectly in order to address the problem. So this role of substitute we can say of a specific answer in regulating digital society has been done for a long time by data protection. Also according to a theoretical framework as the framework said by the Council of Europe and then adopted also by the European Union. That look at data protection as an enabling rights. What it means enabling rights. It means that if you look at the protection, the real goal is not to protect data. You protect data for something. Since the beginning of the protection law, you protect data in order to avoid the control by the government. You protect data in order to avoid the exploitation of your information by the companies. So the data protection is the largest means more than a goal. And the goal is usually another kind of rights. So in this sense, this enabling function of the data protection has opened up to a broader view of the protection that goes beyond the data protection. And for this reason, when we start discussing about the AI, we start realizing that there was much more than data protection. For instance, one of the first point that was raised maybe in US was discrimination. This is an interesting cultural aspect. In Europe, we discuss a lot at the beginning about big data AI in terms of data protection, but not so much in terms of other rights that potentially affect it. In US, we are the first to point out the impact on discrimination. Why? Because of course, having a long history of racial bias and related problem, the first application of AI was concerned in this case. And unfortunately, there was also evidence, you know probably the compass case. You know the compass case? Sometimes I would ask my colleagues which kind of background they want to create for data scientists if they don't discuss some very critical case. But that's it. The compass case was a case in which the compass that is a corporation, provides a service to the courts in order to predict through big data analytics, to predict the level of recidivism. Recidivism means the probability that a crime be committed again by something that is in jail. In  

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    799 tokens
-------------------------

2.

the US system, when you decide to reduce the number of years that a person has to stay in jail, you have to make an evaluation. And in the evaluation, you have to consider the risk that the person can commit against crimes. In order to do that, the typical approach was the human-based approach. The judge, considered a profiled person, is track history in terms of crime, the context, the behavior during the period when he was in prison, and then make the decision. In order to optimize everything, following the quantitative approach that is now the trend of this new positivistic approach, the idea is why relaying on humans that in many cases can be misunderstood, not be consistent from one case to another, be emotional against one person to another, much better use an artificial intelligence that is neutral, that is objective, that uses only data. This is a very important debate about humans versus machine, and humans' bias versus machine bias, because both humans and machine are biased, the problem is only natural bias that will be different. But this is a topic for another discussion. And so they use this software provided by Kampas in order to calculate the recidivism. And the problem was that in order to understand if there were some risks, an NGO made an investigation in the Kampas case, looking at the outcome in terms of decision by Kampas, and looking at the system, and they realized that there was a bias against black people in the decision. And this was a time of the case that creates a lot of concern in the US, and the rise of this approach more focused on discrimination. That is a notion that is much more present in that society than in other societies that are more uniform in terms of composition, of course. So, not on some more uniform terms of composition, but in some societies that are more inclusive, in terms of social relationship. And so this was the first, the discrimination was the first aspect that was emphasized in many discussions about the impact of the society. We also to admit that this is a limited perspective, because discrimination is one aspect. But to the extent that you use AI to make decision in many fields, in many societal interactions, of course AI potential affects all rights, because if you use AI to monitor and to moderate social network, of course this impact on the freedom of expression. If you use AI in order to manage traffic and other aspects concerning the movement, it will impact on the freedom of movement. If you use AI in order to provide the software to parents for the education of the kids, you can drive the education of the kids towards some values on other, this impacts on the freedom of education. So there are many, or if you use AI to create content, of course the bias in the AI can be reflected in the content and so it impacts on the freedom of tools. So many different rights and freedoms can impact it, because AI is a pervasive application. So it can be applied in many different scenarios, so affecting many different potential rights. So in this sense all this stress is the limitation that are in an approach that focuses only on data and data protection. Data protection is no longer enough, we can say, in order to consider all the potential impacts on AI on fundamental rights and freedom. And at the same time the other critical point in relaying all the data protection is that data protection, although in Article 35 of the GDPR, includes the solution that opens to other rights because Article 35 theoretically says that you have to assess the impact on all the potential affected fundamental rights and freedoms, although in practice many of the protection impact assessments devoid of very few space in this kind of analysis. But the problem is that you cannot channel all this issue through data protection. What that means? It means that the impact on the freedom of tools or the impact on discrimination are not a subset problem of data protection, but have a background in terms of theoretical framework, 

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    792 tokens
-------------------------

3.

 in terms of jurisprudence, in terms of legal reference that should be considered. So in order to assess a system if there is no discrimination, it's not just a minor exercise, but we have to take into account all the aspects concerning discrimination and its regulation. And this is true for discrimination as well for other rights and freedoms. So if you want to have an impact analysis that includes all the human rights, the exercise is much more broader than the exercise that you do when you consider all the data protection impact assessment. Because you have to consider a specificity of each potential impacted rights and the legal background that we have created with regard to these rights that include its legal ground. So for instance the charges from the right, then the protection provided by the courts, and so the cases, the guidelines, etc. So it's much more broader and complicated. But at the same time we can also consider that if those are the limitations affecting or relating on data protection only, on the other hand the data protection framework is useful in order to better design, you can say, the future AI regulation. In the sense that in data protection there are some features that can be also used for regulating AI. One important point is for example this idea of the by design approach. You know the by design approach, data protection by design is strictly related with the idea of risk assessment and the idea that you have to consider the impact on data protection since the early stage of the development and the design. And you have to embed this value in the design. This is a key principle that should be also used in AI regulation and is in this sense also reflecting the AIR. Because if it works in data processing it works more in AI application because AI application is not something that you can develop without considering the impact of this application in society. And of course this impact is not something that you can discover at the end of the story when you have designed a product. If I want to create an AI in order to evaluate the performance of the students at school, of course the potential impact on their rights to access to education etc. Or the potential impact or in fact discrimination etc. is something that I have to consider at the beginning. When I consider for which purpose I use, which kind of decision is supported by this tool. And when I decide which kind of parameters are included or excluded. All these decisions are at the very beginning of the design stage. It's not something that's at the end. So the by design approach, the idea that the value is something that should be considered and embedded in the architecture. Since the beginning of the design is something that we know well and we have also after some years implemented, you can say adequately, in the practice of data protection and should be also used in AI. Of course with a broader range of values, not only data protection but all the other fundamental rights and principles. The second point that is important in the experience of data protection and that is also important in the experience of AI. Is keeping in mind the fact that technology is not neutral. So it's combined with this first point. In what sense? That as we have seen in discussion about technology and ethics, the way in which we shape technology have an impact in the way in which we shape society. So for this reason, this by design approach should be carried out with the awareness that what we create is something that will impact on the society. And the way in which we created the tools will have a consequence in society. And so we cannot imagine that it's a mere piece of technology. It's a piece of technology that can shape society. And so also in terms of computer scientists, it is a sort of a responsibility in order to decide to shape some product in a certain way and to put it in the market. And for this reason, there was a lot of discussion also for chat GPD and there was this petition sig 

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    816 tokens
-------------------------

4.

ned by many scientists against this rush in order to put on the market a new product with us. And this was an important consideration for the potential consequence. Not because these scientists are against AI or AI development, but because they rise the concern about the lack of adequate awareness and about the consequence of this kind of product and the kinds of consequences. The third point that is important and is related to the previous one is the importance of the risk assessment. Data protection law are based on risk management and risk assessment and this idea of risk assessment, of course, is even more important in the field of AI. And this is clearly evident in the AI Act that we can say is mainly focused on the risk assessment. It's also a limit, we can say, but it's clear evidence. And of course the assessment means that you have to design models for assessment. And we see that this is a limitation of the AI Act in which the core is risk assessment, but it's not a model in order to assess the risk. And this, of course, is concerned in terms of complete implementation of the Act. And the whole of the protection authority came up with this. In terms of the approach in regulating AI, there was a debate also in terms of how to shape this regulation. And at the end we adopt an approach that is consistent with the GPR, that is consistent with the industry regulation, and is focused more on an ex-center rather than an exposed approach. When the discussion started about the regulation, two acts, two proposals were introduced. One was on the AI Act and the other one is the liability, AI directive. So the focus on liability. And they reflected the truth, as we already know, are possible to be used in order to regulate the technology. We can either shape the technology in terms of some requirements, in terms of some measures that should be implemented in order to prevent the risk. And then we can also add another layer represented by the liability. So what happened in the case of that technology can create a damage. But it's very important to have both. A regulation focused only on ex-poster, focused only on the armor, focused only on liability, would not have been a good answer in terms of AI. Not only because it's generally not a good answer in terms of technology regulation, because focusing on the armor is focusing on the scenario in which the damage is already occurred. It's much better to prevent the damage, of course. But this is more complicated in the field of AI for a similar reason. The first one is that in many cases the nature of the business in the AI is not represented only by the big player, but by many smaller startups. Startups that develop new AI products. What it means? It means that in terms of risk they bring an iron risk with innovation. But in terms of liability they are not so strong because usually they have no money to pay. So the business context should be given into account when we consider how to regulate a sector. Because if the sector is dominated by big players, like car manufacturers for instance, of course I can deeply relate also on an ex-poster liability. Because I have the pocket, I have people that want to avoid paying so shape the technology in order not to pay. But if I have a small company, startups, they prefer to put on the market something extremely new, receive a lot of money and sell the company. So in terms of liability this model is not so good because you have no time to target your object in terms of legal action for instance. And if you are able to do that in many cases you have not a company that have enough money to pay. So for this reason the business context in AI suggests also preventive approach on technology regulation, then focusing on liability and compensation for damages. The second point is the imbalance on power that characterize large part of the AI. Because if you want to adopt an approach based mainly on liability you need to know that someone is liable so you need to know that there i 

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    792 tokens
-------------------------

5.

s some prejudice. Some prejudice occurred. But looking back at the compass case it was possible to know this bias only because there was an NGO that decided to start an investigation. But now that there are dozens and dozens of AI applications developed by municipalities, regions, big malls, companies etc. How is it possible to be aware about all the potential bias or problems of this application? So to understand this system I have developed and a supervisor that is very difficult to be aware about the fact that there is a potential damage and so you can claim for liability. So this one aspect. The second aspect is that the imbalance in power is also increased to the fact that the existing framework has not included a sort of collective protection with regard to the use of data. All the data protection, all the laws also human rights are very focused on the individual dimension. What it means? It means that in the eye context in many cases the individual dimension is not the best one. In what sense? In the sense that people may not care about the impact of algorithms on their life or maybe alone we can say in doing that. In what sense? If I am a consumer and I have bought some products that are not good for me for instance, there is consumer association, I can't know other people that are in the same things etc. If I am a member of a religious community and there is discrimination against my community, I can't go to the community and can't we react. But if I am discriminated for instance by an AI user in an online services that increase the price for me, how is possible to understand what are the other people that are in the same situation? How we can create a group in order to react? Because in order to have a legal protection is important also the number. If you are alone against big companies, it is a bit more challenging in terms of effort rather than if we have a group that can interact in the interest of the group. And the algorithms in many cases works in a way that emphasise this individual dimension. It means that you are not aware about the fact that other people are in the same of your situation. Because the classification created by algorithms is a classification that is based on the traditional categories. So if I discriminate people based on their religious, of course the people that are in the same religious community can react. But if I discriminate you based on one of those parameters about your profile, you don't know other people that are in the same situation in order to be together and to react. So this is the problem. And the last problem is that in many cases this AI system, the most sophisticated one like those using immigration, border control, police detection etc. are the result of public-private partnership. So this creates a complex context in terms of balance of power because in many cases the public entity with the public power, because the public authority has a power that is not the power that exists in the relations among private entities. They benefit from the government power. And in exercising their power they benefit from the collaboration by private entities that be in this partnership. And this creates a sort of imbalance. So to give an example, if there is your local government that wants to detect tax fraud and in doing that to recruit a company that provides an AI service software for this purpose, of course there is a mixture. Because you have to provide the data information to the authority. The authority has the power to give you a sanction but a large part of the job is done by a private entity that is not liable in terms of political liability like the government. So this mixture creates an increase in the balance of power, the complexity of the structure. Because for instance you can access to the decision of the public authority because it is a public authority so there is transparency. But you cannot access to the algorithm because it is a private company. You say oh no this is my interest, this is co 

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    845 tokens
-------------------------

6.

pyright protected or intellectual property protected. And so you have not the benefit of transparency of the public entity, you have the buy-in of the private company, but they work together and you have to obey because it is an exercise of the public duty at the end. So this complicates the balance of power. So in this sense, in this scenario, the idea that the best solution is simply to go to the court to ask for compensation for damages in case of harm is not the best solution because you can be not aware about the fact that there is an harm or you can be aware too late or it can be too complicated or there are people that are not able to pay a certain amount. For this reason, the risk-based approach, the idea to have an ex-sante approach based on the fact that the matter is not about the impatient eye and how to compensate or react in case of damages. But we have to prevent that and prevent what it means. It means that before putting in the market something, you have to assess that everything is okay. For this reason, cases like CHAPGPT are in contrast with this approach. Because the approach should be that you put on the market something, one is mature enough to be on the market. Until there are risks related to this technology, you cannot put that in the market. And this is the idea of a risk-based approach in regulation. The AI Act is deeply based on this risk-based approach. Risk-based approach means that the focus of regulation is minimized in the negative impact of AI through the analysis of the potential risk and to limit the application that entaches risk in order not to go to the market. If they are not properly addressed, the related risk. Of course, this approach, generally speaking, is much more effective because I prevent the risk. Of course, it should be also combined with the other one. I need also a general use in terms of liability for this reason we have two proposals. Because in the case that risk-revision doesn't work, you need to have liability, of course. And there is a problem in terms of the protection. In terms of AI, we did not discuss that but just mentioned. In terms of AI, there is a problem on liability because liability is much more complicated also in this kind of thing. Why? Because traditionally, product liability is quite easy and is based on the idea of channeling. Channeling means that the who provides you the product is liable for the product, regardless of the fact that some parts of the products are done by other parties, etc. In AI, it is much more complicated because if I give you a product that in many AI, smart cars or smart something, if there is a damage, who is liable? The product sometimes is on the hardware and the key point is represented by AI, but also a role of the sensor. So it is a bit more complicated than the traditional product in terms of the allocation of responsibilities. And so, of course, also in the proposal for AI liability, the approach is channeling the responsibility. But the problem is how to channeling the responsibility. Because if I give you a fridge, it is quite easy that who gives you the fridge is also liable for the part that is bought by third company. But if I give you a smart car, the car manufacturer to some extent may build only the structure of the car, but not the AI part and the AI part is the part that is most critical in order to manage the car. And in building the AI, it can be also possible that some key part in terms of software are done by other companies. Or that some sensors that are crucial are provided by other companies. So in this case, how far we can channel the responsibility on the final producer. So it is complicated. And for this reason, although it is not a lot debated, this regulation is, this directive on AI is also under discussion in Brussels. And a lot of doubts that can be approved before the end of this new parliament. So we also have a problem because on one end we will have the AI act to focus on the risk assessment management. But we miss 

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    846 tokens
-------------------------

7.

ed the second part of liability. So the two parts are very, very connected because the idea is that you are to address the risk and if you don't, there is liability. But if you have only the first part approved and not the second, the regulation is incomplete. And looking at the ongoing debate, I'm a lot of concern about the fact that the AI liability will be approved. So let's assume that this act is going on. So they already signed it and published it. And Car Manufacturers just checked all the terms released by this act and an accident happens. So in this case, this means that the manufacturer wants the liability because they already did all the risk assessments based on the act of AI. I hope so. No, the AI act is based on risk assessment and some texts of the AI act have been made publicly available. And the liability directive, also the proposal of the commission is publicly available. The position of the car manufacturers and the industry in general is in favor of regulation. Then in terms of lobbying, of course, they have to manage in order how to address the regulation. But the car manufacturers are not against AI regulation, I think. No, actually I was trying to ask the question that they just checked all the regulations and their product is perfectly fine. No, no, no, they don't check the regulations because the regulations are not the same. After the release of the regulations. If the regulation would be approved, the idea in the AI act that is a risk-based approach, we'll discuss that, but we can anticipate that the idea is to adopt a conformity approach. Conformity approach means that there are some requirements that you have to comply with these requirements. And they are accountable and show that you are compliant. If you are compliant, it's okay. And this may be your liability. If you are not compliant, there is liability. This is almost the approach. And this approach is something that the industry likes, you can say. Then of course, the requirement is a matter of discussion. But the idea of the industry is to have a list of requirements. And based on the list of requirements, know if you are liable or not, of course, is the situation. And the fact that the devil is in the taste because in AI, defining the requirements is very challenging. Because it is something that is evolving, is not a full match of technology. So it's very difficult to say, okay, this is the list to do in a smart car. Yes or no? So the conformity assessment, that is a key part of AI Act, works on this idea of conformity, that is typical on product liability, in which there is some specific technical requirement for the product, and you show that you are compliant. And for this reason, there is also the huge certification that you see. And they want to adopt the same for AI. The problem is that this is possible to a certain extent for some aspects. For instance, data quality, data security, etc. But for many other aspects, it's not so easy to manage. Because the variety of risk is very context dependent, technology dependent. So it's not easy to have a standard that fits for all the smart cars, for instance. And it is even more complicated if we consider the risk, also the risk for human rights and fundamental freedoms. Because this is very, very context-allocating. So this is something that we discuss in the IACTA approach. So for this reason, we have this approach based on ex-ant risk assessment, that is the IACTA, and exposed liability, that is AI liability directly. Please note that there are two different instruments, and also two different harmonizing instruments. Because the IACTA is an active means regulation, in terms of operational aspect, it means that it's the same in all the countries. Why the AI liability is a directive? It means that we see the implementation. And let's go on. In terms of the risk-based approach, the European bodies have stressed this fact that AI in Europe will be based on a risk-based approach. However, the risk-based approach is n 

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    805 tokens
-------------------------

8.

ot a monolithic approach. There are many nuances in the risk-based approach. And some of them also merge during the debate of the AI Act. In what sense? That when we mention the risk-based approach, it means that you focus on risk. Okay. But what it means focus on risk? Because risk is related to the negative impact of some application, but this impact can affect different kind of areas, different kind of sectors, we can say. So it means that risk can affect the individual rights, but can affect society, or can affect economical aspects or technology development. So what is the scope of the risk? Why I stress this point? Because it's a crucial point, because if you adopt a risk-based approach, it means that you assess the risk. In assessing the risk, you have to balance the impact with the protected interest. But if we have a very broad interpretation of risk assessment, it means that you have to consider the pros and cons of a technology, putting at the same level all the potential consequences. So what I want to conclude with it. The fact is that a pure risk benefit approach can also be shaped by the high evaluation in this way. This technology has an impact on individual rights, but it's good because it's efficient. But it's good because we save money. For instance, using the high in order to monitor citizens, in order to provide welfare state services, a very controlling system of AI can be justified because this saves a lot of money for taxpayers. A trade-off between invention, privacy and economical benefits. This is a risk-based assessment, the pure risk benefit assessment. And it's a risk-based approach. A risk-based approach in which all the potential values or interests are at the same level. One scenario. Another scenario is the risk-based approach, but a right-focused risk-based approach. That is the one that we already have in the GDPR. In the GDPR, when you carry out the impact assessment, you carry out the impact assessment, taking into account that the protection of fundamental rights is much more important than other interests. You recall the Google case, the Google Spain case, on the right to be forgotten, that says the pure economic interests cannot prevail over the protection of individual rights. It's again a risk-based approach. It's again a risk assessment, but a risk assessment in which the things are not all at the same level. But the protection of fundamental rights is much more important than other interests. So you cannot say, I use a very pervasive impact for AI applications that can limit some fundamental rights, because it's more efficient for the system, because I save money. Because saving money is not so important like protecting fundamental rights. So the final solution. In the risk assessment, how much room we live for the benefits? And which kind of benefits? Economic benefits again. So this is something that is still under discussion, because the recent amendment proposed for the AI Act, for instance, introduced this reference in the impact assessment to the benefits of AI. Of course, the benefits of AI without any definition, it means that any kind of benefits, including efficiency, technology development, saving money, etc. But this change, there is an assessment that we carry out. So regardless of my personal opinion, as you can imagine, in favor of a human rights approach, but the problem is that in terms of regulation, we have to decide which kind of approach we want to adopt. And not simply talking about risk focus or risk base is not enough, because this risk base can be shaped in different manner. Considering all the potential risks can make a general risk-benefit assessment that compensates everything, or inside the risk, consider that some interest should be more protected than other, and of course it's still a risk-based approach, but a different balance, we can say, a different value of the interest that you compare. So this is still something that is not solved in the AI Act, because the original appro 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    837 tokens
-------------------------

9.

ach was more close to the GDPR, so more risk-based approach centered on the protection of fundamental rights, but some amendments now open the door to a broader balance that includes also more general risk-benefit assessment. Another point of the debate about this risk approach, of course, is the room that we want to live for the different interests, we can say. Because when we focus on the risk, what is the risk that we consider? The risk in terms of safety and security, so the traditional industrial risk, the risk for fundamental rights, or the risk in terms of social, analytical, negative impacts. These are the three options that we can consider. And the AI Act excluded the last one. In the AI Act, as we mentioned, there is no reference to the impact on society and on ethics, but include both the previous one, so include the impact in terms of security and safety, and the impact in terms of fundamental rights. But if you read the text in the AI Act, the 80% of the text, or much more than the 80% of the text, is only on the typical safety and security assessment. So the notion of conformity assessment in the AI Act is mainly a problem of a system that have not to create a negative outcome in terms of negative impact on the safety of the system and on the security. And for fundamental rights, there is only few reference that say that should also not create prejudice with fundamental rights. But what is the problem? The problem is that fundamental rights impact assessment is something that is not developed enough in the theory. Why for safety and security? We have a lot of experience. All the matching regulations, cars, trains, cars, whatever you want, are based on safety rules. We have experience, we have standard bodies, we have a stronger experience in that. For fundamental rights, we have not. For fundamental rights, apart from data protection impact assessment, that is a sector-specific fundamental rights impact assessment, there is very, very few experience in terms of fundamental rights impact assessment. So it means that we don't know exactly how to carry out this impact assessment. And of course, in terms of regulation, this is an issue. Because if I ask a mandatory assessment, but it's not clear how to carry out the assessment, for the company that have to comply with the law, this is a problem. And in the AI Act, there is not any words about the methodology. They ask for the conformity assessment, they include in the conformity assessment the impact of fundamental rights, but there is no indication on how to carry out this impact assessment. Of course, also in the GDPR, there was not any indication in Article 35 how to carry out the data protection impact assessment, but it was different. Because in the GDPR, that comes in 2016, there was an experience of 30 or 40 years, in the country, and in this case, the privacy impact assessment was something that already exists since many years. So in the practice, there are many experts that know how to carry out an impact assessment. So why in 2016, the GDPR said you have to carry out the data protection impact assessment, it refers to a practice that already existed. So people know how to do that. Because if you look at the privacy impact assessment, something that was already used in the 80s. So there is an expertise, of course, it depends by the countries, the historical privacy impact assessments come from common law systems, but there was expertise. So in 2016, saying you need to carry out the data protection impact assessment is an implicit reference to a practice that already exists. In the AI Act, you have to carry out a fundamental rights impact assessment, it's a reference to something that, to an understanding, doesn't exist. So the people don't know how to carry out an impact assessment. And this is a critical part, a critical part, of course, of the risk assessment, because if the model is a risk assessment, but there is not a methodology to assess the risk, of course, you create an obli 

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    798 tokens
-------------------------

10.

gation, but you don't provide the instruments in order to comply with this obligation. So it is a very critical part of this regulation. And of course, we cannot line some general principles for the risk assessment that we already know in data protection and we find again in the AI Act. So according to the traditional risk analysis, we will have a private assessment, a collection of potential evidence about the risk, and the role of expert, and the circular assessment. So the mindset that we know in data protection impact assessment is something that we see also in the conformity assessment of the AI, but not because it's based on data protection, because it is the theory of risk assessment in general. So the circular approach, the preliminary analysis, the identification of the risk, the measure mitigation, the assessment of the measure, the final assessment, and the risk periodical assessment is the best practices in all the risk management systems, so it's not human. We have some models that can suggest something, ethical impact assessment, social impact assessment, humanized impact assessment. So in the theoretical world, and also in some applications, we have already some systems that deal with the issues that rise in AI, and we already have some models, for instance, the humanized impact assessment is something that already exists, but as we discussed, the humanized impact assessment as an existing practice is something that not fits perfectly with the AI context. It's designed for different kinds of contexts. It should be adapted, you can do that, but it's not as simple as an open-based. And the last point that is only partially addressed in between the line in the AI Act, is that when we discuss about the risk and the risk assessment, we have always to keep in mind that the fundamental requirement for the risk assessment is that you know the risk, and you know the impact of the risk. If the risk is unknown in its consequences, you cannot carry out any kind of impact assessment. So for this reason, in Europe and in general, there's a combination of two elements, the risk assessment and the precautionary principle. The precautionary principle is the principle that says that when a technology maintains some risk, and we know that there are risks, but we don't know exactly the extent of the risk, the potential consequences, the precautionary principle suggests to stop, to put the technology in the market, and to investigate more the technology. And this is an approach that we use, for instance, for OGM and other products. Before, permitting to use some products, when the level of risk is uncertain, we have to stop and investigate more, develop more. This is the idea. So the precautionary approach and the risk assessment are two different situations. We have a situation in which we know that some applications may contain some risks, but there is a lack of knowledge in the scientific context about all the potential consequences, about also the extent of these consequences. And so we cannot make an assessment, because if you don't know exactly what are all the consequences, you cannot make an assessment. And so the only solution is not to stop the technology, but to stop, to put the technology in the market. So you can continue to develop the technology, to investigate, to reach the level of maturity in which you know exactly the consequences, and you can carry out the impact assessment. But you have to combine both, because also in the eye, and this is reflected in the eye actor, there are some applications, for instance, in the field of emotional detection or other sector, which is not so clear what are the potential consequences, what are the potential biases of this application, etc. And it's much better to stop, to use this application until we don't know exactly what are the important, the impact, and we can't measure this impact. So the final point is the important combination between the precautionary principle that can suggest to stop some ap 

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    78 tokens
-------------------------

11.

plication and the risk assessment for the application that we can accept, because we know the risk, we can know that the risk can be high, but we can measure and assess the risk. So we have to combine these two approaches. Okay, I think that's a lot of topic for today, and we can stop and we continue with the analysis of the eye actor next time. 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



