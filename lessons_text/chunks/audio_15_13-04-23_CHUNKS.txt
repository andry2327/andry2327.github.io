Chunk #1    806 tokens
-------------------------

1.

 Okay, so I think that's your colleagues almost slowly are coming, so we can continue. So this was about the electrician. So my comments about the computer are another question about the case and the written update. No, okay. So we can go back to the ordinary business, I think I'm sorry. So during the last class we just discussed about the regulation and the proposal for the AI regulation and we start at the beginning of an introduction about the AI act. As I already told you, this is just an introduction to the AI act because the AI act is under discussion. And moreover, we don't know exactly the state of the art of the discussion because yesterday was a vote on some articles of some randomness in the parliament. So there is not a final effect in the parliament. Anyway, after the adoption of the effect in the parliament, we start with the so-called true law. It means that the tax will be negotiated between the parliament, European council and the commission. So there are changes to this tax. In order to adopt this document at the end of this year or in any case before the next election, if not, there is no room to have the tax for a long time probably because we have to wait in the new parliament and the parliament and the parliament and the risk is getting the entire work. So the most big issue in this presentation is that this is the final part. In the meantime, we have to mention that there are other proposals in terms of AI regulation by different countries, some initiatives from the US, other initiatives from, for instance, Brazil that propose a sort of regulation. Brazil, for instance, was proposed a proper draft and the draft was discussed. The first draft was partially rejected because there were some incestines, as you can say, in the draft. And the new one was created by a dark commission. And now it's under discussion in the parliament, but you know, in Brazil there has been a change in the political context, so we have to wait for the new parliament and the words of the parliament. And at the same time, in the US, we have one technical document on the impact of this bill, which is something that we discuss today. And then we have a proposal, a President proposal, a bill of rights for AI, but it is a blueprint for this bill. So it means that something that is proposed but is not a legislative act, made the first stage of a legislative act. For instance, we had a blueprint for digital rights some years ago under the Obama administration, but was not endorsed by the Congress. And so at the moment, you can say that has no impact. So we see probably, with regard to, there is much more emphasis. By the way, there is also quite strong proposal by President Proposal in the Congress on federal protection law that includes a section on AI. So something is moving also in the US. It's just due to provide a very quick, broad framework and at the regional level, there is the Council of Europe that is working on a framework convention. Also, this work was quite difficult, we can say. And now they have a draft that includes some input in terms of key principles and very ambitious part of risk assessment that would help include the many aspects including the impact on democracy and rule of law, so very difficult to be implemented in practice. So this is the showdown. I guess this is another what is the high of the artificial intelligence act. The AI on the artificial intelligence act for now is not to set the list of principles that should guide and drive the AI down. This is not in the act. The act is mainly focused on the risk and the manner of the risk regarding AI. This is quite consistent, we can say, with the first generation regulation of technology in which we focus on the main issue and the main issue typically is the potential negative impact of this new technology. Of course, it is not very consistent with the previous deba 

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    797 tokens
-------------------------

2.

te on the ethics of AI. We are going to debate on the fundamental rights in the European Union, starting from the chart of fundamental rights and the recent proposals of the directive on the respect of fundamental rights in corporate liability, etc. So why there is a clear trend also in the jurisprudence of the AI and corporate justice that is very important in emphasizing the role of fundamental rights? It's quite surprising that the AI actually is not specific reference about the general principles that endorse the European Union way in order to shape AI development and implementation. In this regard, there are some initiatives, for instance, in one of the committees of the European Parliament, it was proposed to introduce a list of key principles, terms of transparency, sustainability, privacy compliance, etc. But it's just a proposal from one committee that just today has been by-partisan and by the shadow reportor and by one of the advisor of the left parties in the parliament. So maybe it is possible that some principle will be added, but at the moment the structure of the AI actually mainly focuses on risk management and risk assessment. There is not a general introduction about the key principle like Article 5, for instance, in the GDPR and also other principles that you find, like privacy bar design, privacy bar default, etc., in the GDPR as well as in other legislation. So the focus is on the risk and with regard to the risk, we have to say that the approach is quite restrictive in the sense that the AI actually focuses on the high risk. The large majority of the AI focus on the high risk cases and there are only few provisions for the rest of the cases and only limited sector, there are specific obligations for the other page. But the focus is on the high risk case. So the notion of high risk is something that is familiar to you because we discussed about the high risk in Article 35 of the GDPR that refers to high risk as a barrier in terms of something that cannot be implemented if you are not able to mitigate the risk. So the first point that we have to underline is that the approach in the AI Act is different compared with the GDPR. While in the GDPR the AI risk is a pre-show, you cannot use data that can create high risk in terms of impact on fundamental rights. In the AI Act, the framework is different. You can add high risk applications. And in case of high risk application, you have to do your best in order to mitigate the potential risk to whatever that is according to the law acceptable. So the framework is different. In data protection there is not acceptability for high risk. High risk means that this application cannot be developed. We have seen in the application of Article 36 the only case in which we can use data in a situation of high risk is when there is another prevailing interest. And for instance we may get samples of the body scan that is invasive but is accepted because we are providing interest in terms of safety and security. But for the rest the idea is that this is a pre-show then we have to keep the application under this pre-show. In the AI Act it's not the same. In the AI Act the high risk is okay if it is acceptable. So it means that you have to mitigate to a certain extent if it is acceptable. I will dig about this notion. The second point is that why the GDPR, the idea is that the risk assessment is done by the data controller in cooperation with the data processor when necessary. But the risk assessment you can say is open in the sense that you evaluate each case and decide if this case or in this case the risk is high, low, medium, whatever you want. In the AI Act there is a list of cases in which the risk is considered as high. So it's not the result of the assessment of the AI developers or providers but it's the result of a sort of pre-assessment, general assessment by the data. Mor 

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    827 tokens
-------------------------

3.

e specifically in the Annex 3 there is this list of cases. And if we look at the list... Okay, well... If you look at this list, of course also this list is under discussion so some changes have been added. For instance, a list removed by a metric identification system, but in the point pre-apprentice we have the reference to AI systems in time to be used to evaluate the learning outcomes, including when those outcomes are used to secure the learning process, natural person, education, vocation, vocational training, institutional, or a program at all levels. So this application, this is the level of description of the AI risk application. So a very broad description. And the second point is that a description like that may include the same categories of AI risk but also categories that are not necessary AI risk. Because if I use the AI to evaluate the learning outcomes, it doesn't mean that they're necessary I give marks to use in AI, or I decide who passed the exam or passed the class to use in AI. But it can be only a support to the evaluation or something like that. And it can be also justified in the case of people or students with specific problems in terms of learning capacity, etc. So this is just an example of a category. The categories are so broad and the vital AI application and the way in which we can shape this application are so many, that it's very difficult to say that you are in or not in the category. For this reason, one of the proposals that is under discussion is also to consider this list as a list in which the applications that are listed there are presumed as AI risk. But you can't demonstrate that they are not of AI risk. So a more flexible list, you can say. But this is the change in terms of paradigm compared with the GDPR and other applications, as a rule in which there is the, yet there is a potential risk that assesses the risk. Here the risk is reassessed, we can say that by the legislator. It's the legislator that creates a list of various cases. And this list will be updated by the commission, very honestly, based on the new challenges or the changes. Of course, also this is the problem to a certain extent, because the list is the very core of the AI act. Because if we have say that the large majority of the provision of the act concern AI risk application, of course being in the list or not being in the list means be under the AI act or not in the AI act, or larger than we can say. So it's quite unusual, we can say, that the commission has the power at the end of the day to define the scope of the act. Because changing the activities, changing the application that are considered as AI risk, is the commission, the object. And the commission is not a body that have the full legislative power, we can say as you mentioned, is distributed a legislative power. The commission is much more close to an executive government body. So also in terms of democratic legitimacy, it's quite strange, we can say, that this huge power intent of shaping AI regulation is in the end, remains in the end of the commission. So this is the Annex Preet, and there are other categories to say that are in the job application, analyze and filter job applications. Of course there is a risk, but it depends by the filter, for instance. You can filter job application, asking AI to delist all the applications that have not some specific requirement. For instance, I'm interested in the people that have a PhD in engineering. Of course, the other PhD in philosophy are not in the list. And this kind of automatic selection, we cannot say that it's an impact processing operation. So the growth in the category is concerned in terms of the effectiveness and the usefulness of this kind of industry. So going back to the presentation. And the other question, that yes, it's one of these three questions, is enough to focus on AI risk. And this is  

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    824 tokens
-------------------------

4.

also part of the huge debate among the after. So in what sense? That the AI risk debate that includes also another category that I have not mentioned, that is the category that is in article 5, of course, remember, that are the banned categories. The AI applications that are forbidden. And so high that are not possible to be download. And we look at the app for just to give you some examples. So we are a group that the application that are not allowed. And they are risk application and the rest of the world. And the problem is how we can be that having a medium risk application is something that is not legally relevant. Why this question? Because it's true that using biometric identification in an open space able to capture you and to follow you in the crowd is quite an investment and quite a challenge. But these are very few AI risk applications. On the other hand, we have many other small medium risk applications that can be much more preventive in terms of users. So for instance, we may have many applications that suggest you some behavioral attitude about your health, about the way in which you select your food or whatever you want. And of course, this has a lower impact in terms of risk. But what it means is that global is there. So only one big high risk application of course right also. But also hundreds of medium applications of community impact are significant. In the theory of risk, this is something that is well known. You have no focus only on the big earthquake, but also small earthquake or two months creates a lot of problems. So the big event of course is the risk, but also medium advantage spread and in a large number can create an intense commutative effect the same result. This is quite common in the risk theory, but it seems to be missing this kind of approach in which we focus on the high risk. And what is not high risk is not relevant, although my effect means also people. And to a certain extent the debate on chat GPT provides as an example. Now the idea is to put the general AI in the list of our risk application. But also this can be right or can be wrong. Because for instance, I think I mentioned the case of the Portugal that wants to use chat GPT in order to provide very basic information regarding the bureaucratic process you can say. In this case, it's true that there's a general AI, but it's also true that in terms of potential risk, it's very, very limited. And on the other hand, we can say that an extensive use of chat GPT, for instance in the chat with the people, may entice a cumulative risk. And I've already some cases of people that chatting with a virtual companion could adopt wrong and risky behavior that affect them. And so we have some cases of suicide decision. So this is an application that per se can be considered not high risk, but the fact that it's so spread around, so frequently used can change. And I have to put time for the point out to the limitation of this very decodamic approach, high risk, no risk. Please. Yeah, we can take. Yeah, the definition of the system was very critical topic. For this reason, when I participated in the Council of Europe debate, I was very skeptical about the definition. But the problem is that according to the common law approach, the European Union used to provide the definition. It's not the only way. There are countries, continental countries like Italy, Spain, France, that in many cases do not use the definition. So there is another definition in the law of some general notion, meaning that the definition is the definition that exists in society. And the other is that the definition of AI is very challenging in both the way, because if you do not provide a definition, companies say, oh, but we cannot understand if we have to comply or not with the AI act. Of course. On the other hand, if you provide a definition of AI, the risk is that the limit in t 

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    740 tokens
-------------------------

5.

his definition will limit the scope of the law. The risk is that the evolution of AI is not fully described or catched by the definition. And this example of general AI, general AI was recently introduced after the chat GPT presentation in December was recently introduced in the I act before there was no any focus on general AI. It means it means that the change of the technology scenario or more correctly, the technology scenario has no value in its later because in the scientific context, the general AI was already known and cast. So the change of the technology scenario may require some changes in the framework, including the definition. The reason is very challenging definition about a technology that is not in fact, but we have a definition, of course. Of course, this is a definition that we had in the last document of the European Union Council. So it means that it's not that the definition that is now under discussion in the parliament because can be changed based on the amendment presented in the parliament. And do the fact that there's no transparency for the discussion is very difficult to say what is the state of the art the moment. But just have an idea, the definition was a system that is designed to operate with elements of autonomy and that based on a machine or human provided data and inputs in first how to archive a given set of objectives using machine learning and our logic and knowledge base approaches and produce a system generated such as content generated system predictions, recommendation of the teaching and influencing the environment with which the system interact. This is the definition. I think that we can open a broad discussion about the definition. The main definition, the strongest one. The definition provided by the OECD. That was also adopted by the UNESCO in the ethics guidelines on AI, recently adopted. And these articles free to and also the master management article three are quite aligned with the OECD definition. Based on the discussion of the definition there are some criticism because not everything inside there are also inside some application cannot be classified as AI etc. From a real perspective is very difficult to provide definition because you cannot provide definition based on the technology used. You cannot say machine learning or neural network because technology can change. So you have to provide a definition like this one that is more based on the goal we can say of the system, how is rich in this sense to say based on machine and or human provided data and inputs. How it works in first extract information using machine range learning and or logic and knowledge based on application. But of course in logic and knowledge based applications there is also something that is not necessary. So this is the problem. Is it okay? Yeah. But it's not the final definition. So you cannot base on that at the moment. But this is one of the quite basic topic of definition of the. Okay, so this is why the criticism about the approach. Of course, we have also to be aware that is very difficult in the first regulation of a new technology immediately find the perfect solution and reach the optimum in terms of results. So all the first generation technology regulation are always quite limited in terms of status. So I'm not surprised about the fact that there are some contradiction about this idea of risk distinction between Irish and the other risk classification or reconsider. It's normal. Now that we can say that is not the result of a strange design. So by the later part is the result of a compromise between regulation and business industry interest. Of course, a system in which there is a closed list of almost closed list of Irish application is much more appreciated by the companies than a system in which you have to assess the risk every time with the consequence to begin or out with regard to  

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    788 tokens
-------------------------

6.

application. With this system, you can say, oh, I mean, oh, I am now. So it's much easier for the company to know if they are under or not. If you use a system based on the level of risk is only shorter company, but since the risk and the consequence decided to comply or not. So it's evident that in the approach that are already mentioned in the previous classes, the top touch of the you based on the fact that there's not a strong industry. Of course, it's important not to create an excessive burden on the shoulder of the companies. And so in this sense, this solution based on a list of Irish categories are listed for the application center is much more manageable for the company. At least in a stage also in which as we have seen with the church, we did. There's a lot of debate about the new application. The new application can be good, can be bad, can be some effect or can be unknown. We have not discussing about the match of technology. We are discussing about the technology that is performing a very significant evolution in this recent period. So for this reason, we can also conclude that from a regular perspective is not so bad in terms of approach. Although, of course, it's not so satisfactory in terms of fully addressing all the potential risk. But this is the trade off that we have paid. Okay, in terms of risk and the Irish, the risk that is considered in general by the IAC is the risk in terms of safety and security and the risk and sense of the impact on fundamental rights. So it's a strange combination we can say of traditional industry regulation. For instance, product regulation, machine regulation and theater that is very focused on safety and security. And much familiar feel the brass data protection and the other Irish in which the focus is on individuals and their rights and their protection. So they try to combine this to so that to be more specific that the first graph that was proposed by the digital market and was very focused on safety and security, because this is the background of the market. And then they add the fundamental rights. And the result is a strange proposal, a strange draft of the act, because the 80% is focused on security and safety. And then there is also the add on and fundamental rights. But what was safety and security is when provided by the act for initial standardization, conformity assessment, accountability documents that you have to prepare to keep. It's quite normal and quite usual in terms of safety and security and all the system if you look at the system like train cyber etc. They adopt this kind of approach is in terms of address mitigate and monitor the risk for the fundamental rights. There is no one. There is no idea about how to extend the two fundamental rights and this kind of risk management, because there is not a lot of practice in this field and they are actually have no any specific inputs in this regard. They simply asked to carry out also the assessment including the impact of fundamental rights, but no work on how to carry out the assessment, how to show compliance, etc. So this is the most critical point that the commission mission. Try to solve saying that out this is something that we done by the standard body. And the commission asked the standard body to reverse standards including the standard for the impact of the right. Of course, the problem is that standard bodies are usually to kind of cable phones computer, not fundamental rights. So, and this is evident because in the draft request of the commission to the standard body, it is a specific paragraph in which the commission say you have to add that to your confidence specific confidence of fundamental rights, including experts in this field. It's quite strange asking to a standard body, you have to standardize something, but the same time say, but you have not enough knowledge to meet with someone that ca 

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    795 tokens
-------------------------

7.

nnot hear you in there. But it is not exactly what is expected, but that's it. Okay. So, we have the high risk. So, there are several cases, this is in the, the a actor. The cases in which we already apply safety law. Someone are already integrated on the new legislative framework. And they are excluded from the application of the AI, but for this two categories, there's not a big issue because our category that order out or regulating them so safety and security. And in this case, there's only the add on for the impact on the fundamental rights. The very problematic category is the category of the standard on AI. So the system in which the focus is on AI and the other system is in article three. So why the first two are cases in which AI is part of a system. For instance, I use AI to better the monitoring of the speed of a train. So this is under trying regulation, we can say is under this kind of already existing regulation of safety and security, because AI is seen as only as a component of a broader regulated product or service. But in the standalone, a new services based on AI that are not already regulated under other regulation, capital, simple security. And so the AI is the only part that is applicable to that. So this is a combination. AI as a part of a regulated instrument. In this case, we apply the rules in terms of conformity that we are in that area. And we have only what is best, most specifically, the potential impact on fundamental rights, but maybe also not having any impact on fundamental rights. And then we have the standalone that are completely unregulated ideal in which we have a specific regulation only to the AI and that those that are in the next. Okay. The red lines are the cases that are forbidden. Let's get quick look to this case because we already mentioned a couple of times that that's the case. The final four P is the new article proposed in order to regulate the general AI, general process the AI system. Unfortunately, it's quite poor because the core of the article is we will provide a new regulation specifically for this meeting. And this is this one to prevent the AI all specific and the identification agreements that are general about. So it's not providing a lot of information in regards to the party. But it's consistent about the fact that they have not the idea about the. And this is article five on private applications. So believe me now techniques. System that has brought vulnerability about specific group of person, this is a very specific social economic situation with objective material distressing behavior or personal pertaining as a group. Social behavior or normal or predictive personal personal characteristics. So I mean social credit scoring. With some limitation, we can say when it uses for the treatment of our people, so the natural personal group. Or. And just the father just proportionate the social behavior. The user real time remove from identification system public spaces. But again, there are some sections. This is a super rule for me. And then get. These are the main categories as in this document that they are proposed in order to extend the document. So as you can see the purpose is on social credit scoring to bring down techniques and. And vulnerable. Okay. Of course, to bring out techniques and also. One. And those are not the article on X free. There is reference to emotional detection. So we can also say that. It's a bit critical to introduce in the IAC reference to subliminal techniques and emotional detection. Consider the state of the art of the debate on this topic. So for instance, with regard to emotional detection, there is a large part of the literature that say that the emotional detection per se is not possible. Because the motion are very subjective or cultural basis. So it's very difficult to have an emotional detection without including bias or risk of  

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    797 tokens
-------------------------

8.

misqualification. There is a significant literature that is again the possibility to have a passive emotional detection. And the same is for the subliminal techniques that also are not so well. Can stay supported. They are effectiveness by scientific literature. So. Regulation are also political act. So sometimes I'm concerned some threats are transposed in the regulation, although from a scientific perspective would be better to say that this is not possible. This is forbidden because there's not any legal ground, any scientific ground to justify this kind of application. Rather than that, he is not admitted. It's forbidden because in case an I always we are made more than an iris in some cases, in my opinion, there is an iris go about not send the result rather than an impact. The impact, the regional impact is not the iris. The impact is the lack of scientific ground with this kind of application. So this is also part of the debate. So that was back to, sorry for going back and forward, but it's important for the other also because the art is very long. There is no possibility to do that. Okay, so we have the forbidden categories, article 5. We have the categories that are already in the list of iris. The fact that there are systems that are considered by other rational virus system. And then we have the case of unexplained that are the cases in which we consider that there is an iris. Of course, the fact that you are in a meeting with them, the air is applicable. But that means that this iris should stay at least. It means that it was human as it is on our risk to demonstrate actually this iris and how to reduce your risk in order to be acceptable. This is getting about the acceptability. Unfortunately, this is an act. We don't know the case law in the application of this act because what it means acceptable. For instance, from a human rights perspective, any kind of treatment that includes a prejudice to human rights should not be acceptable. So we can say that in any case in which there is a limitation of fundamental rights, this means that the application is not acceptable. But this is not clear in the text because the text say that there is an iris that is mitigated to whatever that is acceptable. But what is acceptable is not clear with the status in the IR. Of course, the IR, like all the IR regulations are active, are pieces of law. We need to provide interpretation and usually only after some years we are able to have a clear idea of the meaning of each part of the regulations. So for this notion of accessibility, for instance, is expected to have some cases that will clarify the burden of this kind of acceptability. What is acceptable or not according to which criteria is acceptable or not. I just recall what we have seen a few lessons ago about the iris-paising Act of 135. You remember that the European Data Protection Board provides some criteria in order to add other cases that according to the board are iris. So interpretation of course is of health because you better understand the meaning of the text that usually are quite short. These are the criteria for the IR assessment. So we can see that the purpose, the standard which is based on the IR to be used, the standard which uses systems that already carries harm, whatever impacts the potential extent of harm, dependence on potential effect and person, reversibility, they are limited by the right. So it is a typical risk assessment. Risk assessment based on what did a system or similar system already write in terms of terms. Based on the potential effect of people, how many, which kind of people, vulnerable, vulnerable, etc. And also how much is possible to reverse the count of them. So the risk is not only measuring the level of impact of ability and gravity, but also the possibility to react, we can say, to mitigate. This is normal in the risk theory because of 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    793 tokens
-------------------------

9.

 course may have a quite significant risk but quite easy answer to the risk. So for instance, I am in a kitchen, I am a chef, I am cooking, of course there is fire, but I can add water or other tools that can start the fire immediately if there is something that is not a usual business. So the context of course can be better words according to the situation and this affects the level of risk. Here are some considerations that we already made about the flexibility of an analysis and the decentralized approach. So we have to stress the fact that in a closed list this reduces the flexibility and reduces also the role of the manufacturer, we can say, that cannot decide the level of risk and the fact that it is a very top-down approach. In terms of structure, the structure is based on the conformity assessment. There is a specific article on the conformity assessment and the conformity assessment means that we have to track the safety and security and the impact on the mental right. The same approach that we have seen in the GDPR, it means that it is not only a matter of liability but it is also a matter of accountability. Accountability means that we have to be able to show that we have proper address to risk. So we have assessed the risk, have done the mitigation measure and checked the effectiveness of this measure. Something that we already know because of the same data protection practice assessment. And the solution is, of course, that we can eliminate the risk or we can reduce the risk. In this sense, the act is not clear because we move in the risk, it is not like reducing risk, it has two different strategies. But this is related to this flexible categories notion of acceptability. So we can imagine that some risks that can be easily removed and so, in this case, the request is to be removed. Other risks that are more difficult to be removed, for instance, if you consider strategy-based, but it is difficult to remove some risks concerning the errors in terms of hallucination, etc. Also in the future version, we can mitigate this one but we cannot exclude it. So this is an example in which we could say, oh, but this is acceptable because we can inform the people about the fact that this works but sometimes makes some mistakes. By the way, it is something that we already have in many applications. Also when we use an instrument in order to measure something, there is a margin of error that is declared according to the natural instruments that we use. So it is not unusual. What is your problem? No problem? And mitigation control measures in relation to the risk that cannot be eliminated. So the idea is if you eliminate the OK, if you don't eliminate the adopt and mitigation measures, or re-adapt any system in order to control or to limit the potential impact of the risk. And the other point that is not clear is the idea of providing relevant information. So if you look at the three strategies, the reduction of removal of the risk, mitigation and information are quite different in terms of impact. Because providing information is something that we already know in product liability and this information in which you say, oh, this is something that is dangerous for you. For instance, if you buy something like, I don't know what, a phone, in the instruction that you receive, this is written that under certain conditions, the battery can create a problem and explosion or fire, something like that. So are very limited cases, but it's possible and also happens. So it is an information. This is a way that we already know in terms of product safety, the idea that when there is a risk that is quite limited and cannot eliminate this risk, we inform you about the risk and how to prevent, we can say, the situation in which the risk can happen. Of course, all this stuff works well when we discuss about safety and security. When we discuss about 

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    822 tokens
-------------------------

10.

 fundamental rights, it's a bit more difficult because if I adopt an AI system used to provide social aid to the people based on the information that I'm going to provide as about the condition, of course, you cannot imagine that I can say, oh, but I inform you that sometimes our system doesn't work well, so you probably cannot receive the AI, although you are entitled to. It's not the way which I mentioned this situation. So of course, according to the different situation, in fact, we can imagine that this mitigation strategy cannot be all used. And so again, it's important to have a balancing test between the impact of rights and other impactive interests, if you want. So for instance, if I use AI in a class to give the students a tool for self-assessment about the knowledge in a specific topic, for instance, in the future, for testing your knowledge before the exam, you can have self-assessment to AI that use some questions, etc. In that case, you can imagine that I can use the last option, provide information. So I can say, oh yeah, this kind of assessment tool sometimes has some problem that provides you a not correct evaluation, and so please keep taking into account the fact that this is a potential limit. The impact is quite limited because it's a self-assessment tool and it doesn't change your life, I've got to say. But the same thing cannot be used, for instance, if AI is using an environment, because I cannot say, oh yes, you make it a time, but sometimes the marks are random. The information cannot solve the problem because the impact is much more significant. So this free level cannot, this strategy cannot be imagined to be at the same level. It depends, but in fact, right, in some cases you can use one or the other, or three, but in other cases you can use only one. In terms of gravity, of course, when the gravity is quite large in terms of impact, you have to eliminate or reduce, at least, the risk. One impact is lower, we can say, in terms of frequency, in terms of consequence, you can also imagine that other types of business are all information. So this is a sort of a different range of terms or solutions. Yes, with regard to the risk assessment, of course, there are some concerns. Leaving aside the fact that it's applicable to only the high risk cases, so for the other cases there is no indication of the carry out impact assessment. Yet with the reference data in the other cases, we'd be adopted, we'd be a soft law approach, based on cause of conduct, etc., etc., but you know that there are not a much effective solution. The other point is that this conformity assessment is considered as a unique solution that combines every kind of risk. And this is a mistake, in terms of maturity of perspective, because assessing the impact in terms of safety and security is not assessing the risk in terms of the right to right. The technique that we use to assess the risk of difference. So it's quite strange that in the articles of the IAC, these two categories of risk are put together as it regards the result of the same assessing process, why the others are a very different kind of assessing process, because for impact assessment with regard to security and safety, we focus more, for instance, on the technical side. We focus more on the functioning, we focus more on the traditional, we can say, product safety logic and the solution. Why fundamental rights is much more flexible and varied, and they need a different kind of approach, much more contextual based and not disstandardized, because each kind of application may have different kind of impact, etc. So combining this in the same so-called conformity assessment, not adequately point out the complexity of the assessment. The other thing that I've already mentioned, and the key point is that we have not a model to carry out this impact assessment. And this of course is a  

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    814 tokens
-------------------------

11.

concern, because if I have to carry out the conformity assessment, but I'm not any model to carry out an assessment for fundamental rights, because there are not a lot of experience in this field, we can say, it's very difficult to comply with the act. And the fact that the commission say, oh, wow, this is a matter that we solve by standard, by standard body, it's not a very effective answer. Also because in the proposal to transfer this company to the standard body, they say they have time till 2035 in order to issue this standard. So it means that we are gap between the adoption of the AI and the standard. Of course the AI will be adopted where it's not immediately effective, we will wait for a couple of years as usual. But of course it's quite strange having a very focused on a risk assessment and not having in the document in the AI any kind of input in terms of methodology. I'm referring to something that's out. Yeah, for the no AI risk, there are some limited application in terms of some limited obligation in terms of transparency. And then as I mentioned, soft go, cause of conduct, voluntary assessment, etc. If you want to look at the cases. Okay. So we are at the transparency obligation. As you can say, are limited to some specific application. So for instance, if the AI call you in order to provide some services by phone, etc. If you call a person that call you about an AI, you should be informed that they are talking with an AI. Or if you call the municipality in order to have information about your position is there is no longer an officer about this and AI that told you you should be informed that this and AI. So this is the obligation that's of transparency. Unless it is all these. So biometric categorization system should be shown in form of the operation of the system, the natural presence both you do. This is about biometric. So using emotional recognition system showing from the operation of the system, the natural presence both they are doing. So I should inform you about the emotional recognition system, but I don't want to recall what they already said about the limitation. Okay. So this is the system that generates a manipulated images audio. This is the fake news or fake, whatever you want. That now is possible quite easy with AI. So I can create some public figure that makes statements that they have never been etc. And the idea is that for this deep fake to be disclosed that is a deep fake. Of course, all this is based on a factor of sanction if you are not provided. So I mean, it's then this fake without any slurge about it is like fake to deny sanction. This is the system because of course, so that we did fake not say this is a big fake if this is done for misinformation. It is the joke of the cable because misinformation they over know any interest so that the provision works because of the sanction. And Okay. So this is the other part. For the rest, we can say that to the system. We can say that the system is largely based on. The system is largely based on the idea of self assessment or certain time for some application. The basis of a certain for other application is required to have a sponsor certification. But although there are a lot of provision about the certification certification body blah blah blah. And there are some provisions that say that the eight standards will be provided by the European Union. The adoption of this standard is like the certification. So the idea. Also this part is a sort of. In transition phase we can say that regulation because there are elements of self self assessment. There are elements of the certification and there are elements of conversation. What is this variety. Basically because we don't know when we will be able to have standards of course. And until we have not standards we need to have a system that provides some safety and security and the system. So in t 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



Chunk #12    514 tokens
-------------------------

12.

he absence of standards we need someone that certifies the system. But then when we are we will have the standard application of standards we replace the certification by the one. So for this reason there are these two systems in the regulation standard and certification. And for some specific categories there is also the possibility to have a self assessment. Like the GPR or the RCOG. The other approach is the Council of Europe about the already mentioned of course the differences. The Council of Europe is more focused on fundamental rights. It's not a treaty. The ambition is to create a convention for something that will not be effective. It's not implemented by the number six. So it's a different kind of approach. So it may just be a fact. We end at seven. Yeah. This is a recap of what we already said about the different kind of categories. And about the different kind of risk. Industrial risk and societal risk. Of course the societal risk in terms of impact on society on social values. If you want to know where the certain kind of AI that shape our society, our city, the center is not part of the AI. The AI actually not included is broader view about the social values or the ethical values that was part of the previous debate on the data analytics and the AI. It's very focused on the risk and it's very focused on the risk related to the product of AI. And not a broader perspective in terms of the first question that we have to ask if you want AI that is useful for some purpose or not. Of course now we shift to the assessment. And we continue in the classes about the assessment just briefly to say that in the assessment that we have in the AI act like in the DVR is an assessment that is carried out by those that create the AI. But the idea to have a participatory approach in the assessment like the stakeholder engagement in the article 25 is not very, very relevant. It's not one of the key elements. We see in the future but for now there is not the idea to have this participatory approach to the assessment. That in many cases is very useful to better understand the potential impact of the AI. But still the idea that the assessment is mainly carried out inside by the company or by the act. So there is a limitation. Okay, I don't want to add more because I close the test. And we continue next class. Next class I want to briefly conclude on AI. I think it's the last class that we have together. And so. 

- * - *  - * - * - *  - * END CHUNK 12 - * - *  - * - * - *  - * - * - *  - *



