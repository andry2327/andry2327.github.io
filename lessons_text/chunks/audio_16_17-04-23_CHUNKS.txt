Chunk #1    779 tokens
-------------------------

1.

 The risk assessment is that you have to consider two elements, the likelihood and the severity. Unfortunately in these proposals sometimes they consider only the likelihood and not the severity. Or they consider the severity in a very strange way, for instance considering that the severity is something that you measure in terms of number or impact of people. I severity is if more one million. It doesn't make sense because it depends by the cluster that is potentially affected. If my system can potentially discriminate people with a specific disease and the people with a specific disease are only one thousand in my country, I've never reached one million for sure. But I cover all the people, so the impact is the maximum that's on him. So it's very stupid that we have an absolute number as a pre-show when our evaluation is contextual. And this is something that is well known in risk assessment, but you can read in this very authority that you can read, but this is passing mistakes. So the basis is quite immature we can say. But these are the two elements, the likelihood and severity. These two elements are subdivided in other two elements and the likelihood we have to consider two aspects. The chemical notion of likelihood, so the probability, how much is probable that some negative impact happens. But also the exposure, how many people can be affected. And the second point in terms of severity we have to consider the gravity of prejudice in terms of fundamental rights, the gravity of prejudice. So for instance, limit the access to school using an AI system or screening. And create another prejudice because the limit of the access to education compared to the fact that the user AI is cool to assess and provide marks is another prejudice. Because one thing is not having access to school, other things is having some bias in evaluation. And the prejudice is lower if the AI is used not for evaluation, for instance, but for interaction or educational purpose but without an outcome in terms of evaluation. So the prejudice is contextual and evaluated, we have to consider how the single rights of freedom is impacted. This is another important point that we also discussed further. There is not a cumulative impact. You cannot say from a fundamental perspective that your tool has a high, low or medium impact. Because the impact refers to each specific rights. You cannot balance. You cannot say, oh, you created with your camera that control people in the street. Of course, it's quite embarrassing that some privacy, but there is not a lot of impact in terms of, you know, deformation. So we can compensate. Each rights, each freedom should be impacted. And the impact should be considered in autonomous mind. There's not a mix. You cannot put together different kind of rights and compensate. The fact that they affect more your privacy and less another rights does not change the fact that your privacy is significantly affected. So from a fundamental perspective, if only one rights is highly impacted, this creates a system that has problems in terms of fundamental rights. There is not a compensation. It's not possible in terms of compensation because these rights are protected from the mental. It's possible not to compensate, but a balance of interest, but not in terms of your effective rights, between your rights and other interests. For interests, crime control, security, safety, etc. You can justify some limitation of your privacy. But it's not a balance between your interests. It's a balance between your interests and other interests. This is different. This is something that we are already seeing in the other protection. So, intensive severity. The gravity is important. But it's also important to consider the effort to overcome the prejudice. This is another important aspect. In many cases, the prejudice can be significant, but it's quite easy to address it. For instance, I can create a virtual companion for kids. The virtual companion based on charge 

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    887 tokens
-------------------------

2.

ability can say things that are not so good for kids. But if the virtual companion is used in a class, in a school, the presence of the teacher can monitor the dialogue and can easily address any kind of prejudice, the sense of interaction between kids and robots. So, the context can limit the situation and that's the risk. Of course, the situation is completely different if this companion robot is used by the kids alone at all. So, the effort is higher in that case. So, we have to combine these different variables. To combine these different variables, we typically use scales. As you should know, it's important to have a scale that does not make possible to say in the middle, as I say. So, usually, it's better to say 4, 6 rather than 3 or 5. People will always see the middle. And the scale in this case is not a mathematical scale. It's impossible to measure the impact in the case of the prejudice of fundamental rights. You can only assess if it's high, very high, medium or low. It's not a matter of what the question cannot be so granular. This assessment, of course, is based on what? It's based on knowledge. It's based on someone that has the knowledge of human rights, of the human rights jurisprudence, and can, based on this knowledge, assess the specific application, how much impact that's right. So, is an expert-based assessment... It's quite normal that the protection practice is a lab-based assessment. Environmental practice assessment is an expert-based assessment. A large part of the environmental assessment is based on experts. Because if you don't have a very simple situation, like the threshold of pollution in a river, if I have the chemical component and the maximum threshold, I don't need an expert. That is something that's very easy to assess. But when you assess complex systems, you need an expert. It's not something that we solve in a mathematical way, medium. So, some of that you have to consider. Of course, being an expert evaluation, as usual, writes problems in terms of selection of an expert, what do I expect, what are my decisions. Of course, this exists, but it's not new. Again, it's typically not an assessment, an ethical assessment, or a legal or technical assessment. Also, the court, the judge at the end of the day, is based on an expert system, and math can make mistakes. Of course, for this reason, we have three different degrees in the decisions. So, it's an expert system, expert-based evaluation, sorry. And in terms of number, you see here, here is a number, this number is cardinal. So, I'm not to the result of a sum of modifications, but the result of weights, we can say, that based on the contextual situation, you consider. So, if you consider, for instance, the kids that talk with the computer, you can say, oh, if the interaction is in the school, the probability of the risk can be medium, because the charity has a lot of problems in terms of controlling content, so it's medium. And this pressure, in terms of how the impact, in terms of population impact, it can be on medium or high, it depends on how many people use the product. And then you combine this situation. But if it were not CHEPGTT, where another robot companion with a very basic information, much more controlled information, for instance, you can say, no, it's low in terms of probability, because the contents are very, very monitor, and so, of course, the combination is different. But this weight is the result of a specific evaluation, it's not something that is the same in all the scenarios. It's based on the expert to combine each time these values. We have the same for the gravity, of course, and the other values. So, four values, two metrics, and then the final one, in which we combine the results. It's not to son you, but it's the only way to address this topic. Unfortunately, in the legal debate, this is the only model that was presented in terms of using this kind of more logic approach, if you want. The large part of the other models  

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    839 tokens
-------------------------

3.

are very general. They are very focused on the question. So, how many rights do you think that can be impacted? Which kind of rights? How do you come to address this issue? And that's good in terms of exploring the potential impact. But the fact is that then you have to a certain way to demonstrate what is the level and how you can mitigate it. And simply asking questions about what is potential impact rights is not a problem. So, the questioner-based approach is very useful in order to create awareness in the designer, and in order to make people that are not expert in the field to understand what are the topics that they have to consider. But at a certain point, we have to assess. At a certain point, we have to decide how much is the risk related to the application with regard to different kinds of potential impacted rights. And this is possible only through this kind of tools. And in this sense, the result, of course, is a sort of radiograph in which you have the different potential impacted rights and the level of the impact on issues. And this is exactly what you need in terms of design. Because if you have that radiograph, you can understand why we have to start in order to mitigate it. Because, of course, if one rights is more impacted than another, here is exactly why you have to start to mitigate it. So, in terms of counter-indicates and in terms of design, this is very useful in order to better understand how to interact with the system. Because if I have one solution that has a very minimal impact on one rights, this is not my problem. But it's a high impact for instance on price and the potential. And in this graph, of course, I have to start from there. And what is important is that you have the impact for each single rights. You have the overall impact that is this one. And then you have the mitigation measure. Like in that potential impact assessment, you have to define how to mitigate the potential risk. In order to mitigate the potential risk, at that time you have also to consider the risk that you have not to mitigate. It was sense. There are some risks, some impact that are justified, for instance, by the law. A case in which the device should be done in a certain way because it's the law that requires you to provide this kind of design. So if it's a monitor requirement, although it might impact on individuals, this is not the problem because it's the law that authorizes this kind of design. Another issue is when there is an impact on some federated rights, but the impact is justified. And exactly the case of plan control, et cetera. Of course, when you use a camera with the Fisher Recommission and continuous monitoring the crew, of course there's a nine impact on individual rights in terms of dignity, in terms of freedom, in terms of social interaction, whatever you want. But to a certain extent it justifies the fact that there's a criminal area, there's an IRA's, a cathedral. So this is not a mitigation measure. This is not the assessment of the impact. This is a combination between other competing interests that you have to consider. Because other competing interests can make possible to use something that has still a nine impact, but it's justified by the presence of other federated interests. And after this exercise, when you have identified a mitigation measure, hopefully you are able to reduce the impact. The result should be this one, in which you have a different kind of radiograph that show the fact that you have reduced the impact. And of course this is quite useful in terms of the impact assessment and accountability. Because the IAC test used to be accountable. So we are accountable, it means that you have to demonstrate the entire process that lead you to the final design of your product. And in this way you are able to show what was the set report, how you react and what is the final outcome. And if you have not this kind of tracking of the entire design process, it's very difficult to demonstrate that 

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    817 tokens
-------------------------

4.

 your solution is the best solution, in order to address the potential negative impact of your device. So of course this is a methodology, it is one of the possible methodologies. But unfortunately for now there are no others that are so thoughtless we can say on the structure of risk assessment. Another important point that I want to briefly address before concluding on our evaluation is that it's not only a matter of data protection, fundamental rights, but it's also a matter of ethics and social value. The debate on AI ethics or digital ethics that we had some years ago, and that was talked with the creation of the AIR proposal with this new phase more focus on regulation. It's something that we should not miss. So the fact that now we are discussing about the AIR doesn't mean that the ethics of AI is no longer relevant. Because they address different kind of issues. AIR is useful in terms of security, safety, hopefully impact of fundamental rights, but it doesn't address the key question that is about ethical and social value AI. What it means, so ethical and social value AI, it means if we want to adopt some AI applications, if we want to adopt the impact that these applications have on society, it's possible to have AI in schools, it's possible to use AI to evaluate students. They really want this kind of approach. They really want that it's no longer a professor but an AI that evaluate the performance of the students. It's not a matter of fundamental rights, it can also cope with fundamental rights. You can also find a way that is not discriminatory, that is not bias, it's not impacting on dignity, etc. But is that the way of society that we want? This is the question. What's the same that we discussed about the charge-gbd? It's not only a matter of bias, etc. or privacy. It's a matter of access information. Do you really want access information to oracles that give you the truth without having any disclosure about the sources, how they manage the sources and how they reach that result? This changes a lot the terms of access to information and knowledge. So this is a radical question that we have with regard to AI and foreign general technology. It's not a matter of... when we talk about the edits and societal value, it's about the relationship between technology and society. So how we want to shape our society using the technology. And it's not new. Now it's about AI but before was about biomedicine and all the experimentation on genomic experimentation, etc. In this field, for instance, in Europe, we have made some decision. We decided that genomic analysis, experimentation have some limits in order to protect human life and humanity if you want. And the same in agriculture, we stopped with some technology because we consider that the risk is a whole saga, etc. So this is a broader topic about how technology can shape society and how we can address this. In this sense, the ethical debate provides a lot of guidelines on AI and the user of AI. Because the guidelines, of course, are an instrument that may possible to better design. The limits of the guidelines are that in many cases focus on general principles. They outline a key principle like human-centric approach, sustainability, transparency, explicability. And we all agree that they are very good principles. But the matter is how to prevent that. Because why they say that in design AI should be transparent. What do you mean transparent? Transparency means that the algorithm should be available, that the training data set should be available. What is the level of transparency that you want? And the same is for the rest. So in many cases, the ethical charges, including some proposal that now someone wants to add to AI acts, the definition of this value is so general that to leave a lot of rooms in terms of interpretation and application, and it's not clear how you set the threshold. So it can be sustainable, transparent, explicable in many different ways, and are not the same  

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    813 tokens
-------------------------

5.

in terms of binary results. Another approach, as I mentioned, was based on questionnaires, based on detecting what are the values that exist in a community. So in order to understand if AI is consistent with a community value, we ask the community to understand the values. We ask what AI we want. This is based on participational debacle, it's an interesting exercise. Of course, we also recognize that it can be used in order to manipulate the results, because we will make the question in a certain way also affect the answer. If you don't ask something, you have no answer about that topic. So the civic participation in order to detect the values that should be embedded in AI products can be also very manipulative if you want, because it depends by which question, which people engage, how they engage, the people, etc. and the result can be different. And this is something that we have seen in some large scale project of Mars cities, in which they make a lot of city engagement, but about very far from where we ended to what was their goal. And so it's more a legitimate exercise than a proper analysis of values. Roller expert in order to detect the values is something that they have just mentioned before. And how is the real life in AI developers? How do AI developers deal with this societal impact of their technology? Based on empirical studies of the big cooperation, we can say that there are three main groups. There is a first group of companies that have created the ethical charter, that have created the ethics committees, and that provide some transparency about how these tools interact with AI management. So give us some information about how the decision of the ethics committee have impacted on their business. And these are different cases, but there are. For instance, one is the Facebook oversight board, that we know is used in order to monitor more the critical cases in Facebook. Most of all, most of all, one was Trump, Ban, etc. Of course, this is an example in which we know that there is a board, we know the member of the board, we know the decision of the board, and Facebook is committed to respect this decision. So the test of transplants and ethical impact is quite significant, but we have also to look at the test. The oversight board have the task only to monitor the decision about stopping or moderating the same general content. The oversight board cannot make any decision about the entire structure, about how Facebook works, about the fact that in Facebook some content are prioritized over other, about the fact that this kind of approach, in some cases, could be used to spread the system. The very core of Facebook is not under discussion. So in terms of ethics and ethical approaches to AI, this reveals a strong limitation of a system that is transparent, that is effective, so simultaneously, but never discuss the core points. So the company creates an ethics board, but limits the ISA activity in order not to create problems with the core value and the core business. A different approach we use for ISA is by Axon, Axon produces arms, classical, and Axon adopts a board that issue non-binding recommendations, but based on the information provided by the public available, since they follow the recommendation from the board, and it's more specific on all the products, and it sometimes suggests to reconsider the design of the product. What is interesting in this model is that they have two almost persons that are inside the company, and may be possible to the designer to interact and to raise questions, and that is also in a anonymous way, to the almost person. This is an important point in terms of the application, in terms of operability of ethics in companies, because in a way, company creates an ethics board, usually with very well-known experts, that they meet two, three times per year, discuss very high level products, and that's it. But in day by day, with the developers of the I, there is nobody that supports them in order 

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    833 tokens
-------------------------

6.

 to give ethical advice about the specific product. So in this sense, it's very important to have a sort of connection, and also sort of constant monitoring of ethical issues, whatever. For this reason, the idea is to have someone in the company who you can, which are referred to, in order to discuss and to raise questions about ethics, and this person can also create a bridge between your concern and the decision on the board, is very important. Because it fostered a connection between what is that, involved to the company culture, and an effective ethical advice. Another experience in this regard is also the experience of having achieved ethics officer or something like that. So a person that inside the company monitors in the different teams what are the ethical issues, and whether it's an ethical issue, is it easy, address it, it is more complicated to ask to the board, to the external boards for an advice. So these systems are all systems that create a bridge between inside and outside. And this inside and outside is very important in terms of dynamic, because if you simply create a standard board also for the developers, it seems that a sort of court or someone that judge you, or someone who wants you can externalize, outsource the activity. Okay, I don't care about that, it's because it's an external board that doesn't care about that. So it's very important if you want a real ethical, bio-oriented design to engage the people that shape technology, and to do that, you have to create some interaction between the day work and those that have to monitor the ethical issues. Of course, there are problems also in this system, the problem are the sources, how many money and time and support to deal with these bodies. The second problem is the access to information. For instance, you can have an ethical course, but if you don't provide all the information available for the case, of course, it is limit. And the third point is non-disclosure agreement. Non-disclosure agreement is a limit because in many cases you sit in this board, but you cannot interact with the stakeholder revealing the information that you have. In the assessment, it's very important to participate in the stakeholder in order to better understand how some solution might affect society. But if you cannot reveal to the stakeholder what is the solution, of course, you cannot have this kind of participation interaction. So the non-disclosure agreement is quite critical, and unfortunately many companies are very problem-free. That's it. We have a second group of companies in which we know they have a board or something like that, and we have no information about what they are doing. That's the problem. So they have this board, they interact with some of the stands, and we know also some management, the people that are in the board, but we don't know exactly how much is affected. Among these cases, there is one case that was interesting for this figure that I already mentioned, all the chief heavy-parks. And then they have the worst case, that is the third group of companies, in which we have a general statement about the fact that they have a board that is not a board that is worth. We don't know the number, we don't know how they work, we don't know now. So this is typical ethics washing. The idea that I put something, so I say, okay, butter, I am ethical here. So the main findings are that there is a lot of variety, and the variety is also positive, because maybe it's not necessary to address this situation in always the same way. So there is not one solution. It depends by the company, it depends by the idea that they are involved. And so the idea that the external, internal, ethics survival, etc. can be combined in a different way according to the specific needs. So there is a fact that in any case there are some criteria we can say, and the most important is the level and dependence and reputation of the member of this board. Because of course you have as an ethical ad 

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    792 tokens
-------------------------

7.

visor, people that receive money from the company before it doesn't work. And unfortunately, after, you know, when Google created the expert committee to deal with right to forgotten, one of these was a colleague from an important UK university, and you say before some grants from Google for our PhD students. Of course. The dependence of the academics is not discussed, but as humans, you can imagine that this may affect also indirectly, also in an unconscious way, we can say, what is your attitude? And so this is just to recap what we are saying, the importance of transparency, the importance to be effectively means that you give other resources to reach the goal and also monitor what happens in the company, how the company follows the indication of the ethics advisor, you can say. Just briefly mentioned on this part of this ethics experience, the role of ethics committee is not something new, but it's something that's already regulated in some areas. For instance, in this sense, it's important to make a comparison between the medical ethics. The medical ethics we have, those that require an ethics evaluation for the products, for instance, for the medical products, or for the experiments, we involve the humans, etc. Of course, this experience is an experience that we can look at in order to copy some logic, some ideas, or we had it so they are. And so on this basis, almost this consideration, but because it's important. And the last point is not only the importance of ethical dimension, but it's also the importance of participation. Participation is important because it makes it possible to better understand that it's impacted. If you are in a room with us, but yet imagine the world outside, but they don't know exactly. It's based on their experience, based on their knowledge, because it's not like having a direct interaction. So the participation makes clear the situation in many cases and the potential impacts. And then also from a democratic perspective, participation is crucial because through participation we can design an AI that will impact society, something that is accepted by the group that potentially will be affected. This is very crucial. In cases like the Toronto case, that's when the large-scale project was stopped on March 10, a theory showed that without the active willingness, we can say, of the citizens to accept the project, there is no reason, there is no possible to reach the group. Of course, the participation, these are the potential aspects of the participation, how we can create participation. This is something that is not for the legal staff, of course, it is for the social scientists. But of course what is important from a legal perspective is that the participation should not be biased. It means that we should be able to engage in a variety of target groups. The methods can be different, but should be the most adequate in order to preserve this variety of opinion and to be inclusive. Money away participation, for instance, you create a smart cities and you create a participatory platform in order to discuss about the design of the smart cities, etc. But if the platform requires assessment, after a final assessment, whether it is a smart home or some abilities in order to use computer technologies, you can imagine that some people, for instance, old people or marginalized people, are not access to this platform. So their voices are not representative of this platform. And this is an issue, in terms of participation. Participation means also that you should be able to transfer the knowledge that is necessary to understand the problem, to have an effective participation. You have to put people in the condition to understand the technical aspects and the concept of all the technical choices. And the goal should be the idea to have a co-design, so participatory design in which we design together a solution. This is something that I already said. These are the risks, participation, or what I said before, driving  

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    922 tokens
-------------------------

8.

the participation was your goal. Manipulating through oriented questions, oriented activities in order to support and not to have an effective participation. And also a risk is related to the voluntary only participation, because of course the voluntary natural participation sometimes creates bias, because only most active person can be represented or this can be a polarization, or people can be excluded because they have not time or abilities in order to be involved. So for this reason the participation should be applied and based on different kinds of target groups. If you want to have more in the book, there is also an hour reading materials for the course. There is a specific section on humanizing processes of AI, where all that, the skills are described in the text. And there is a specific section on AI where all the cases are described in the text. So if you want to have more, you can find it. Okay, so we can stop for now. We have an hour. Yeah. I wanted to ask about the applicability of the AI and the vision and the skills like individual, as we come to the AI and the complies with the AI, or is it applicable for the AI or is it applicable for the individual? Look at it, and in this part of the definition, the focus is on the company. So AI bill. By the way, an individual that doesn't buy more self is not so easy to find, or a very basic application. Here the problem is something that you put on the market. No, something that you do recognize the imagery of your company. Just to say the scale, what is the scale? Okay. Provide a place in the market of the AI system. And we will look at the definition. Naturally, a person, fabric, a 30-angels, or other bodies that have the AI system, or that that has on an AI system, that also embraces the AI system on the market. So there is no limit to the company. It's also natural, but the focus is on the fact that you put something on the market. What? No, Alina, the difference is that the factor is the fact that you create an AI and you put on the market. Thank you. So the condition is that there is an impact. If you tell a AI in your home and there's no any outcome outside, it's okay. This is different. This is different. If you put make available your product, of course this creates... I don't know what to say. This is the AI at the category and maybe with the scope of the application, our discussion. And by the way, if you go back to the transaction, we spend the time and we are not exactly the scope. And the first case is that you are able to put something on the market. Exactly on the private exception, also the exception. So just to play the point. So of course this will define the scope and also the learning, the teaching between your reserve, industrialization, product, etc. This is the first thing. As usual, the actor is not the whole group. We are the same age group and we are the same age group. And we are the same age group. But the first thing I did was to use a fork and maybe I won't do it. And the example scope is provided places on the market for the impotent. For the impotent. For the impotent. Yeah. We know there is a division also for that. It's for the impotent. Division. How is this AI at? I don't know. The market means that first making a valuable AI system. You can see. No, I don't know if that explains it. Making a valuable in the market. Any supply of AI system for distribution or use. Whether in return of payment or with you. Yeah. At the moment it's a broad definition. But it's very focused on the entry in the market as you can see. So the stage of that kind of thing is something in the field. That is only for the reserve, for instance, it's not something. Any other question? Okay. So. Now I want to give you. From gute. Don't do that. NATIONAL But when I told you I'm going to with Non in need. Please do it. I'm going to make it. I'm going to make it. Then maybe I'll have to change it. So maybe... Yes, it's true. It's like a crazy friend of mine. So this was one of t 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    968 tokens
-------------------------

9.

he cases that you were talking about. So it's useless to copy and paste it. But if you want to, then you have to have it as you go. So let's think how you could address this question. Okay? And we have almost 20 minutes. I'll give you 10 minutes. Try to write something. Your exercise is to write. Try to write something. And then we discuss what is your choice about the question. Of course, during the next exam you have more time. But first take some notes. I'm going to write something like that. Let me say that. How? We address this kind of question in an exam. I think the picture is not the essential function. The essential function is like the flag on that. And if you want to take a picture, it's okay. Let's think about that. Because the positive aspect of this exercise is that you start writing something, you can discuss it. Simply taking the picture doesn't change a lot. So look at this. That's the... We are here. And consider how to apply the knowledge that you know in terms of impact of the impact assessment, use of data, and you want it. I'll give you 10 minutes. Thank you. Thank you. I'll give you 10 minutes. Ta-da! Thank you. Thank you. Thank you. Thank you. I don't know what to say in Italian. You see? I don't understand this sentence. Which one? It's... People are trying to understand the language. It's a matter of... It's a matter of... It's a matter of putting a watch on the basis of the person in front of you and changing the gender and the identity of the watch. So it's a matter of putting a watch on the basis of the person in front of you. Yes, he... He's in a different way. Yes, exactly. And so you have to take the information on these things here, on the person in front of you. And then, in short, you have to take the gender and the identity, especially the one in front of you. Yes, yes. I don't know what to say in English, but I'll try. I mean, I don't know, I don't know what to say. I don't know what to say. I don't know what to say. The story of the video, too. What do you think? What's the point of destroying the... And how do you say it? Showing the illy scan of people, you know, in case of... of that armor of the scale. Oh, yes, yes, yes. Maybe in this case, they can be destroyed by video, only for a period of time, in which maybe the agents... Yes, maybe in a temporary way. They can do it in the controls, they can enter a week if everything is detected correctly. But they have to be inactive. Yes. Especially the kind of gender, the kind of language you can't even accept the kind of macro data so that there's no specific sense. If a person... I mean, because he could have sense, he could take some generic information that he can't identify with a person, but in reality there are so many specific ones. But yes, then I have to think, this creates prejudice, because what does it mean that if this person is also aware and... well, the Marocchina, I don't know, then they have an effort in the... there are more remains in making him use it, and if this person is also white, less white, the problem is ethnicity, which is a bit complicated. It's a mistake. I mean, Skiber, biased. Biased. Then I don't know what it is, because then I don't have the sense of being personalized. I mean, in the language, there was... Obviously, yes. Even if I meet a National Border, a good agency, it's not that they don't speak my language, they speak English to me. But it could be useful. It's a very easy thing to do, okay? But I don't understand the gender sense. It's a bit... It's also gender, it's a bit... Gender is used to detect better and they are keeping me at least. But then there's also the expression, you have realized that each person reacts... I mean, those people who know that one has some kind of problem, and you have to ask them, and maybe you make decisions based on something you can't know, so you ask them for more personal information. In fact, the problem in general, and this thing doesn't seem to work, is that they have g 

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    921 tokens
-------------------------

10.

iven a method where they perfectly detect if one attacks them only with the real thing. In fact, there are some things that don't exist because they don't want to be... I'm sorry, you have the particular case. You forget that one who knows has a cut here, and for some reason she interprets that it's a criminal one, when it's high, maybe it's not. In fact, it's not a certain thing, unless it's an element, something that's verifying in a second moment, but there's no sense, because there must be one behind a writing that looks at the video, and it's coming when it could be completely empty. It would be more sense that you have, like, with the help of a person, maybe you have a starting screen for the person, that gives you a level of confidence, like 90% of the time... We know that we are in a second control step where there are physical pressures. Do you know what the exam is about? It's like two exams, like this. Exactly. In fact, the exams are... What kind of exam is this? It's like an exam exercise. I mean, it doesn't make sense. Yes, but it makes sense. Or maybe you can do it just to make it... trigger the... It can be. The way you think. I can't find the answer. And... Two questions. Which may be what is that? Two questions. One is about the English and the other is about the computer science. And between these two questions and two questions, which are four, what would be the answer? So, could there be a theoretical question? No. Yes. But yes, at the end of the day we will try to... Yes. Guys, time to... Open up the story. You turn it around and you get one thing. What to do, guys? It's time for comments. Anyone? I see the other group. Wait. We have any here? Someone wants to start. I don't want the final solution. Don't evaluate your feedback. But just to have something starting, discussing and looking at how your answer could be evaluated in an exam. So, anyone want to start? Please. No. I see. Yes, sir. Hi, Steve. Hi. So, I think the application required is to so people to put inside the information about the gender and physical and language. I think as long as the application is not starting processing those personal data it's going to be the armed regulations. But however, there might be some trouble of the person who has to be the data and stuff that's safe to use to be the data center. You know, there's kind of a barrier to the assessment. Your concern is about the fact that somehow it's not my concern. Yes. Do we need concern? Yes. Are you all agree that we need concern? Someone disagree? Maybe. Yes. Yeah. In the GPPR there is a specific provision about public interest including for the control. The problem is that also when there's a public interest it doesn't mean that you can do everything you want. And there is also in this case a balance of interest. So the problem is in this case as the product is designed do we have any problems as a balance of interest? So, you can't skip concerns. You can say, okay, there's a public interest you can collect information. But the problem is which information you can collect in order to reach the purpose. And if the entire design is consistent with the legal and the values that we know in terms of the protection AI. So any comments about that? Imagine that we can use public interest but let's see, okay, so we can download all these tasks or there are some concerns. So, that will be the second part. We can do some integration with the two ways the third is the system of non-binding. So, we can have people in the room who can mitigate the impact of the AI system. Also, we can do some of the things that we want to do with statistical measures. We can decide the risk of it. Because if we want to risk it it will be a problem. That's the mitigation. Mitigation is a lot to do with the public interest. Okay, that's true. But there are two elements that we can add. The first element is you have not discussed the system in itself. So, one of the foundation of the second part of the system is th 

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    827 tokens
-------------------------

11.

at we have to recognize emotions. Is this something that is scientifically commonly accepted, you can say? Not at all. There are many concerns about the impact of this visibility. So, this is the first point that we have to start. So, before we can also reach this conclusion. But before considering how to mitigate them the first step is is this technology okay or not to reach the goal? Because if this technology is used to assess the emotion in order to evaluate if you tell the truth, not. But the technology of emotional detection in itself is not realable. And it means that there is a lot of errors and a lot of mistakes, etc. Of course, this is the first element that we just not to use because the trade-off between the potential impact although you have a human in the loop or whatever you want. And the trade-off is very high because if we use a system that have a lot of uncertainty, we can say about the value of its outcome that this system cannot easily be mitigated. Or if you want a more efficient perspective the cost to mitigate is more than the amount. Because if I say that it is very weak you have to correct the action to spend more time than I use the person who assesses the future without the art. So this was the core point about the technology. Is this technology realable or not? Because if not realable creates a lot of problems of account-resisting and so on and that impact that is very important. At the second point the first part was much more difficult is that do you see any problem in the way in which the system is created in terms of human rights but passive and what to say? So the fact that imagine the situation, you are a migrant you are running to the border by stress etc. and you are discussing with an amateur do you see something critical in that? So much. What to say? Then I give you what is there, then you are. In computer science you are created to squeeze everything about AI in terms of bias but the bias is only the main part of this type of system. Is it easier to address through statistical computer science is to answer but not cover all the bias? But discrimination is only one aspect. When you protect fundamental rights if you read in the book there is the case of the body double that double the kicks and there is in fact a private seat there are multiple. Bias is one, it is important it is crucial in the use of large scale data but it is not the only one. In this case the issues is about dignity. Human dignity because talking with a person or talking with an amateur is not the same. If you talk with an amateur I don't respect your certain status in your human dignity whether this amateur is asking you questions and questioning whether to admit or not in a country. Is this the issue we have with the use of telemedicine or other remote medicine in order to provide bad notion bad news we can say to the patient. People with cancer and they decided to transmit this information through the camera. The doctor appears in the camera I am sorry to inform you that. Okay, the information is always the same but the human and rush is different. The emotional dimension is different. So in this application there was a response about the impact on human dignity. The fact that you come to a road and you are screened by a computer we don't spend people to consider you. It's the machine that do that. We are like a brother to pass through. I know that for computer scientists it's not easy to graph but in terms of human rights it's important aspect to respect the personality, the human nature of the person. And I present in critical decision, in critical introduction of machine only is something that the impact on this dimension. So for a technical approach the fact that it's a machine that says it's something right, the doctor is the same. But from the cultural perspective of the human rights it's not the same. Because the face to face human-to-human interaction is different is more respectful of your dignity and your person. I k 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



Chunk #12    43 tokens
-------------------------

12.

now that it's not easy to get but this is an issue. Ok that was just an example and this was the key aspect. So I hope that you have a good and a good person. Thank you. 

- * - *  - * - * - *  - * END CHUNK 12 - * - *  - * - * - *  - * - * - *  - *



