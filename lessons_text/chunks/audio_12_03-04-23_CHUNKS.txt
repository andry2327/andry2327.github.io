Chunk #1    772 tokens
-------------------------

1.

I suspect that you want to start discussing what happens on high in Italy, that has potential impact on many other country decisions, which is this decision of the Italian Data Protection Authority that has issued another of temporary stop of data processing with regard to CHAP GPT. As we already mentioned, CHAP GPT raises several issues. To be frank, data processing issue is not the most critical issue of CHAP GPT. I'm much more concerned about the potential impact in terms of bias, historical bias, the risk of hallucination of AI, the risk of easily developing solution based on this kind of tool without full understanding of its internal risk. For this reason, in the same days, we have seen a petition signed by 100 scientists in order to suggest a sort of stopping, reflecting about the larger scale models that are more concerning use and not to adopt a more specific regulation of this model. But of course, as you know, the Data Protection Authority should focus on their topic and the topic here is the processing of personal information. This decision is a temporary interim decision. It means that it's not a final decision, but it's a decision based on the fact that there is a sort of urgency in order to address this kind of problem. If we look at the background of this decision, we have to keep in mind that we already have other similar decisions or decision-ridra-related topics. For instance, in terms of protection of biners, we had the decision-ridra-gal tick-tock in Italy, but also in other countries. Tick-tock was alerted, we can say, from the Data Protection Authority because it gave access to all the users without taking into account the limitations in terms of protection that exists for minors. And after initial stop, a tick-tock has partially changed its approach to the access of minors to its services. And there is another important case, also not only in Italy, but also in France and many other countries in Europe. That is about the clear VRI. Do you know clear VRI? Of course not. As a computer scientist, we are quite far from the recent data of AI, since... Clear VRI was an AI tool that collects information on the web and provides this information to the policy department in order to identify true face detection of the people. So for instance, they collect your image on your personal website, and this image is provided to the policy department in case that they stop you, collect your image and can compare this image with the database provided by Clear VRI in order to recognize you. This service was provided, again, in the US company, was provided to several policy departments in Europe and was stopped. What is the problem? Before looking at the decision, the main problem affected the machine learning community that over the time, since several years, I decided that in order to develop machine learning systems and to increase their performance, it was necessary to have a large amount of information. This is all well known in the field. And to have a lot of information, there are mainly two ways. The first way is to collect this information in a fair and legal manner, asking for this information. We can send to the people if people are involved, etc. And this has been done by several large databases that are used, for instance, for facial recognition, and other databases in which the people that are present in the database, the image is present in the database. This information has been collected properly according to the existing data protection law. But there was also another approach followed by machine learning guys, based on a misunderstanding about the legal framework, not only Europe, but all around the world. And based on the idea that whatever is present online, whatever is available online, can be freely used. This is completely wrong because the availability of in 

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    785 tokens
-------------------------

2.

formation doesn't mean that you have the right to use this information. For instance, if I am an artist and I have my painting, you cannot look at my painting, but you cannot take a picture of my painting, reproduce the picture, or sell the picture. Because I have a copyright, I have the right to decide how to manage the image of my products. And the same is for personal data in all the countries, and almost all the countries in the world have data protection law. In all the countries in which there is a data protection law, you cannot simply collect the data because it is unvaluable. As we have seen in the data protection principle, the data is available for a specific purpose. If you use this data for another purpose, this changes the scenario. And you need a legal basis for this different purpose. So for this reason, already clear of the U.I. was stopped, and now is the turn for chat GPT. The simple point that I want to underline also based on the crazy comments that the so-called expert provided over the weekend in the central newspaper, etc. Is that this is not intention to stop innovation. Somebody say, oh, guarantee stop innovation, we lose the opportunity to write articles for newspaper without really nothing, and using machine learning for writing the articles. Or there are professors that complain because they use chat GPT in order to make their research, etc. Regardless, the fact that I'm quite surprised that the scientists and journalists need the chat GPT to produce their results. And also, this writes some questions about the quality of their results. The legal side is, because you got comments, the fact is that it's not a matter of stopping innovation. As we have said at the beginning, when we discuss about the law and technology, it's a matter of which kind of innovation we want. If we want an innovation that is based on some values, principle, legal principle, we as a consequence need to limit the initiatives that do not take into account this kind of innovation. Of course, there are other options, and there are countries that follow different kind of paths in which innovation is based on exploitation on humans and individuals, in which innovation has an eye-impact on the environment without caring about that, etc. It's a matter of which kind of innovation we want, not a matter of stopping innovation. So, in this regard, in Europe, due to the fact that we know European Union countries, we have a lot of protection law. We cannot admit that companies coming from other countries collect data about a citizen without any legal ground and build their product based on this kind of practices. Of course, the product itself can be useful, but the fact that something is useful doesn't mean that it's also legal-acceptable. Also, the production of the British empire in many countries in the past were useful for many British people and also many European people, but they were based on slavery. It doesn't mean that it was okay. So, something can be useful, but it's a region that can have some problem. And the problem, in many cases in IT, of their region, in the old refrain, move faster, breaking things, it means make business without taking into account the accounts of the consumers of making business. OpenAI is a typical example because OpenAI was created as a research project with the idea to create an open AI and more transparent AI. And the outcome is OpenAI chat GPD that is not transparent because also in the last report, we have no any information about algorithm, and is business oriented because the strategy adopted by the OpenAI in the last few weeks or months is to product services for business. Of course, this changed a lot also in terms of legal framework because while for research there are some derogations also in terms of data protection, for business and profit initia 

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    775 tokens
-------------------------

3.

tives there are different kind of regimes that take into account the business that you are doing and the fact that your business cannot be done without respecting the rights of the people that indirectly provide resources to your business. So, unfortunately, this is another case of this idea to extract value from information of individuals without involving them in this kind of decision. So, this is the background. If you look at the decision that unfortunately is in Italian, but can make some comments. So, the structure of course is the structure of temporary and emergency, we can say urgent order. So, as is common in all these kind of orders, there is not a specific detail analysis like in a sentence, like in a final decision because it is the nature of the urgent order that do not need to provide a lot of another argument, but have been taken based on the risk. It is a typical balance that we have in the law and in the court decision between the need to activate an answer to a significant issue and the need to properly address this issue. Like in all the system, we have this kind of procedure, the urgent procedure in which the court or the authority can make a decision on preliminary information about the case. This decision normally is not the final decision, it is a temporary decision and open the door to a broader analysis of the case that will be the object of the final decision. So, this is common not only for the guarantor, but also for the protection of the authority, but it is common that you know the court around the world. So, it is just due to the fact that typically which final decision on a case, you need some months or in some case, yes, it depends by the legal system, and this is incompatible with the urgency that some situations present in terms to address some challenges. So, what is the position of the protection authority? The first one is that the data has been collected by OpenAI without proper information. This data collected without proper information are not the data that you provide to OpenAI when you access to the service, but refers to the data that has been collected by OpenAI in order to create the service. The fact that when you ask for you or other people that is known, and OpenAI provides you some information, it means that OpenAI collects and processes personal information about natural persons. And this natural person were not advised about the fact that this collection of data have been carried out by OpenAI. Please take into account that it is different from the scenario that we have in Search Engine. Because why might we use Google or Bing or other search tools? In this case, all the websites have a specific instruction in the text, and it is a file, a XT file, that provides information to the crowder in order to index or not to index the content of the pages, and which content should be indexed or not. So, it means that when you put something in a website and make this available, it is your decision to make this available and you are aware about the fact that the crowder can collect this information, and it is not only available through search tools, and it means in order to limit and not to index content. This is different. In this case, people were not aware that certain points in the past, a tool was used collecting all information that were available, not only good available, but physical, technical, and online. So, the first point is the lack of information. The second point is the lack of a legal basis for this collection. As you know, the legal basis can be concerned, legitimate interests, research purposes, etc. In this regard, for instance, according to Italian data protection law, there might be some rooms for research purpose, although the fact that the operator decides to create a business model makes this kind of approach quite comp 

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    781 tokens
-------------------------

4.

atible, we can say, with the research purpose. In the Italian law, that is quite generous for research. There is enough room in order to collect data without consent, for instance, of data subject for research purpose. But if we use this data to make a business, of course, there is no longer research purpose, and this change in natural processing. The third point is the risk that this information is not corrected as being attested. Sometimes, the operator provides misleading information, including about personal and natural person. Of course, one of the requirements in data processing is that the data are processed in a correct way. It means that the information provided is correct. Another point is that there is no specific limit and more specific age verification system in order to check that the user is over 13 years old, according to the GDPR as implemented in Italy, that requires specific consent, parent's consent, for the user under the threshold of 13 years. And this was the tick-tock argument. There are all these part of the minors. As is said in this paragraph, the urgency is the main reason. And as you say, Nelemore, the completamento research service tutorial. So an urgent decision, why we are investigating in order to have a full understanding of what is happening and the legal compliance of what you are doing. And the issue, the decision of the data collection authority was not exactly blocking. That's another point. The block is something that is used, for instance, for child pornography websites. And the block is done at the level of DSM, so not made possible to access that kind of resources online. This is not a block because the guarantee of data protection authority has no power over the internet infrastructure. Why this power has in the end of the policy, cyber policy in order to limit the criminal action. So it's not a block, technically a block of the website, but it's a source of the use and processing of personal data. Of course, the consequence of stopping the use of personal data to the fact that you have created a system and you cannot extract the Italian part from the system, is that you cannot continue to provide the service in Italy. But it's not technically a block. A block happens when you stop a child pornography website that is no longer accessible online. This is the typical blocking. Or you can block in the case there is a copyright infringement, for instance, when they distribute copyrighted material. This is a block. This is not a block. It's simply stopping processing personal data. We can say that in many other situations the stop of processing personal data will not impact in stopping providing the services. But in this case, that the personal data are deeply embedded in the services and due to the fact that this is an AI training on personal data it's not possible to extract this kind of data from the system. Of course, the consequence is that this system cannot be evaluated, but it's important to have key other differences. Um, immediate decision, immediate effect of the decision from the moment in which the change is adopted. And the sanction, of course, according to the GDPR and the Italian law. This, this is a, as usual, is a preliminary part, we can say. And this is the mandatory part. The mandatory part is this one, temporary limitation. Immediately affected temporary limitation and the sanction. So, this is the situation. The decision of an AI was to stop providing the service. Someone commented that, oh, but it's accessible via VPN. Of course, it's accessible via VPN. The fact that we have some provision that forbid to stall everything doesn't mean that people do not continue to stall things around. So, there is a distinction between what is forbidden and the only way in order to have access to illegal content in this case. Of course, yo 

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    786 tokens
-------------------------

5.

u can use a VPN, but it's a circumvention of order provided by an authority. The move of the other data protection authority are, at the moment, the French data protection authority has decided to start an investigation on the cases. In Australia, there was a declaration about the need to have a more deep focus on the chat GPT and also on AI in general. And in the USA, there was a petition, a request to the Federal Trade Commission in order to investigate the chat GPT case. So, what it means? It means that there are several initiatives that rise concern about this application. As I mentioned, this is not a big issue. The most impacting issue is about the quality of results, the potential bias, etc. But, in terms of data protection, what is important to highlight is that although your product would be very nice and very useful for the future of humanity, it doesn't mean that you can't realize this product in French and the law. If there are some laws, you have to create the products according to this law. Unfortunately, the machine learning community shows a limited knowledge of the data protection law. Very focused on the approach, the strut value from information, regarding the fact that this information is information about individuals. And by the way, as I mentioned, the beginning is not only about data protection, because of course, in this AI system, there are also copyrights, materials, facts, for instance, that have been collected. And for this reason, also, the copyright owner has interests in order to have a clear framework for this kind of innovation. So, this is not a political decision. A political decision to stop innovation or to stop some kind of product is something different. It's based on a mere intention to follow one path or another. This is simply the application of the existing law. Unfortunately, in the US, they created many case services that provide all around the world. Without having a clear awareness of the variety of the legal systems and the risk that this entail in terms of compliance with the local legal system. So, at the moment, it's quite difficult to understand what will be the final outcome of this situation. It's clear that there is a huge pressure from industry and many other services that find an open AI chat GPDA solution in order to simplify part of their job. And we can also discuss about the impact of the work of this kind of option. But it's not new. Some years ago, when the European Court of Justice in the Costa Rica cases decided that there is a right to be forgotten that the natural person have with regard to information provided by Google at that time, Google raised a lot of concern and started a rock show around different capitals in Europe to demonstrate the tragedy of this kind of decision. And in South America, there were some colleagues that write articles in which say, oh, this is a way to remove our past. All the dictator of the past in South America now can be asked to have the information provided by them. We are erasing the history of our countries. After many years, we can say that this didn't happen, that Google continued making its business as usual, that the right to be forgotten has been implemented by Google without any big issues in terms of cost and in terms of impact on its services. So of course, we have always the same problem. When you design something, when you create a technology, the easy way and the cheapest way is the best one. Sometimes, legal compliance, respect of all the principles, requires an extra effort in terms of time, in terms of cost, in terms of design. Many companies have not the maturity to accept this kind of game, that is the game of responsible innovation, and prefer to have short way, beveling product without taking into account the potential legal implication, as far as there is not somethin 

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    835 tokens
-------------------------

6.

g that stops them and then they complain about the tragedy. So, I think that this is the situation and of course, the fact that now we are discussing the AI Act, this kind of decision, I am more in general in this debate on chat GPD, has raised a serious concern in Brussels about the regulation of general AI. One general AI in Brussels is, means, old purpose AI models. And at the moment, we can say that in the proposal of AI Act, there are not a lot of specific answers to this problem, but it is something that we have to address. Probably there is not one solution that can solve all the problems, but of course, the idea that you can't delve technology, but in delving technology, you should be aware about the legal framework, is the first step in order to avoid this kind of problem in the future. So, this is just to comment what is an important user in the situation of the topic that we address in this course, but of course, I imagine that there are some doubts, questions. My question is why this is not related to the data breach that's listed platforms? Yeah, that was also my doubt. I didn't understand why they have not put in the list the data breach that they have suffered, that was a good argument, because having a data breach, they have to notify, and this defied that data breach also includes potentially EU citizens and Italian people in Italy. Of course, this was a good argument. I don't know why they didn't put that on thecast. Imagine that. They miss the point sometimes. Personally, if I wrote this kind of order, I will introduce also a reference to the data breach, because the data breach is in the lack of notification of data breach and is to. Any other question? Please. It seems to me that the crux of the question is the gathering of protected data, and then the use of it. But my question is the main corpus of training data that has this characteristic is a data set gathered by common control, an organization, and a whole suit of other actors are employing it, and the organization that has collected it in the first place, why aren't they the target of this action? Yeah, thank you. Thank you. Yeah, you are right. OpenAI say that they use third party data. Why the OpenAI was the target and not the website? Because when you make a decision in terms of authority, you focus on my issue, and my issue is now the application that this database through OpenAI as in many countries. But you are right. The region is in that. And by the way, it is not the only one. There are many other database that have been created, not in a manner compliant with the data protection loop. But at the same time, for the data protection authority, such as for criminal authority, there is a sort of freedom in their initiatives. It means that you are not an urge to investigate one thing or another, but you select the target according to what you think is most relevant in the moment. So why that database clearly was used by the Ashurian community to not rise a lot of concern? Now, chatGPD may be available to all the potential users. Of course, why is a concern in terms of societal impact? This is the first answer about why one and not the other. The second answer from the legal perspective, the fact that you use a database that is created in a manner that is not compliant with the law, does not make legal your use of that database. So if the database was not legal compliant, you cannot use this database. It's like if something has been stolen and you buy something that is stolen, that you know that it's stolen, you are criminal liable. So the problem is that in terms of the defense of open air, say, oh, but we use a database of third party, but not solve the problem, because before using personal data, you have to check the sources, and the fact that the sources provide you data that are legally compliant in 

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    783 tokens
-------------------------

7.

 terms of data collection and data processing. So, two more questions. Any other questions? Okay, first. Why is something happening in Italy? Since all the European countries except for the GDPR should not be blocked by GDPR? Here we have a problem of the GDPR legal system. The GDPR legal system makes a distinction between the controller and the processor that has established it in one of the EU countries, for instance Facebook has established it in Ireland, and the entity that has not any legal establishment in Europe. When the entity is established, there is a procedure according to Article 60 of the GDPR that makes possible to have a sort of common decision among all the data protection authorities that have potentially just given the question. In this case, this sort of co-decision or concurrent process of case decision is not possible, because there is not a legal establishment. Open AI has not a legal establishment in Europe. So, as a consequence, all the protection authorities are free to act at an individual level. As far as we know, at the moment, the CNIL, the French Data Protection Authority, that by the way was the one that provided an assumption to prior VLA, the CNIL say that based on the initiative of the Italian DVIA, has the intention to investigate this topic, and in the few months it will decide about the case. We don't know the position of the data protection authority for the moment. We know that CNIL has asked the data protection authority in Italy to transmit information and to have a sharing of information about the case. And we see, once decided on Friday, it's quite difficult to predict the reaction from the other data protection authority. Also because it's a quite sensitive topic, if you have a look at the main newspaper in Italy, there are many so-called experts that were against this decision, let's say that was an undemocratic decision, that the guarantee does not understand technology, that we stop chat, GPD, as well as we stop artificial meat, and it's very funny and strange comments also from people like Minister Salvini, that is not exactly an expert on large-scale natural processing languages, but maybe recently have created Instagram. So in this sense there's a flourishing of reaction as usual in this case, but the problem that chat GPD arises is a serious problem. And serious in terms of data protection, because the Mashulian committee should be much aware that they cannot continue to use everything they find online, based on the fact that it's there, because there are also your photos, your name, your information, etc. And they have not any legitimacy to have this kind of extractive approach to innovation. This reshift innovation is not necessary. For instance, in biotech we have the same problem, and we have created a large biobank based on the involvement of the people, etc. So I think it's not a problem of shift innovation. By the way, also in Mashulian community there are a lot of different approaches, the idea to add very large database in order to develop Mashulian is no longer the only approach. There are also approaches that require more limited number of information. And then of course there are issues that are not about only on data protection. For instance, one of the main issues that you can recall is the fact that ChefGVD is stopped in November 2021. So it means that there is training on information that exists at that time. Of course this has an impact in terms of historical bias on the decision and the feedback that this system can provide to the people that use the system. And of course this is an important aspect to be considered because this system is used for all purposes, to asking about political, economic, development, etc. And having an oracle that is stopped some months or years ago is not exactly the best way in order to a 

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    809 tokens
-------------------------

8.

nswer. So there are many issues and data protection is only one of these issues for this reason. It is necessary to invest both in our regulation but also I think in a better training of a computer scientist. For this reason we have these kind of courses for you. Any other questions? Okay, please. I might be. Because you have the future in this field. My question is that this is not the first time that there is a data breach from a tech company even about important data. So why in this case the reaction was so strong? Maybe it's my fault but I don't remember any other case in which a service like Facebook or something like that has been blocked for reasons like this. Okay, so first was not for data breach. Data breach as you see in the order there is no any reference. Personally I will introduce it but there is no reference. So it was not stopped for the data breach. It was basically stopped for the lack of a legal basis in collecting the data and the lack of information in processing the data. So this was the reason. Second, the system is not blocked. They ask to stop the processing of the personal information that were collected in the infringement of the law. But the nature of the system that is an AI and use training data is impossible not to stop the system per se. But technically it's not a block of a service. So in this sense there is not an overreaction. It is exactly what happens in many other cases. So a large set of cases in the decision of Italian and any data protection authority in which you process data without consent. For instance we have large cases in Italy about marketing in which there are marketing companies that collect from everywhere information without asking for consent. And that uses information to provide call or marketing material. In this case, like in this, the data protection authority says there is not a legal basis for your data collection. So you have to stop to use this data. So this is normal. It's not for charge GPT. In any case in which there is not a legal basis, the order is typically to stop and to destroy the data. This is the system. So it's not an overreaction in this sense. Of course the complexity of the AI system makes it impossible or rather impossible to separate the collection and the outcome in terms of the system. While in marketing you can say, okay, I have half of my data set that was collected without consent. I stopped to use this part. I continue to use the other one. When you use data for training in AI, you cannot say, I can stop using part because it's already in the system. And this is also another tricky point about the fact that although this data is in training, for instance, and then it can also be destroyed. And the model in set does not include this kind of personal data. Because the training is a learning phase and then the data are no longer in the system. But according to the standards of machine learning, the fact that you use some kind of training data may possible for the model to have a better knowledge of that kind of data. To be more explicit, if I use a phase in order to train a phased recognition system, although I deleted this information from the original training data, this system is more able to recognize that phase than other. So this, of course, raises new kinds of concern because for this reason, it's a very tricky case. Because in the ordinary case, the data are in or the data are out. Here you can see, but I used the data, but now the system creates the answer without that data. But the fact also that you delete the data does not solve the fact that part of the memory of the system is based on this. This is a typical situation in which the existing law does not fit well with the new technology. And I myself and others, since several years, say that for AI we need a some specific provision, that dat 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    781 tokens
-------------------------

9.

a protection provision I'm not having a way to know this to my address. So I think that in this case, we use data protection because we have no other option. And because there is an issue in data protection, but we can imagine that we can create some specific ad doc provision for the development of this kind of product. Of course, the ad doc provision cannot legitimize the fact that you can collect whatever you want without any kind of protection of the people involved. Also because chatGPD is created for one purpose, that is purpose in order to benefit the humanities if you want. But you can imagine that the same services are created for very different purposes. And that you already mentioned information I use for criminal purposes or for other purposes. And there is a recent also report from the security services that say that chatGPD has also a potential high impact in terms of financing, you can say, cybercrime and other activities. So the application writes a lot of concerns. And to address, to fully address these concerns, we need to have proper regulation. Proper regulation means that when you create this technology, you should be more aware about the impact of this technology on society. For instance, carry out a human rights social impact assessment in order to check what are the consequences and how you address. The report on chatGPD 4 is not a clear example in this sense because there is not full transparency about the services, there is not full transparency about the algorithm. And there is not specific concern about all the potential negative consequences. In the report they say, oh yeah, there are some problems trying to find a solution, but in the meantime you continue to use the system. And this is not exactly the approach. It's like, if you compare for instance in self-driving cars, although we are testing self-driving cars, I think since 10 years, at the moment nobody is selling full automated self-driving cars. Because we all know that the technology is not still mature enough. Why in machine learning we have not the same approach, so we can put something in the market and say, oh, there are a lot of failures, but in the meantime continue to make business and use it. So this is a problem and it's not shifting, limiting innovation because self-driving cars are still under development and we reach a time in which they will be okay and we can use it. The fact is balancing the innovation and the impact that is in the Action Act. So if you want to make business, you put chat, GBA, immediately available and you as a being that you have a very limited services at global level, embed this in order to rise the quality of your product. But this is a marketing. It's not responsible innovation. So it's a matter of which kind of approach we want to adopt and in other fields for instance self-driving cars or the use of hydrogen in order to as a fuel, etc. We have a much more responsible approach. In the machine learning based on the art, based on the trend, this kind of responsibility approach is quite related to it. So can I give one, may rise, my contribution is a better awareness that also in the data science, responsibility innovation is important. Any other question? Okay, so go back to the ordinary business. Yeah, yeah. So this was about the large escape project and how we can address large escape project and the idea was to have in a project in which there are different components, different kinds of processing operation is important to have a sort of coordination. And the coordination, of course, is more complicated in large escape projects because you have not only one entity, only one data controller, but you have a variety of data controllers. In many cases, embedded in a common platform. So for this reason, it's important to create a sort of interaction among these di 

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    774 tokens
-------------------------

10.

fferent actors. And this interaction is possible through, for example, the use of cooperation among the DPO. It means among those that have as a main goal to define the strategy in terms of data processing operation. So in this sense, setting up a sort of coordination among the DPO is very useful in order to develop a good answer to the complexity of the system. And as like in Chagy-Kuti, a large escape project may have a societal impact that is beyond data perfection. It's also important to create bodies of expert, committee of expert that can advise about this kind of impact. So it's not only a matter of data, it's not a matter of data protection, but it's a matter of social impact. And this was done also in some critical projects, for instance, in Toronto Project, Sidewalk. There was an ethics board, an ethics committee that advised the companies. And also in the advocacy, there are many companies that have created ethics board in order to have advice about the critical impact of the application. So data protection is not the only part that we have to consider. It is much more than data protection when we use large escape projects. Of course, in a large escape project there is also a problem of management, we can say, of the legal compliance. So it's very important if the system is integrated and includes different kind of processing operations, it's important to avoid a situation of too much, of having too much variety, we can say. So it's important to have a uniform approach, for instance in data transfer, in the collection procedure, in the security procedure, etc. Because if there is not an harmonized approach, the risk is also to limit interoperability. So the system that co-exists needs to be interoperable. But to be interoperable, of course, it means that in terms of data protection, they follow the same path. If one system is committed by its rule and agreement with the notice and the consent of the data sergeant, it is committed not to transfer data outside Europe, and another system is authorized to transfer data outside Europe, it's very difficult to manage the two data sets because one can go outside and the other not. And so it's very important to have uniform approaches, because without uniform approaches, when the system should be integrated, the risk is that you have data sets that have legal requirements that are different, so you cannot merge or fully integrate the data sets. So for this reason, our coordination is very important. And also, if you imagine the system like in Smart Cities, it's important coordination also in terms of reducing the burden on the citizen, reducing the burden on the data sergeant, because if I have 20 companies that possess data for the Smart Cities, I cannot imagine to have a 20 notice and 20 consent form, etc. It's much better to coordinate, they have only one or two main systems to collect personal data through consent information. And the risk assessment, what we have to point out with regard to the risk assessment is that the risk assessment, in this case, should focus not only on individual processing operations, but also on the community impact. So if you assess the single risk, you have not the full picture. For instance, you assess the risk related to Smart Mobility, about the risk of the platform for citizen engagement, about the risk concerning the energy monitoring, etc. But there is also the community effect produced by the interaction among these systems. The fact that I can extract information about the place where you go and the declaration in terms of political orientation, etc. that you provided in the platform about citizen participation, for instance. So there are some community aspects that are the result of interaction of different streams of data, or different streams of processing operations, that should be consid 

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    770 tokens
-------------------------

11.

ered. So the data pressure impact assessment also includes the community processing operations that put together all the systems that are developed in the Smart context, we can say. And another important point, what we carry out in this large project, is to take into account the role of participation. This is something that we recall again when we talk about AI. The role of participation is important because in many cases the risk of data scientists is to have their own opinion about the work, but the world is much more complicated or different from their own opinion. And through the participation, through the active engagement of the citizens, you reach two goals. The first one is to have a better understanding of the problems. And the second one is also to have a higher level of acceptability of the solution that you provide, because I have no longer top down, but as a result of a cool design, we can say of this solution. So it's very important to have this kind of participation when we talk about AI. So this is a case, an example of large-scale data processing operation. It's about smart mobility, and this is the case of the BMO region smart mobility, the so-called BIP card system. In this system there were some issues in the background, you can say. Of course there was an interest in order to have an integrated model for mobility, but there was also another interest that was the interest, in order to monitor how the public funds have been used by the transport companies. Because as you know, in the public transportation, it's a sector in which the results, you can say, of the service did not cover the cost of the service in many cases. The fact that if you provide the services in very removed or small areas, you are not able to cover the cost. So for this reason, almost all the countries public transportation is funded by public administration, in order to compensate this lack of market pay return. So in this sense, there was an interest by the region, not simply to facilitate interaction between different kinds of transportation mains, but also to monitor how many people use this system, because they provide funds based on the declaration of the number of users. The declaration that was provided by the company, by the reliability of this declaration was not so sure we can say. It's only provided they want to have more granular information. For this reason, they create a system based on the market that may be possible to know who uses the system, how many people use a specific line, etc. In terms of the protection, what it means, it means that you have two main streams of data. The first one is about the contractor information, because we have the card and to assess the service, we have to provide information about your identity. And sometimes also additional information about your profile. For instance, if you are a student, you might have some discounts or people that have more than fifty-four years, etc. So there are different profiles and you need this information about profiles and about the identity of the person, for managing the contractor information. But at the same time, there is another stream of data that is about the mobility data, the mobility information. So which kind of lines has been used, or which place, what are the places, etc. Of course, if you don't care about the protection, you put it all together. But if you focus on the protection, you have to keep separate the two streams. Because why the contractor information should be always accessible in order to manage the contractual issue? The mobility information are necessary in order to predict user needs of transportation, etc. But to understand this kind of analysis, do not require personal information. You need to know how many people use one specific line, or how many people stop in a specific area, but it 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



Chunk #12    800 tokens
-------------------------

12.

's not important to know the density of those people. So for this reason, you have to adopt typical separation that we have using certain organization. So using a system that may be possible to separate the two datasets. And as far as possible using full anonymization for the data concerning mobility. Because you don't need to have information about the data. You need only to have information about mobility data. So for this reason, the system, also based on the experience of the London Bike, do you understand the case? Yes or no? Yes? No? No? Okay. The London Bike case was a case in which the City of London decided to make public available all the information about the bike sharing services. Under the hyper of open data, they decided to make available all this data. And they make available the data, they provide the information about the line of each bike, including the ID of the user. So it was possible using the same ID to detect all the track down by the user. Of course, this makes possible to identify some main track, like in this case, that show with an high probability the place in which the people live and the place in which the people work. In a city like London, this may be not a big issue in terms of identification, but in a different kind of territory, scale, in terms of, I imagine, to have a rural area with a few people that move from a small town to the big town, this is an increase in the risk of identification. So this is exactly an example how not to implement open data. And so based also on this experience, so maybe on this point, that through the monitoring of the tracking, the movement of the people, we can infer information about the place in which they work and which they live. The idea that was implemented in the system, in the German case, the solution was to have a sort of progress when the role came out. Thank you. Okay, we'll self-mute the term. So the idea was progressive anonymization, what it means. It means that the first step that we adopted was to keep separated the two datasets. But of course, we have to take into account what are the issues related to this certain anonymization. When you need to have information about mobility and when you need to have information about the person. Genuinely speaking, in almost all the cases, you don't need to combine these two information. Because you need information about the contract and everything about the contract, the lack of payment, etc. It's matched only based on the contract agreement, it's not necessary to know which kind of use of the card was done. And on the other hand, if you have information about mobility, this information on mobility does not require any information about the card. But there are some cases in which you have to match this information. And typically it's fraud detection or the case in which you miss the card or something like that. So in a very limited number of cases, it's possible that you have to combine information about mobility and information about the contract. But typically this happens in a short time. It's not something that you realize after one month. So for this reason, we set a short period for certain anonymization and in this period it was possible to recombine the information. We use an hash function in order not to use the ID like in the bike sharing, but to generate another code in order to track the movements. So all the information about the use of the card concerning the movement had an ID that was different from the ID of the card. So if you look at the database of information concerning the movement, you cannot know the identity through the ID. But through the hash function, when needed, you can match the two databases. So the first step was creating this kind of certain anonymization. And we had a short time, zero three days, in which it's possible to match  

- * - *  - * - * - *  - * END CHUNK 12 - * - *  - * - * - *  - * - * - *  - *



Chunk #13    791 tokens
-------------------------

13.

these two databases. Because we imagined that was enough in case of people miss the card or stolen cards, etc. That after we have an anonymization of the system. What it means anonymization means that we break any connection between the ID of the card and the ID that we have in the data set concerning the mobility data. But this anonymization, it means that it's not possible to reverse to use the hash function in order to identify the ID that we have in the other system. But based on the experience on the London case, we made a step further. It means that after 60 days, all the track that are based on the same ID in the mobility data are broken. It means that if you take a train from Asti to Turin and then a bus to Turin to Transdation to Polytechnic. For the first 60 days, these two tracks have the same ID. After 60 days, there are two different ID. So it is impossible to recognize through a reverse analysis of the track any kind of information about why the people are staying, why the people leave, etc. Because every single track is broken. This was the approach. Here in this article that is published on International Property Law and there is a paywall, but it is available on the SSMN. Here you find all the details about how was organized the system. We worked on this system for three years and now it's what you have in your pocket if you use the new card. In the site there are also a few information and also in the article because the system was a bit complicated because we have different level. The regional, the local and the company. So it was important to combine all the factors in terms of their role, with the controller, with the processor, etc. And also considering the fact that they have not all the need to know the same information. For instance, the region, the regional level, have interest to know the movements and the use of the service, but not to have information about the user or the contracts. Why at the local level, they have interest in knowing the contracts and the use of the card, but not to have information about the fact that you use other means of transportation etc. So it was very important to also to cut and separate the different parts of the system. A few words, yeah, last slide. A few words. Now to conclude this part on data protection that we discussed about here in the next classes. The data protection is not only a matter of EU regulation of course. We focus on EU because we are in EU, but there are many other countries that have data protection laws. And there is also a sort of competition at global level in order to set the global rules for data protection. Because as recently was read by the cases that we have also discussed today, there is a clear economic interest who regulates data, regulates also the economy and the digital economy. For this reason, there is a competition among different models, very, how we say, very authoritarian model based on the control of citizens and the use of data. Like China, Russia and other countries. Very market oriented system like US for the moment, but there is a proposal in the federal, for a federal protection law that can change this approach. And by the way, we already have some state laws like in California and other countries that have restricted this market oriented approach. And another approach, the European Union. At global level this also is debated in several forums, the Council of Europe, the Asia Pacific Economic Area, the OECD United Nations. There are several discussions at global level in order to set the sort of global regulation of data protection. For now the only existing international instrument is the Convention on the Council of Europe. That is the parties of Convention are the member of the European Union plus other European countries and non-European countries. Among the parties there ar 

- * - *  - * - * - *  - * END CHUNK 13 - * - *  - * - * - *  - * - * - *  - *



Chunk #14    118 tokens
-------------------------

14.

e countries from Latin America, from Africa, from Asia, etc. That is the only Convention that exists at international level on data protection. United Nations declared that they want to do that, but it's not easy to make a global Convention on data protection. But this was just to point out that the regulation of data is a very complicated topic because there are clear economic interests behind. So who is able to regulate data as well as who is able to regulate AI as a competitive advantage. And for this reason there is a lot of struggle and I think we can cast that for him. 

- * - *  - * - * - *  - * END CHUNK 14 - * - *  - * - * - *  - * - * - *  - *



