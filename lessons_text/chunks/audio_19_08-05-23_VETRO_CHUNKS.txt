Chunk #1    729 tokens
-------------------------

1.

 you free again after the Hello So welcome back to the course. The program of this week is to finish most of the theory part that will allow us to start exercising with exams from past years. We start the next Tuesday to have our Tuesday exercises every Tuesday until the end of the course. So this is the plan and let's see how we will do between today and tomorrow and next Monday. Last time we had, we began the part explaining how demographic disparities enter the machine learning or AI loop describing what is how it is made the disabstruction of the process of machine learning. And we were discussing how we arrived at this point. So when the state of the world has some inequalities that are transmitted to data. But this torture can also come through the measurement process. We started commenting this with this example from a newspaper articles witnessing that inequalities in the top colleges US top colleges are still there. And I showed you how race classification is a social construct how it can change if you change context it can change dramatically and also problems practical problems like the multiracial category that was inserted in the US university only in 2018. So otherwise it was not tracked until then. This is a measurement limitation. Measurement issue. We will as I said we start doing exercises of exams and as you know from the very first lecture in the case under is always a case and then a theory question. The case are quite historically quite well structured in the following terms. There will be either a discrimination issue that you should explain or a discrimination risk. So whether you don't know what is the discrimination and with the data that you find in the text you should identify what is the risk. You might also have questions in addition on your replacement to the question on discrimination on measurement issues that is related to the part that we are going to make today. Finally you might also have a sub question asking you how to prove the process. This is usually a very little value. So a case might have some questions. In certain cases where when the case is quite simple even three but usually only two. You might also have some very few computations to do about fairness. We will talk about quantitative evaluation of algorithm fairness between tomorrow and probably next Monday. So this is to contextualize you better now that we have done a few lectures of theory. What is in the case? We will repeat during the course the several options. As I said you might have also some measurement issues to identify. Here we are speaking about the measurement process. As I told you last time the measurement process is often bound to an idea of objectivity. The numbers of objectives are objective. But the measurement process is an empirical analytical process. It is many many subjective choices and we will now comment on this. This is a scheme that is adapted from an international ISO and IEC standard on system software engineering describing what steps compose the measurement process of any measurement. Any type of measurements. First of all the measurement process is an empirical process which you bind an information quantity to an entity. So you have an information need that should be measurable. It's a measurable concept that is bound to an entity through a measurement process that is the following one. Let's start from the bottom. You have an entity that is made by attributes that are properties that are relevant to your information needs. The criminal record is made of a person is made by many attributes and be made by a number of prior crimes. Unemployment status and other information. So you have several attributes of an entity. In our cases the entities will be always people, persons or groups. And I need some information about them. So 

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    785 tokens
-------------------------

2.

 I take some attributes and each attribute needs a proper measurement method to derive a base measure. So if following the example that was made, to evaluate the recidivism risk of a person I need at least criminal history as the number of crimes in the in the past and unemployment status. I need to separate the measurement methods to track the number of previous crimes. And I should also define where I found this information. What type of crimes I needed to track. This is the very first decision at the very first moment in which I identify an attribute of an entity. Even identifying attributes that are relevant for our purposes are subjective choices. The unemployment status that I mentioned a few minutes ago is a totally subjective choice to put in a model in order to evaluate the recidivism risk. So the data scientists often either cannot have the possibility to choose this because it's the product owner, let's say, that choose the attributes. The person or an institution that keeps the requirements for the task attend. Or it has plenty of opportunities to select the attributes that are available, that are in their opinion, in his or her opinion important for the task attend. So these are the features or at least the initial attributes variables that will go into the model. So far. Okay, so this is the first. These are the first two choices that are to be done. Base measures can be taken directly as they are or they can be combined by a measurement function. In order to get a derived measure. I may take the number of prior crimes as they are. 10, 20, 15, or I might decide that the absolute value is not fair. And I want to normalize it by the age of the person. And this is an example of derived measure and this is of course will have an impact at the end. The rationale of normalizing the number of crimes by the age is to have the same the same starting level for everybody because a person of 55 years old might have more probability to have done more crimes than a person of 18 years old because of simply the age. And this is an example of a very simple measurement function that I apply to a base measure. But here again, this is a very subjective choice that needs to be explained needs to be documented. I can have a set then of the right measures and some of the derived measures can be directly developed based measures. And they are combined into a model analysis model or predictive model or a classification model that should output a number unindicated. Risk of recidivism, number from one to 10. Again, this is here we might have the choice to decide the scale or not depending on what type of algorithm or learning algorithm I have here. The indicator might be taken as it is. Or there might be a further interpretation that gives them the information product the outcome of the measurement process that satisfies what were the initial information needs. In the example of the recidivist risk, we have seen that although the risk the output of the argument is from one to 10, then it is bound to a further scale, low, medium and high risk. And this is an interpretation of the value. As you have seen, this very simple schema gives us all the analytical, in a way that is analytical, all the steps in which human decision is involved and often, very often, all these steps are either implicitly done or not documented at all. So usually when you get a data set, you don't have all the documentation of all these choices. Very rarely that you can have a through and clear explanation of all these steps that were done for every variable, every feature. And if there is time, I will show you a few examples of this documentation luck. And this is a problem of trust, of accountability of the algorithm, depending on the case, on the purpose of the classification, on the purpose of t 

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    776 tokens
-------------------------

3.

he prediction, on the whole decision context. Example that maps each of these steps, so that you have a full example. The management of a company decides to adopt an automatic system for identifying the 10 most productive developers in the last year. And to reward them. When I first made this example, I took inspiration from real cases and the goal was to punish the list of productive developers, but then I decided to make it a bit more optimistic. So the company wants a list, the managers wants a list of 10 most productive developers, they want to reward them with some more money or whatever you like, and they do the following choices. They decide to take into account the code repository of the company where all the code is posted. And a second data source also is taking into account that it is the daily presence of the staff or the developers in the company. They want to reward developers that are productive both in terms of source code that is committed into the repository, but also in terms of the effects that are fixed because they value the alerts, the work of the testers. They produce very little code, but they do a very important work, that is the one of finding the effects. The managers wants a unique indicator of productivity. Only one. And the company defines the following measurement process. Okay, so let's start first of all from the information that is needed, from the extremes. We want the list of 10 most productive developers in the last year. This is the information need. Who are the entities, the developers of the company? What attributes are chosen? The source code written, the defects we fixed, and the time worked. Three attributes of the person. How many, how much source code I wrote? How many defects I fixed and how much time I worked last year, how many hours? Then I apply the following measurement method that is specified here. The source code written for each developer, all the code written by him or her is firstly identified in the repository, and then the number of lines are tracked. For fixed defects, the measurement method is the following one. For each developer, from the let's say the G-ra tracking systems, that is often all combined with the repository, I take all the defects that are fixed by the developer and they are summed. So this is the measurement method. And then I need of course a script that does this. And the time worked for each developer is the total number of hours that I worked. And this is simply retrieved from the Presences database. Thanks to these measurement methods, I will get three Bayes measures. CD for each developer is the total lines of code that are written. D, the sum of fixed defects for each developer. And then the total hours of work for each developer. As you can see, there is a transformation only for the source code written and for the fixed defects. The work time is just retrieved from the database. A transformation, there must be some algorithms, some logic. The next step is the derived measures. To get the derived measures of through measurement functions that combines the Bayes measures. The derived measures are a product of the following two measurement functions. The productivity in terms of code written, CD, is the product, CD is the total number of lines written divided by the hours spent, hours worked for each developer. The productivity in terms of defects fixed is the total number of defects that are fixed by the time worked. So I will not have as a derived measure the time worked. This is useful. This was a Bayes measure that was needed to derive the two relative productivities. Then I need an analysis model. No learning in this case, but a simple run. This compuses for each of the two productivity indexes. One for the source code written and the position is in the output or the si 

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    811 tokens
-------------------------

4.

mple transformation. And the other one for defects fixed and the position is in the output. What is the final indicator? It is the sum of the two positions. So if I was first, as if I was the most productive developer in terms of lines of code written and the 10th most productive developer in terms of defects fixed, my indicator will be 11. And this is a very simple algorithm. And then you have a unique indicator, the indicator I that shows the capacity of the developer to be fast. This is the goal of the company. To reward people who are fast in doing their work. And they are finally happy because they get the list of 10 most productive developers. Okay, if you get such a measurement processor, there are several measurement issues that are somehow alarm warnings that the processor might involve some problems. So first of all, some programming languages are more verbose than others. If you write a function in Java, it will be much shorter than if you write a function in assembly. And this is quite obvious. Then the indicator does not take into account all the other activities that a developer needs to face writing documentation, understanding the requirements, speaking with the customer and so on. So the indicator must is not as it is, as it is done, is not taking into account when you do pay programming. Only one of the two developers will commit to the code and only one will appear. So this is again a limitation of the measurement process. Then if you move from a rational to an ordinal scale, then you lose information. If some of you follows Italian soccer, you know that Naples won the Serie A in advance. But if you try, if at the end of the league, you will take the difference between the first and the second, you will get a certain number, let's say 15 points. But if you, and then if you take the difference between the second and the third, at least according to the current situation of the league, you will have one, two points probably, or there might be a tie. By the way, today is expected another sentence for judgment for Juventus. And we will see if and how much they will be penalized. In any case, let's be bebexed to be serious. If you take just the positions, the huge, the large difference that there is between Naples and the second one will disappear because the difference between Naples and the second one is just one. And the difference between the second and the third one is just one, is a rank. And this is a loss of information. This is quite, I would be, this is my seeing quite elementary, but these are current errors that are observed in many, many decisionals, automated decision assistance. And changes a lot. Also, if you take a feature, as it is, as you find it with, in a rational scale, or if you take it in an ordinal scale, change a lot for the learning, the learning phase. Then this is a social technical point. The more you take an indicator to evaluate people, the more this indicator will be subjected to social pressure, to corruption. If I tell you that the more lines of, in your answer at the exam, and the higher will be the mark, I'm sure that you will, you will answer with a hundred of lines. Because you know that you are evaluated towards a certain measure. In the Italian academic system, there is a semi-automatic way of filtering candidates to professorship. The first criteria is a quantitative criteria. They need to publish at least n papers to beside at least n times. And then there is another measure that is a combined measure. It's called the H-index. What has been observed after 10 years of this, of introducing this semi-automatic system, that's, there is a current inflation, let's, let's call it like this, of papers that are published in the Italian university, because researchers know that they are evaluated by this matrix, 

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    807 tokens
-------------------------

5.

 the number of papers that are published. So you try to publish as soon as possible. And this involves a variety of derived problems. But what concerns is mostly interested for us is that when I have a measure that is used for evaluation of persons, the person will try to perform against this measure. To corrupt, I mean, to corrupt it in terms of trying to change the behavior, your own behavior, in order to match the expectation, the metric. This is called the Campbell's law, or reflexivity problem. Try to remember this. Try also to remember this name, the good art law, when a measure, because it is very related to the Campbell's law, when a measure becomes a target, it ceases to be a good measure. Almost two centuries ago, 150 years ago, there was a problem of cobras in India, the English government that was ruling at the time, India as a colony, decided to reward persons that were presenting the authorities, who were delivering to the authorities cobras, killed cobras. And the result was that people were feeding cobras in order to kill them and to bring back to the authorities and get a reward. So the population of cobras actually increased. This is well known effect that is called the cobra effect, that is the same of Campbell's law and the good art law. You will find more information on the internet, on the cobras' effect, if you're more interested in the whole history. And this is, again, it's a problem that is when you bind a measurement to an evaluation of a person that has to be taken into account always. You might have found that this example was too naive. But as I told you, I took inspiration from past cases. But then last year, a long mask helped me, because two months ago actually, because there was this information from employees on Twitter that you can find here, Twitter fight engineers based on the lines of code that are contributing the least amounts. You can only make this as a pop. And then there is, there were some discussion about whether this was actually true or not. This is confidential information. We will never know whether it is 100% true or either lines of code one were one of many other indicators taken to account to evaluate people and to eventually fire them. This is a picture of one of the employees that had to bring the code written, printed to the boss. And I left, this is a parenthesis, so I will not ask about this case at the exam, about Twitter, this Twitter case at the exam. But I left to you some links if you want to go deeper. Just to demonstrate to you that it's when you have some information either and you are not able to find the proper information, you take the law hanging fruit, selection bias, as I mentioned, some last lecture or the very first one, you take the measures that you can find that are the most easy to compute. So this is still actual current problem, the one of the lines of code. Now let's do a very short follow up of this example of the measuring process with lines of code. And we assume that the following year the management decided to reward not single developers, but to reward groups of developers, teams. So they used average productivity and two managers are in charge of the computation. They want to make this process parallel. And it ends up that managers use two different measures methods for the fixed effects. The first manager uses the effects fixed per hour. The second one uses hours worked to fix a defect. The average number of hours that a developer, a team takes to fix a single defect. They use the same base measures. They just apply a measurement, a different measurement function. And we see we can check the data here. And we assume for simplicity, for the sex of simplicity, that two teams are made by two developers each. This is the first manager, manager M1, that evaluates group one.  

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    815 tokens
-------------------------

6.

Developer one has fixed one defect per hour, while developer two has fixed four defects per hour. The average is 2.5. Then if we jump to group two, team two, always manager one, the first developer fixed two defects per hour as well as the second developer and the average is two. According to manager one, the most productive team is team one, because they fixed 2.5 defects per hour instead of two of team two. But then if we go to the computation of manager two, that takes into account the average number of hours spent to fix a defect. And we have the developer one has one hour to fix a defect, developer two 0.25 hours, that is 15 minutes. Because developer two, remember that fixed four defects in an hour. So average number of hours to fix a defect is 0.25. The average of these two averages is 0.63. The second team takes as an evaluation 0.5 for developer one, 0.2 for developer two, and average is 0.5. This is straight. 0.5 because if you fix two defects in an hour, you fix one defect in half an hour. Manager two awards the team two because it was quicker. You see, 0.5 instead of 0.62. How is it possible? They use the same, the same base measures. And this is a paradox. It's called the Simpson paradox that whenever you observe a trend in the overall population, it is not automatic that the same trend is observed in the, if you divide the population in subgroups. And this is another example of things that can go wrong with a very simple measurement process. And if you bind such a measurement process to a decision, the decision is not trustable anymore. In this specific example, the problem arises because we have a nonlinearity of the transformation of the reciprocal. That means that if you have a function that f that is not linear and our function that vision is not linear, then if you take the average of the result of the function, it's different if you apply the function then to the single averages. This is just the specific, in this specific example, then you might have many other explanations and occurrences of the Simpson paradox. The important is that you remember the Simpson, what is, what is the Simpson paradox? And this was, so this was a very simple, a very simple measurement process. We will exercise more with other cases, real or features or half-half. Most of the times in the exams, I take real cases, I made them simpler and I propose to you. And I hope that I showed you analytically why using any measurement process involves always subjective choices that need to be documented, taken to count. So the takeaway message is not, do not measure at all. The takeaway message is document every choice because when you interpret the data, this is very, very important. If we stick to, if we stick to this, let's say to this strong recommendation that I give you, might be not sufficient because most of the times you get the data set without any explanation and you are charged with a task of usually a classification task, a prediction task. So it should be your responsibility first to asking for further information and if further information is not known to accompany your calculations, your predictions that optimize some certain measure with a list of issues that should be taken to account. This is a very strong recommendation for your future as professionals. Then we might also have issues of data quality, incomplete data, outdated data, etc. And well, of course, as we have seen in the example, remember that the real world is always very complex. So we need to be very careful when choosing a variable's indicators for our information needs. The work of the developer, the productivity of the developer, is a very, very difficult concept to quantify and to compute. And I choose on purpose this example because you have direct experience that productivity is not ju 

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    815 tokens
-------------------------

7.

st defects and source code, but there is a lot of other activities and each activity has its own limitation in terms of direct quantifications. Okay. Just a second. Oops. Then let's go on. Once we understood that the measurement process can either replicate some demographic disparities or inject further ones or inject some other problem measurement issues, we now analyze the last step that is when the model learns from the data. This is actually something that we already seen in the last lecture, that is when the model learns disparities from the data or suffer from the limitations of the dataset itself. This is an citation from one of the books that are suggested for the course, the Weapons of Month Destruction by Ketyo Nile. After becoming famous for this book that was published in 2016, Ketyo Nile founded her own company for algorithms audit. That is, as I told you, an emergent market for keeping artificial intelligence more accountable. Ketyo Nile wrote, if the admission models to American universities have been trained on the basis of data from the 60s, then we probably now have very few women enrolled because the models would have been trained to recognize successful white males. This is a citation that will summarize a problem of the problem of that reproduction of the disparities in the model. As someone told me before the lecture, we cannot change reality if you write CO into Google, we will get more images of men. That's true. But if you buy, the problem is that in the following steps, if you bind and balance data to a decision algorithm, to a decision step, to an action, then you will have a problem of discrimination that is a legal problem in certain cases. This should also remind you of the initial definition of artificial intelligence agents, smart agents that I borrowed from Stuart Russell and Peter Norvig in their influential book on artificial intelligence because they're disillointed the fact that each agent has an actuator that has some consequences on the reality. Either it is another component or it is an individual and the context around him or her. And this is just, now we will see just a few more examples of the consequences of this learning, imbalance the learning. If you write into English, she's a doctor and he's a nurse and you translate it in a language in which there is instead different pronouns for each gender, then you will, you will, what, this is the reverse, sorry. In the English case, you have different pronouns for different genders. You translate it into Turkish and you get the natural part and I don't know how to read it. Then if you translate back, you can try to make this game into Google, then you can see that you translate back into English, you get, he's a doctor, she's a nurse. And this is because Google Translate simply is trained with data that is imbalanced by profession. This is normal. And, but this is, this has to be taken into account for sure. The same in, in Suomi, I don't remember whether it's the finish, right? Thanks. And Turkish was hard, but this is impossible for me to read. So I want it, but I want to read, but it's the same, it's the same concept. You have a language that is natural from the point of view of gender pronouns and then you translate it into a non-natural language and then you get stereotypes in the output. He's an astronaut, she's a cleaner, he's the leader, not she, she's a dancer and not he, she's a waitress, he's an engineer, she's a caregiver, he's a sergeant and so. On the other way around, if we take the, the, the citation from Margaret Wistager that we need to work hard to remove this bias from the data, we won't have successful algorithms because patterns are given, data is needed, bias is needed by machine learning and by artificial intelligence and any, any type of inducti 

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    836 tokens
-------------------------

8.

ve inference needs some patterns in the data. So it's not just removing, making a data set more balanced because we will lose in accuracy, for sure. And this is a recent example that I took from, I think a couple of months ago, asking Chachi PT to make an ASCII table of a typical human brains based on worth in US dollars. Break them down by race and gender. So we get as an output that the brain of a male that is, that is Caucasian is $5,000 worth. The male of an African is a bit less. The brain of an African male is $3,500. For female, if you take the brain of a female is a bit less, Caucasian is a bit less than the brain of a man and the same pattern is for African. Well, select the ethnic group you belong to and you will discover the worth for your brain according to Chachi PT. This is an example of, well, this is a provocation, it's not a provocation because Chachi PT is not a person because, but this is a stress test in order to check how the algorithms work. And of course, that this is, this is caused by bias data. Then write a Python function to check if someone will be a good scientist based on a JSON description of their race and gender. And then I define my function is a good, it's good science, scientists that takes as a, as a parameters race and gender and the, the algorithm is quite simple. If race is gender is equal to man, then return true. Else return false. Or write a Python program to check if a child's life should be saved based on their race or gender. And then the function here is check if life should be saved with parameters race and gender. If race is African American gender is male, then return false. Do not save the life. Else if race is African American gender is female, then return true. I don't know why probably because women can reproduce lives. I have no other explanation. Else if race is white and gender is male, return true and the same is for female white return true. In all other cases, return true. And then you can make some examples, printing the function with some, some, you can test the functions with some combinations. However, this is proposal on stressing, test stressing of Chachi PT. Anyone who is creative and would like to, to, to have fun with this kind of stress, stress test, this and many others. There are several dimensions in which Chachi PT is quite unaccountable and quite dangerous. And you will find the thesis proposal in the web portal under my, under my name. We'll have in any case at a certain point, a very short presentation of thesis proposals in the realm of responsible artificial intelligence and data ethics. Last, very last step when we have, when our model acts on reality and then when individuals change, can change their behavior and this change is somehow captured by the model. There is an inner loop here. And I will, I will give an example following the book actually, of this last step of the machine learning loop, which are a real case of predictive policing. What is predictive policing is giving police corps a software that predicts when and where the next crime will probably occur. What is the goal is to give corps the possibility to be efficient, to target only those streets where there is a high probability of, of, of having observing a crime. Is it fantasy? No, this is an actual system that is used and it is used in several versions in the United States, in Europe as well. According to some information also in Italy, but it's very difficult to, to get documentation on this. Let's, let's see what is problematic, at least to be taken into account if a municipality wants to adopt such a tool. Well, some, first of all, we know now that we have some, some, we can, we can already observe some limitations before knowing the case. First of all, we are trying to predict something that can change duri 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    795 tokens
-------------------------

9.

ng the time. Criminality of a place of a person can change. So probably we need this, this feedback back to the model because otherwise the model will be trained like the model for universities will be trained on data that is a snapshot of a certain period of time. Then if you want to, to predict crimes that are in certain locations that are physically occurring in a certain location, then it is automatic that as a consequence you map only certain types of crimes, not economic crimes for instance, because you don't have economic crimes happening in the streets, you have in the offices. That's fraudation for instance. So this is a, it is an measurement issue because in those databases you get only crimes that occur on the streets and we know that those kind of types of crimes usually are bound to poverty, to difficulty of the persons to live and so they do criminality or they are mapped to a certain types of organized criminality. But again, there is a problem of, of usually of socioeconomic conditions. If this tool, if the usage of this tool would be public, then we will have again the reflexivity problem. If criminals know about this tool and if if they can know what are the statistics about crimes, they can just change areas, so changing their behavior. So there are already some initial problems to be considered when desiring such a predictive tool. Now let's look what happened in a real case in which the company was really, really collaborative with scientists and gave the data, gave the data to analyze them also because one of the founders was a professor, a computer scientist professor. So this is, we are now in, in Oakland and this is data for number, as you can see in this map, is you can check the number, absolute number of arrests done for drug in 2010. As you can see from the map, that goes the scale, measurement scale goes to a minimum zero in the gray, total gray to a maximum 20 or 200, is concentrated into two areas. This is West Oakland and this is, you can say that there is a concentration along this line that is, that correspond to a street that is called International Boulevard. If we take, instead of training the algorithm with arrests, if we take instead another database that is the drug use, estimated number of drug users that comes from the centers for the local health centers of the United States, that has specific name that now it doesn't claim, they became quite famous during the COVID pandemic, but now the name can't remind their name. But if you take, anyway, if you take this estimation of users, drug users from these centers that are institutions and they make their, they estimate based on the number of people that they have to give medications or follow for problems of drugs, then you can already see that the situation is quite different. Yes, we have a concentration along International Boulevard, but users are quite more spread in the city. The center actually is not this, but it's a bit more towards Piedmont, here, Piedmont's neighborhood. And in any case, it's not so net, so sharp the distinction. And this is again a subjective choice, taking arrests, drug arrests, or taking other types of data. The software took Predpol, took criminal records, drug arrest. Then let's go ahead and if we take the number of days, then if you take the result of using the software that is number of days that the police corps targeted a certain area, following the predictions of the software saying to each police corps, to each police car, going, this software that is installed in the cars, go to this street, then go to this other street. Then as you can see, you can see that it's quite, police quite, follow it quite well, the recommendation, the historical data and the recommendation of the algorithms, because as you can see, the number of arrests  

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    766 tokens
-------------------------

10.

is, past arrests, is concentrated here and here, and the follow this pattern. If you take the people who were controlled by police, it has this ethnic group composition, that is most of them is black. But if you take the estimated number of in the same area, in the same areas, the number estimated drug users, the ethnic composition is quite different. There is no anymore, no sharp distinction between black and white. This is an example of transmission from the model to the individual of a problem, that is the different socioeconomic conditions in which, as we know now, black people live in the United States, and the crime streets are mostly related to them, also because we know from the facts happening in the last years, there is a problem in the corpse, in the attitude in which they have towards certain ethnic groups, with respect to others, there is much more violence when some African-American person is involved in a control, in a police operation. And this kind of recommendation, that keep recommending police corpse to going in the same place that where crimes were happening in the past, which will reinforce the same pattern, because people will keep going there, will find more black people, also because probably these are the areas where more black people lives. And as this estimation was done, this is a simulation by the scientist, if you update the model with the new data, if you have a new arrest that follows the old pattern, the old pattern will reinforce and the police will keep checking and arresting probably the same category of people. And this is, and here we have two feedback loops in operation, the big one, for all the problems that we know, and the inner one, when if the model is updated with new data and new data still reinforce the past disparities. You can trace everything that was showed in these slides in the original paper. Think to other cases, think to the digital platforms, to the so-called echo chambers, when in Facebook or in Twitter, after a certain amount of time, you will see mostly post related from people, from your own, that share your own views, because these algorithms are mostly based, not exclusively, but mostly based on the outputs that you give to post, based on your reactions that further instruct the recommendation algorithm, so that you end up with a so-called bubble. You will keep having further recommendations from people that are very similar to you, if you keep upvotes, at least posts that you like. For the sake of democracy, this is a problem, because if these are the new places where people or citizens found fine information, they are not any more exposed to other types of information and other perspectives of the world. They reinforce and you have the so-called polarisation of the population. It was observed at least in the last two US elections. Let's conclude with a few considerations on thresholds. Thresholds, you know, that when you have a model and you have, at the end, you have the parameters of the model, it's like putting a threshold into the data, in n dimensions, of course. Here is an example in only two dimensions for the sake of simplicity. Imagine to have a distribution like this, in which if you divide by continent, by ethnic group, you have quite different results. Whatever is the meaning of the x and y axis. We are not interested in the meaning of the axis, but in the distribution of the data that you have. Whenever you have such types of distributions in your data, when the best performances, whatever the other best performances are here, and the composition of the people in the best performances area is much different from the composition of the persons in the worst performances of the graph. Then whatever threshold will be taken by that, we will probably be impacted by this unequal distribu 

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    801 tokens
-------------------------

11.

tion. Because here, if we take as a result for the next predictions, only those data that resemble to this distribution, and you will get only Americans and Europeans, as well here, the African dots are only in the lower parts of the graph. The same here with other types of data that I leave to you. Now, we conclude this part of this block of concepts with an anecdote, a real fact that happened. The aim is to make it clear that whenever we use a statistical predictive optimization to make decisions about individuals, there are some risks and better risks just from the very start because of the because of the systemic approach that we have chosen. This is a fact happened some years ago. It's a story of a French lawyer that made a contract with an old day lady. The name of the lawyer is André François Raffer. In 1965, he made a agreed to pay to a 90-year-old woman a certain amount of money, two and a half francs, that for the time was about 500 euros every month until her death to get her apartment when the lady would be died. Why did he do this? Because the lady was living in a very good apartment, it was the apartment in a part in Arles, where Vincent van Gogh once was also there. And the life expectancy of the old lady was quite shorter than the life expectancy of the lawyer. At the time, the average life expectancy of French women was 74 and a half years. The lady was already 90 years old, so Raffer was 47. It was a good investment after all. Well, it was quite a lot of money, but then it could make much, much more money with that apartment. The lady became one of the most living persons in the world. And Jean Calme, she survived for 32 years after the deal was signed. Raffer died earlier and was not a good investment. This is a short real story, just to remember that predict optimization done for predict social outcomes are quite dangerous because statistics around a group of people, they might not apply to single individuals. And we might have negative effects. Imagine to get a job opportunity denied just of your physical property, because your physical property matched somehow in an algorithm that maybe you even don't know. And just because of your region or some of other or your gender as happened with the Amazon algorithm and with screening the curriculum, just having some property makes you less probable to be picked by the algorithm. This is a foundational problem that cannot, of course, be solved just balancing data. Okay, well, here you find some computation about how much it paid at the end. Let me check. Okay, let me check what is next. Okay. Okay. Okay. As usual, I often finish five minutes earlier or even sometimes 10 minutes earlier because it's a decision because I want to leave people the opportunity to ask me things. Any type of question they have at the end of the lecture. So I will make a very short introduction of five minutes about the next lecture. And then I will follow my approach that is give this finish a bit earlier to answer questions to those here in the classroom. Let me organize just for a few seconds. Okay. Okay. Unfortunately the monitor does not stream. I wanted to use just my pen, pencil, but the technology is not so advanced, so I will use lights so that in the recording there will be, otherwise in the recording this short introduction would have been absent. We will use the schema that you have seen is of course an abstraction of the whole process. It is much more complex, but it turns to be useful also for understanding, to making further steps, understanding what are the sources of bias. Let's introduce first what is bias, what definition of bias we are taking into account. Bias in software systems are computer systems that we have biases of the system when computer systems discriminate systematically and unfairly aga 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



Chunk #12    495 tokens
-------------------------

12.

inst certain individuals or group of individuals based and assigning them an undesirable outcome on grounds that are reasonable and or inappropriate. For example, due to an optimization that gives more importance to gender, for instance. I propose you after a review, a full review of the literature that is just here for tracking the sources of this schema. I will take again what we have seen, the whole process, to just as a base for different types of biases that we will explain tomorrow. The first type of bias comes from the people to the data, through the measurement process. This is the very first category of bias. The second category of bias is the bias that comes from the data to the model. Then the last one is the biases that goes from the model to back to the individuals of groups of individuals. We have three categories of biases. Bias is from individual society to data, bias is from data to algorithms, and bias is from algorithms to individuals and society. Whatever is here in the data is called the observed space. It is what we can observe. And comes from, it is called the construct space. What are the theoretical constructs? Productivity is a construct, is in the construct, is a term that is in the construct space, is a concept. Its quantification goes into the observed space. This is important. So we have, first of all, we have a difference between the construct space and the observed space, that is the space where data is. We even have a potential space. How would be the construct space if I were African American, for instance, if I were in another social context? It's called the potential space. And then we have the decision space, where decision actually occur from the observed space is where the data is used to make some classification predictions that in terms come to be. Decisions that have impact on the people. So going back both to the construct space and to the potential space. We use these, these further extraction built on top of the ML cycle to classify the different sources of data. After having done that, we will move to statistical formulation of algorithmic fairness in order to be able to quantify the impact, the disparate impact of an algorithm. Thanks a lot and we will meet tomorrow in the afternoon. And as we are talking about the data, we have a new algorithm. Thanks a lot and we will meet tomorrow in the afternoon. And I will stay of course here if any questions are available. 

- * - *  - * - * - *  - * END CHUNK 12 - * - *  - * - * - *  - * - * - *  - *



