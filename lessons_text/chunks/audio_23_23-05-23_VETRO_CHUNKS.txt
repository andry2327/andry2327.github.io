Chunk #1    694 tokens
-------------------------

1.

 Today is the second session of a solid exercise. Can you hear me or not? Yes? And like this, can you still hear me? Yes? Okay. Second today is second session of exercises. Before that, I would like to make two things. First, a couple of announcements. And secondly, I would like to quickly go back to the last exercises of last week, so that we can comment the solution that you already have in the portal. And then we will start the new session of exercises today. It will be devoted only to fairness computations. Regarding the first part, announcement. So first of all, I would like to remind you that I think that until tomorrow you still have the time to vote your student representatives. So I encourage you to inform, to be informed and to vote if you would like to have a meaningful representation. Then I already sent you a message on the portal with a warm invitation to follow either physically or remotely or to look at the recorded video of this seminar that will be in Italian. The presenter, the speaker can speak also English, can answer to English. In any case, I think that the topic of this seminar is very close to most of the topics that we have covered during the course. So this is not part of the exam, the seminar. It's just encouragement that I make to you. If you'd like to deepen with a scholar who is a, she's a philosopher from University of Pisa. She's very in also the technical details of artificial intelligence and her thesis is quite radical because she will show that according to evidence that she collected, she will show that some of the other or even many of the current applications of artificial intelligence are violating human rights. So it will be an hour presentation of an hour discussion. So just a warm welcome. If you would like to deepen the topics that are touched in this course. More in general from the, from aspects of data ethics that we will not cover during the course, but still very relevant. In June, there will be a seminar this time in English that will be taken by, will be given by Milagros Micheli. Milagros Micheli is, she's a well known scholar. She's working both at the Weizsemberg Institute in Berlin and at the Dyer Institute. This is a new institute that was created by Tim Tegebru after being fired by Google. Do you know who is Tim Tegebru? Nobody knows her. So Tim Tegebru, let me make this parenthesis because it's important. She was part of the ethics AI team in Google. And about four years ago, she wrote a paper that was quite important for the time. Stochastic was the title was on the dangers of stochastic parents. She was learning already the time on the dangers of applications based on large language models. Of course, she was already working on the birth model of Google. There was not yet, charge GPT was not yet there. And she was fired after this study. She and other colleagues, she became quite famous in all over the world. And after that, she founded the Dyer Institute, which is a distributed, let me show you, is a research institute on artificial intelligence. Which has the goal to improve diversity in the AI landscape. Interdisciplinary globally distributed research Institute. Rooted in the belief that AI is not inevitable. It's are are preventable. And wh 

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    660 tokens
-------------------------

2.

en it's production deployment include diverse perspectives. And I will show you what is diverse perspectives and delivery processes. It can be beneficial. And let me show you people involved. So she puts together many important scholars from all over the world. With wide representation. So now the pictures are not showing of ethnic groups of non binary gender, like Alex Hanna. Many people from Africa, from other from South, from the global South. So trying to propose an alternative development of artificial intelligence that is not the Silicon Valley based. This is the, the, the main purpose of the Dyer Institute, one of the main purposes. Milagros Micheli work also in the Dyer Institute. She's one of the most important scholar in the field of understanding documenting the human labor behind artificial intelligence. And she documented in a careful, well detailed way, the work of labellers. So the people who take huge data sets and labell them in order to allow the big companies to have the so called the golden truth. So labellers data sets in which they can train their model. And she she documented the way labellers work, especially in South America, how they're paid the condition of working, what their work is about. So it's a very important part of the data ethics issues that we are not again and we are not covering this exam but I think it will be very interesting and formative for you. It's up to you if you want to deepen this knowledge will be on Wednesday 14th of July at 5pm. This part of the next Wednesdays that are organized with one hour discussion and one hour, one hour presentation, one hour discussion, it will follow a news on the on the portal, as usual. Okay, this concludes the part on the announcements and let me go back now on the previous on the solutions of the exercise that I were left from last Tuesday. I'm going to give just an overview because an answer to questions because solutions were already published since one week. This is car insurance and this is Facebook advertising. Let me check the recording. Okay, let me start from the from the last one. Facebook advertising platform. This was a special exam special because it was in the special session. The one which in which you can enroll only if certain conditions are met if you are left. I think it's a very few exams and and other conditions that are of administrative nature. And it was special also because probably it was very, very too much simple. It was very easy. So this is below the average of difficulty. Nevertheless, I still had the alpha the people who couldn't pass it. And I don't know why. So this was inspired to the real case of the Facebook advertising platform. I assume that you already read the text. So I'm not reading again the text and just guiding you for the solution is already published. And the core of the solution was to identify proxy attributes. So Facebook could not use anymore. Directly ethnic groups. And this is also as is reported in the exam text. So could not use directly African American to target people and to let African American people receive special advertisements or Asian people receiving dedicated advertisement. This is illegal from certain domains. So into introduced other categories. And that 

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    642 tokens
-------------------------

3.

 is the following ones. However, discriminations discrimination issues were still there. Why so the question was why the discrimination discrimination issues remained also after the removal of the racial categories and for which ethnic group. This was the question the first question. And what was expected that you could do was to analyze these list of categories and identify for which ethnic groups they are proxy. For example, Boca, you know, soccer team. Is clearly a proxy for Hispanic or for Latino, let's say. So this is a proxy we can use the yellow. Oh, I smacked country music is a proxy for the whites for the. White Caucasian ethnic group. And I'm who's is a proxy for the East culture. Usually. Notice hero Univision was a proxy again for Latinos. Black girls rock. Pink is quite self. Black lives matter is a proxy for African American. Harvard is a reasonable proxy for white Caucasian. Was green. Telemundo is a proxy for Latinos. Black lives matter is a proxy for African American. Black lives matter is a proxy for the East so blue Maldives holidays usually they are a destination for if you have quite a lot of money and for the global now north people that goes to Maldives Maldives as another country we can we can target green Latin music. Well, it is. Is from Latinos. Gospel music and hip hop music is something that is related to African Americans. And Bollywood movies is something that is related to India so to the East. My at least my also my goal was quite. Was quite easy to identify this proxies. You didn't have this XM to come up exactly with the same organization. But you should have at least identified the most important ties. And this actually this is you will check in the solution or you already checked that this is actually everything is true. I taken some of these categories from the real categories used in Facebook and some others are invented to make them more explicit. And as a social technical intervention. And the suggestion where well first of all, most obvious to remove the proxy attributes but this is very, very obvious and whenever a correlation between a race and new categories arise so kind of monitoring algorithm that could check every time a correlation between a certain category used and. And anti group is let's say above, assuming 70% for at least two three weeks. Be something like this. And then you remove that category. So saying just removing the proxies is not enough, you should propose something is this original so this is a very general example. Then you could have a calibration of the recommendation this is what you find in the second part of the question. You should do not let the commander go alone with its own recommendation and put cups to thresholds cups sorry to recommendation so after reaching a certain level of targeted people. This is another example with African American American after you reach let's say 50% African America that are targeted with some advertisements, you buy you stop recommending them a certain advertisement to them and you increase the recommendations to the other groups. This is another example of possible improvement to mitigate the problem. Questions. Okay, I assume there are no questions. Then we have the car insurance problem. So this 

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    653 tokens
-------------------------

4.

 here was an exam in which you already have discrimination you should have should explain it with several categories let's comment to the graph that you already checked. So we have the target is repaid accident cost. This is not explicitly listed here. However, as it is written, it is a very implicit, a very direct assumption to say that is every as a road to reply the accident costs are assumed to be the target variable for which the company has the historical data and the strain of the risk of accident costs. Because you don't have a didn't list on a target variable but I told you that the company is using historical data is collecting all the specific characteristics for the goal of a, I didn't predict the higher risk of accident costs. So the main assumption that is taken the retrieve the form from the text and you could reason in this way. So first of all, we know that let's take car. And it has two levels, large powerful cars that are on average, never reach most expensive, more expensive than small cars. It means that preparing them. If you have an accident cost more, maintaining them cost more. And directly related to higher cost but also to wealth, because if you take a large powerful car, usually costs more than a small car. There is no reason in terms of used the used cars, but still the comparison between the relative comparison between large powerful cars and the small cars still is kept also for the used market. Assume the correlation with wealth and wealth correlates with a team group. We have one possible explanation of the discrimination that is, that is observed that is related to car. The other explanation is birthplace that is clearly related for the way the attribute birthplace is related to ethnic group. The relationship with accident cost is weaker. I exposed you in the solution, a reasoning in which if you were born in another country with different rules, so you might have a different driving style, you might have different habits, at least for a certain period you might be more exposed to accidents because you are driving in a different context. I assume this, I assume this as a weak correlation, not as strong as the correlation with car. Okay. That's why I put it with a dotted line. The reason that is correlated to accident cost is the city because if you have historical data, so assuming that historical data is trained and you have the company has the history of accident cost, we know that there are large differences between cities in Italy. There are many different levels of premiums for the same car, different in Naples and Milan, in Potenza, in Trento, this is general knowledge, and well this is also general statistical knowledge, and you could, it was acceptable to also have this correlation, although the correlation with ethnic group is weaker. Because you have to assume that certain cities have relevant, have relevant proportions of people with other ethnic groups. If you make this hypothesis, then you can, it is acceptable. Of course, the more you use a car, the higher is the probability to make an accident and to have higher cost, but this is not related to ethnic group, so it's not an explanation of the discrimination. The same for age, usually the younger you are, th 

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    665 tokens
-------------------------

5.

e higher is the probability to have an accident. That's why you pay more with your insurance. Well, unless you could say unless you inherit the risk, the level of your parents, there is a special law in Italy that allows that. So far so good because it doesn't change nothing. There is no correlation in this context, this specific context with ethnic group. So certainly we have a correlation with the target variable age and then claim history. So the younger you are also, and the longer, most probably is the claim history, but you could not explain the such such the discrimination with such with such variables. You notice that this was not invented. This was a real audit made and you find the link. Then I made a turn it into an exam. But I think I put the link, right. You have the link of the of the scientific paper and then the new, the paper was also had also media coverage, the Republic and other news, other national newspapers cover that this scientific study actually made from colleagues or University of Padua source of some Italians. This is a possible explanation. This is the starting point of an audit. This is what you are expected to achieve at the end of this course. Then, of course, you need a statistical data to check that all these hypothesis are correct. In real life, you would need to check all of these, all of these links. But this is the starting point when you do a nodded. This is the model, the casual relational model from which you start to identify the risk of discriminations. You have to take action against this to mitigate the possible risk. Okay, nothing to be to remark on the most common errors in this exam. An example of measurement issues. You will find that my reasoning is about the future space that is very limited in terms of the actual factors of accidents. So my reasoning is that the information that is collected is in any way poor, because we have other risk factors that are that are more important, like the age of the car, the status of maintenance of the car, the status, the weather conditions, the status of the roads, and so on. But this is a possible, possible issue that you could remark in this specific exam. The model is that is built, that the features that are selected are not the best. Then you have here the fact that the luxury cars are a very, very special segment that could provide some outliers. And that could, that should be devoted a specific model. The number of claims should be normalized by age of a number of years driving to avoid this link. And finally, the ethnic groups was collected without considering other attributes and why I mentioned it here. And in the ethnic group, we had only levels, where it is, Caucasian, Black, Asiatic. There might not be representative for the Italian population, taken from the common US attributes, and some other ethnic groups are missing. So a category order should be placed. And there are examples of measurement issues that were acceptable. Others were also acceptable, depending on how you present them and the reasoning that you explain. Most common errors that I found in the exams, that you repeat the discrimination analysis also. You present some discrimination issues as measurement issues. No, that was part one of th 

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    647 tokens
-------------------------

6.

e question. Or very poor explanation. So you indicate a measurement issue without explaining it. Remember to explain everything, otherwise I cannot really accept it. Now, third questions. And here was really a bad surprise, because I asked which changes in the experimentation data collection process would introduce to check fairness in terms of separation. And most people, really more than 50%, did not read this last part of the question and automatically thought that this was the common question of improvement. No, actually asked a very specific thing. What do we need for computing separation? We need error rates. So we need to collect the actual data after doing the quote. And this data that is collected and this part that is treated in the exam is only the data entered when you get a quote. You want to make insurance for your car, you connect to the website and you get a quote. If you want to get the error rates, you need to continue the experimentation and collect data after the quote to check whether the quote was the risk level that was determined and that was proportional to the quote was adequate or not. So this was the improvement. It was just one line and was enough to answering this question. And unfortunately, many of you, many of your colleagues, not you lost this one and a half point. These questions of theory is mostly related to the part that was covered by the Martin. I will repeat as an example for theory questions from my side, take the exams from last year for as examples of theory questions that I could make. I have questions. Otherwise, I will close with the solutions of the last time. Okay. I hope that this was useful for you and recorded it. Let me introduce the first exercise, because this is a new type of exercise that is fairness computations. And the first exercise for today, we will solve the first one partially together and then you will be left with the other two. I think this ever flew symptoms for elderly patients. This was the special session of the second November of few months ago. I think that I used this, this is this first exercise to make you to make for your simulation of exam. If you are already active, you should find already on the portal, through the platform exercise, you should find already a hypothetical exam with this exercise plus theory questions. It's important that you try it. In order to check how exam platform work in case we didn't have the opportunity to check earlier whether your laptop works good with the lockdown browser. And I cannot accept that you come at exam and you say, I don't know how to use the exam platform. You have already well in advance a simulation exam that you should try to check that that your laptop or your browser compatibility, your hardware, that everything is fine. So please use it just to be sure that you won't have bad surprises at exam. Now let's go to the exercise predicting several flu symptoms for elderly patients. Data driven algorithms can be employed to screen and predict the risk of various forms of diseases, including common flu. They can find patterns and links medical records that previously required visiting a human doctor. Algorithmic predictions can be used by patients to decide whether to see a doctor for  

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    692 tokens
-------------------------

7.

the flu. Suppose we have two different algorithms predicting the severity of flu symptoms in patients and would like to decide which one should be deployed in the real world. So you have two algorithms read the two models is the same. We don't have the features here that they're useless. We have the results of an audit and experimentation each algorithm was tested on 400 elderly persons. So 400 for algorithm one and the 400 for algorithm two. And results are shown in table one and table two divided by gender female and male with a binary classification female prediction to develop a severe flu symptoms is marked with are equal to one in the tables. And the results are equal to zero and why equal to one is the target variable. So the actual development of several flu symptoms is marked with why equal to one and why equal to zero. So this is the common format. This is the format that in the slides you find for computing a separation. So this switch of the two algorithms is preferable in terms of lower systematic discrimination by gender and why support your reasoning by showing computation of a fairness criteria one only one. This is important because sometimes I write one, I underline only one and I still find more than one computation. And if you make errors, one of the computations. I cannot take only the right one. So stick to the, to the question only if I write only one. And especially here I wrote and repeated only one computation. And if I ask only one computation is because the answer is the same regardless of the fairness criteria that you choose. I design tables in such a way that's doesn't matter which fairness computation you do. This was four points. And now, and then the second part is considered a possible errors of the predictions and their social implications in the context of application. So which common limitation of the two algorithms do you observe. So even if algorithm one or two, and you will see in Hawaii is preferable. The question is still in common in the two algorithms and considering I think I help here, the possible errors of the predictions. If you want to compute independence, you might have had if you studied the mechanically the, the computation of independence, you might have had made the errors of taking only 20 year and dividing it by, by 100. So in the slide you have one table in which you take this cell and you divide by this one, but the table of independence is slightly different from the table that is built to compute separation. So, I call it a difficulty because I saw this error many times the difficulty was. And so I think that was because of the pressure of being in an exam. The difficulty was to let me take the pen here to compute independence. Wait a second. You should take 20. When the pen is the probability of predicting positive. For a given gender. So who are the positive ones for my female 20 and and 10. I then buy. 150. The probability that are is equal to one given that the attribute is a female. 20 plus 10. The sum of the ones that are predicted one divided by all females. Right by 150. The teaser 30 devices by 150 and was 0. The same for. So this is forever is one the same for me mays. It is. 60 plus 50 divided by 250. This was 0.32. The delta is 0. 

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    829 tokens
-------------------------

8.

12. You should compute the gap between female and males. The answer is 0.12. Do you have questions or is clear. Yes, please. We are not. We have to make. We just have to say. Which of the two algorithms is preferable from the point of view of gender discrimination. So the one we should identify the other is that discriminate less between genders. You don't have is not is another type of case is not a case where you should reconstruct and explain the cause of discrimination. So I cannot even ask you a question because it's a totally a black box here. And the goal is not to explain discrimination is to check in terms of fairness criteria. Yes, please. 50. Yes. Yes. This is. Sorry. Wait. Yes. Yes. This is how I underlined. And this is how I messed up. Thanks a lot. Sorry. Yes. Zero dots. Yes. So I took algorithm two. And I put the results of algorithm one and made the comp. I didn't realize that I was already an argument to sorry. And this is algorithm. True. The steps are correct. I just messed up with sorry. I'm sorry. 60 plus so I'm going to is 20 plus 10 20 plus 10 divided by 150. Mail 60 plus 50 divided by 250. I apologize for making me a little bit of a mistake. I'm sorry. I'm sorry. I'm sorry. I'm sorry. 250. I apologize for making me this confusion. Algorithm one is 20 plus 10. Divide by 150. So I got it one. Is for female. Is. 30 divided by the same. Zero two. Then I will. For me is. 60. 60. Plus 20. Divide by 250. Okay. Wait. I'm making again confusion. So this was. Zero 44. Delta two. Is. Delta two. Here. One. So the delta. Is less. For an absolute value. For algorithm one. So we prefer from the point of view independence. If you selected independence, I'll go with one. Yes, please. No. If you put only the results, I cannot accept it, but if you make a computation. If you want to be sure you show also the steps. At least for one so that I can, I can differentiate if you made just a computation error. It's a really minor error and. Or if you made a conceptual error in the computation that made a difference. I wanted to make this computation because I found many errors because of the shape of the table. Many, many in this exam. When computing independence. Can I go on with separation and sufficiency. Sorry. Okay, separation will be in blue. Okay. The palation concerns the error rates to positives. Post positives. So there are more computation. However, because the shape of the table is exactly the shape of for separation was easier because of the size of the table. Because for algorithm one. The number to positive is given by. This 20 because they are. Because they are the female that are predicted one. In the right way divide by 100. So this is. 20% of 020. For male is the same 60 divide by. By 150 that is a zero four. And the delta here is 020. 24 to positive. The same is for a green tool is 20. Divide by 100. And then is 60 divide by 180. And if my notes are not wrong, but should be 033. Delta here in terms of two positive is. 013. Sorry. Here. Say again, please. No, we're taking a service separation, 60, 60 divided 180. 60 divided. So I'll do it one which cell here here here, right. I don't understand. I think I understand. Okay, yes. Okay. Maybe I was so this is the delta of this and this is n 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    738 tokens
-------------------------

9.

ot the delta of this line. This is the total. Okay. The total we don't. We don't take care of this because you have mixed information female male. Okay. Can I go on? What is positive. What is false is what is predicted positively, but was not positive was y equal to zero. So is 10 divide. By 50. Divide. By 50. This zero two again. And 20 divide by by 100 is zero two. So here the delta is exactly zero. And obviously is the table below since it is I presented it in the same way is 10 divide by 50 that is zero two. And then is 50 divide by 70, which is zero 71. The delta is much higher. But here you have a particular situation because you have. You have that the delta is slightly better for the true positives for algorithm two. You see here the delta is zero two here is zero 13. However, the delta for the false positive. This is the second condition for separation is so high here. That's a algorithm one is preferable also in terms of separation. Did you understand. But here they're very close. But here this difference is huge. Outperforms these these these friends. Just waiting a few seconds if there are questions. Just waiting a few seconds if there are questions. Just waiting a few seconds if there are questions. I didn't do an average. Because it was so clear that an average could. You could say they are equivalent. Or if you choose the average. You can use the average and say making the average of the two differences. You could also reason it's more important in this specific case to have a wider delta over two positives or false positives. You explain why and that's fair enough. But if it's the explanation is fine. Okay. Okay, last separation and then we will start with the you will do the exercise. Well, then we will answer to question two. And then you will start the exercise. Okay. Let me check. Check. Okay. This table was the table is the table that is. Mostly prone for separation computation. It should be reversed to to make a computation of sufficiency. If you want to stick with the slides otherwise you could reason in these terms separation that the condition in an insufficiency sorry is that. Is on the. Is on the classification probability that why is equal to one given that are is equal to one. You can check quickly in the slides but the vibration is that given the positive class. Given the condition is on the classification. And for a given attribute. This is the short form. Whatever a. Okay. For the bias probability. This is equal. To the joint probability. Why equal to one and are equal to one. Divide by the probability. Of equal to one. Do you agree. So if you remember even if you you get a table that is not. The right one. If you remember the formula you take. What you take you take 20 so let's let's make the first this first part of sufficiency. For a good one the probability the joint probability that why is equal to one are is equal to one is always 20. But the probability that are is equal to one is the sum of the cells where are is equal to one so is 20 plus 10. And so let me check. Way to make it clear. Positive predictive value. This is the first term of sufficiency. For female. And so let me check. The way to make it clear. Positive predictive value. This is the first term of suff 

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    824 tokens
-------------------------

10.

iciency. For female. And so for algorithm one is. 20 because it's the joint probability of why equal to one are equal to one divided by 20 plus 10. It are all females that were that were predicted one. Sorry. And this is 067. We are. The positive predictive value for males is 60. Divide by 60 plus 20 for the same reasoning. You follow me. This computation. Yes please. Can you raise your voice because I cannot hear you. Why. No I meant for to be short. I wrote the regardless of the gender. It means for each gender. Yes for each gender. Sorry. Take as a reference the slides and the formula here. I wanted to make it short and fast. Okay. Then for algorithm two. Is a symmetric that is. 20. Is the same for female. While for male is 60 divided 60 plus. 50. And is equal to. 045. You have here a delta of. Zero dot 0 8 and here a delta of. Zero. So they are mostly equivalent but I will in one is better. Now let's compute. The. Pulse of omission rate. That is. Similar because. The. The. The. The. The. The. The. The. The. The. The. The. The. The. The. TheÓ The EMIR is we're putting. It's similar because. Pulse of mission rates for females. These were 3 reasons in terms of are equal to så. Or. What. Bad. THEAA. WHIGH skilled rules. The probability that Y is equal to one given at R is equal to zero. That is for bias. The joint probability. Divide by the probability. That are equal to zero. And so this is. Y equal to one are equal to zero is 80. Divide by all. All the lablet zero. Is 80 plus 40. So this is 80. Divide. 80. That's 40. And should be. The. Yes, 067. Or for males is. 90. Divide. That's. 80. Is 053. Delta is 014. And for algorithm 2. Is is symmetric because it's 80 divided 80 plus 40. So 067 is the same. For female while for male is 120 divided by. 20 plus 20. Is 086. Delta is 019. Right. Yes. So false omission rate is less for arguing one. Positive predictive value is less for algorithm one. So again, the answer is one is preferable. Now quickly because otherwise you won't have time to make your exercise by yourself. About the second part of the question, this limitation that is in common of the two algorithms. Regarding the errors, errors can be either false positive or false negative. So if you look at the number of false negative in this example, they are very high. They are. Is 80% for female 80 divided by 10 hundred. 80% also in algorithm to 80 by divide by 100. You see they are false negative they are predicted negative but they were positive. And is 060 for male 90 divided by 150 and 057 67 for algorithm to 120 divided 180. This means that with 80% of false negative and almost 70% of false female. This is in both algorithms. This is a huge limitation because you cannot trust the negative predictions. So it's like you, you don't predict the flu as a severe when it is severe and this is a big social implication because you are missing people that should be treated carefully with a higher with higher medicaments. Okay. Did you understand this point. So this, the, the first negative are still very high. You could even not compute the first negative appointed to the number of errors. Even without computing the exact percentages. By showing that's a 90 over 150 showing computing all the errors. Okay. Let' 

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    686 tokens
-------------------------

11.

s wait a few seconds if you have a questions. You can now read my yourself exercise to because it is very, very close to this one. So it's a very good exercise because it's very similar. And then you go on with exercise three. So you do two and three in a row. Christian is here that helps you. You manage yourself with a break. And in about roughly an hour and an hour or something. I can check with you the solutions without doing all the computations because they're the same. But I will upload the solutions you can check. You can ask me questions. We stay like me until the very end. And that's all. So you can start with exercise. Exercise to and exercise free. Make them in a row. So, as I was saying, the first question was identical while the second one was likely different because I asked you to focus only on the second. Well, on the algorithm that you indicated as the most preferable. And to identify only one limitation that you still observe on that algorithm. That was identified. I was saying that I remember that many people left indications for both algorithms. And again, it's a problem of reading carefully the question. I'm just telling you the frequent errors that I saw. You will see the results of the computations are not doing them step by step. I uploaded the file that we did earlier. And I think that you can check whether your computations are correct. And if you don't understand why you can, we can check it together. You ask me or Christian and I will come. And regarding question two. Again was was you could, you could reason in terms of error rates, both in terms of false positive rate and in terms of false negative rate. Agri two was the most preferable. But even if it was the most preferable. The error rates are so high that should be not deployed. The limitation is on the error rate because we had approximately on average six out of 10 patients, regardless of that any group or 50% for white and 70% 75% for black. And if you made the analysis separately that are false positive means that they go unnecessarily to through the high risk medical interpossible interventions or through further treatments and this is an unnecessary cost. You might also provoke or damage the person. If you make some, how they're called some analysis that are very invasive, for instance, so this is the cost. Also not in economical terms of a false positive. False negatives are also very high is 50% for both subgroups. And so you, you are losing half of the of your or the negative predictions and given the context attended is cancer prediction. This is not good. So this is a limitation of Agri two. The results are even if separation was better. And in terms of all through positive was even at zero as a delta and the false positive was was not okay but was not horrible was 25% The rate is unacceptable. So this is for. This is all regarding exercise to exercise one. As I said, you had four tables because there were four versions of the of the exam. He was asked to comment the results and to list any other ethical issues including measurement issues. So both questions were on purpose very broad. In the guide to the solution you will find the computations for independence and separation and examples of comments for indep 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



Chunk #12    654 tokens
-------------------------

12.

endence and separations for example, you have that independence is respected in almost all cases. So if you have a delta of six, seven percent. You have a case that was free. So if you had the table three, and if you computed the independence, you have that the probability of positive classification is cured towards foreigners. In fact, table three. So table three 31 against 076 with a delta that is considerable. So here you have a problem. As separation is concerned, we have in all cases that the true positive rate as it was observed by some of you is much higher for French people rather than non French people. And I reasoned in terms of data quality given the type of data that is collected. We could assume that quality of data is higher for French people, and less foreign instead for non French people. For example, I hear I made consider that you use. If you are a German and you use Amazon dot there. And this data is not tracked in Amazon dot FFR you use a different domain use different commercial website. It's just an hypothesis, but it's a plausible hypothesis that could be the reason for these relevant difference in true positives. Then in false positives that they were was very similar in cases one to four, not for case three was a considerable difference 49% more for foreigners. So you could not trust that that predictions so that here was made on purpose. The possible answers were really a lot depending on the top and the table that you received. So it could be, could be meaningful to check the type of measurement issues and generally speaking ethical issues. So first one that is only high volume e-commerce sites are analyzed and not other type of purchases that could be more, more relevant to understand the level of wealth of a person that could not correspond to the tax that are paid. And a social network access is not uniform across the population but also the patterns of usage of social networks are very different. Old people, younger people, etc. We already discussed this in theory. And since the social network is a source of data uses here this should be taken into account when relying on these measurements. The wealth indicator is compared with the average of the last three tax returns. So this is written in the text. However, for foreigners who moved very recently to France, you might have no data that represented that kind of population. So this was a possible measurement issue. And then this was very broad and I could accept it only if it was not the only issue was identified that you, the purpose of that collection is different here from the usage made by the authorities. And the usage of data that was collected for totally other purposes. However, this could be legal because when you use Facebook and any other website you, you agree that your data is used in any case. If you think that I invented everything, this is wrong because again I took inspiration from reality and you can find some links here. So this closes today's exercise I will stay here until the end if you have a specific question. The next week remind you that we will go ahead with with theory on Monday, and probably I might present you some also set thesis topics. And on Tuesday, we will make another session of exercise 

- * - *  - * - * - *  - * END CHUNK 12 - * - *  - * - * - *  - * - * - *  - *



Chunk #13    134 tokens
-------------------------

13.

, the category of exams will be on discrimination risk. It's very similar to explanation of discriminations but you don't know whether there is a discrimination or not. You should identify the risks of discrimination. I will not solve next time since the typology is very similar to explanation of discrimination so I will not solve together the first exercise as I did in this first two slots. So, since the beginning, you will be given the text, you will start doing the exercise. I and Christian will be here to answering. This is how next week is organized. Okay, thanks a lot. And if you have any questions I will take otherwise have a nice evening. 

- * - *  - * - * - *  - * END CHUNK 13 - * - *  - * - * - *  - * - * - *  - *



