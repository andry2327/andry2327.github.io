Chunk #1    1085 tokens
-------------------------

1.

 Okay, good afternoon and welcome back to the lecture. Let me recap what we started last week. We started formalizing what are the sources of bias in the typical ML AI cycle. May I ask Kandri if someone you can close the door in order not to disturb my colleagues. Thanks a lot. We started with the identification of taking as reference the previous model of the following categories of bias bias from individuals to society bias from individuals to data bias from the data to the algorithms and bias from algorithms to back to individuals or society so that these are the three categories of biases and then introduced the terms of construct space observed space potential space and the decision space showing you that they are all connected. I will now move forward using formalization that will be used throughout the course. And that I will start using the blackboard so that in the following slides you still have as a reference, you will still have the most important parts of the formalization. So as you know from many other courses, usually, the prediction algorithm is simplified, formalized with this simple terminology that is you have a target variable Y that is a function of a set of features. Then the function can be as simple as a logistic regression or as complex as neural network. This is always some kind of error. This is the phenomenon that you try to study to approximate. This is what is called the construct space. The space where we study the phenomenon and here above we have what is being called the potential space. So potential space to the construct space we see that occur the historical bias with we occur so structurally in a quality in society that makes a potential potentiality potential capabilities of individuals less expected less effective because of a variety of cultural, etc. cultural explanations that we somehow summarized in the past in the past lecture. And then we have when we collected that we have something like this because we as it was repeated several times. The measurement process can introduce some some error, or you might have a proxy of someone that you want to measure and not a direct measure. So you can use the term x with these heads x with these heads that is generally the collection, the data collection function, you want to measure something and, well, of course, there may be also an error or this is always the data, the measures that we get is always a function of the real measure, sometimes is impossible to measure the real construct. And so we use a proxy and we use some kind of ideal function of the real variable x, and we get x with the heads, and the same is with the target variable that is a function h of the real variable that we put or not remind that there is always an error. This is what is called the observed space. Going from the potential space to the customer space we have with my tab some introduction of some kind of bias, in particular, historical bias here, we usually have the so called the measurement bias from the construct to the actual observations. So I want to measure happiness, happiness is really hard to be quantified and I get something that I cannot solve, but here I have some distance from the real construct. So if we are using our model, then we can, yeah, there will be of course, we, given these measures we can build our model, classification model, that will give us some predictions, specifications, we call them are, I'm using a sticking to the, to the terminal to the remainder of the book, fair and male book, even though you don't find this in the book but I want to create a direct link with the book in case you will use it as a reference. That is a function of the observed x. Yes, please. We live on the top after this. You have everything in the slides. I'm just using the blackboard because then I will move from slides and I think it's useful that you can still check that. And then since we are interested in predictive optimization, that means decisions about individuals, we will always have a decision that is made based on either on the classification alone or using the logical or a mix of the classifications that protected attributes we call a to stick to again to the book formalization some other environment. So that means that I could take. In consideration, not only the classifications but also some specific attributes of the person. Or some other variables, environment variables, depending on the decision process is very general. So this take into account whether you the decision takes considers many other information. So here we go to the decision space. Every step that we do now. We have to hear. In practice, in a typical pipeline of automatic decision making that is automated, we can introduce there is a risk of introducing some kind of bias at every step. And to remember the lecture the past lecture on the measurement process. We know that especially here. So, when we go from the concept space to the server space, we have a lot of subjective decisions that can insert limitations errors biases. Here in this measurement process, we have all the inner steps that we have seen last time. We use as a base disformalization to analyze the different sources possible sources of biases in in the ML cycle. Any questions so far. Okay, so as I said, everything that I wrote is present also in the slides so I will go fast just checking that I did not forget anything. 

- * - *  - * - * - *  - * END CHUNK 1 - * - *  - * - * - *  - * - * - *  - *



Chunk #2    1088 tokens
-------------------------

2.

 I remember our definition of protected attributes that we decided to stick to the article article 21 of the European chart of fundamental rights. There can be gender can be raised can be color ethnic or social origin and so on. Anytime that I ask for in the exam for discrimination risk or explanation of a given discrimination should always the discrimination should always be with respect to one of these one or more of these attributes. I don't have any on some other attributes that are present variables that are present in the text. Please remember this. Yes, as I say, then we have all the steps that brings us to the decision space. So, I'm just going to go back to the mapping that I also did there. Then what else we need to further analyze all possible biases. We introduce a C a variable sees as a capacity of an individual can be an economic resource. So, property can be also some personal talent skills, whatever a capacity that you have. You can see it also as a resource, human resource that you have or a practical resource that you own. And then it's taken into account for prediction. And then we can have a proxy variable that you might be able to track instead of the capacity. The quotient of intelligence is a typical proxy variable we cannot measure intelligence we use the so called intelligent intelligence intelligence quotient. Equal. You can have proxy also for any other types of variables also for the target variable or for the protected attribute eight or for any additional variable that we call we will call a queue. So, the additional variable queue may or not may impact me or not may be relevant for the problem so might be a feature that is a lot of predictive power or not is in influential. I can write somewhere else here just to remember. So, the queue is a capacity sees a capacity that queue is any additional variable. We can be of something or can be POC for example proxy. We use as a graphical notation where each attribute is represented as a circle. It's gray if it is used is employed in the model F or white otherwise whether is not is not used but it might it is correlated connected to a certain other variable. So, there is a dependence a correlation between, let's say a proxy of a capacity and, of course, its capacity and a protected attributes. So, this is a right in this way. This means that a, let's say gender is not tracked, but we know that for some reason is connected dependent or correlated to a proxy for, for example, intelligent quotient of our capacity that again we are not able to track and we are using a proxy of it. Both variables have are dependent are connected with P or C, which in terms is connected to the target variable. This is how to read this, this graph. So, the first type of bias that we mentioned are the bias historical bias that are biases that are introduced from the potential space to the construct space. So, there are all the inequalities, these proportions that already exist in the world that have somehow have an impact on, can have an impact on a certain phenomenon. For instance, we know that 95% of fortune 500 CEO are men or the large majority of managers chief CEO are men. And this is something that's impact the capability of a woman to be in the top positions of a company. And if you are trying to predict the probability that a person will will be a CEO within five years, let's say, we should take into account this fact is a well known fact that we also covered in our overview of the most prominent and most relevant inequalities. And the average income between men and women, this is also a factor that is quite persistent in the larger part of the world of the most advanced economies, and it is as is a type of historical bias that should be taken into account. Or as it is written, and that's whenever either the target or a relevant capacity, where capacity remember me means in terms of resource something that you can, you can employ for making something is dependent on the protected attributes on a protected attributes. And so you can have the fact that a given capacity that you are able to track is in this case is dependent on the protected attribute if and the capacity is correlated to the target variable. So you will have a bias and you will have that the target variable and all predictions are so dependent on the protected attribute. And so you will have, as I said, that other types of situation like protected attribute directly connected to the target variable or end the capacity as well. And so the situation like this where protected attributes is connected both to the capacity and to the target variable. And the same, and also the capacity is connected to the target variable will make a practical examples in a few minutes. These are all situations in which I started by us can occur I can have an impact on your or your prediction classification algorithm. So moving one step forward to the construct space from the construct space to the service space. So when we collect data, we have a bias that is very, very similar to the historical bias. And it is the measurement bias. But it is the difference is that it is not involved in the phenomenon itself in the structure and quality but is rather connected to the measurement process. For instance, you select features that implies some dependence on protected attributes. And all the types of subjective, subjective choices that w 

- * - *  - * - * - *  - * END CHUNK 2 - * - *  - * - * - *  - * - * - *  - *



Chunk #3    1065 tokens
-------------------------

3.

e have identified in the lecture on the measurement process. In all these steps, you might inject some measurement bias. It occurs also when you have a quality of data that might vary across specific groups. And let me make some examples and then I will go back to the formalization. For instance, reporting violent street crimes. And reporting on the streets on the roads is usually much more accurate as an information than minor crimes. Or financial crimes. And we know that the first type of crimes street crimes are also connected to to socio economic conditions. And that is the traffic data. Or traffic data that might be more precise, more detailed in your binaries rather than in rural areas. Or the former mentioned intelligence intelligence quotient that is often correlated to socio economic status. Or opportunities that you had as a child as a boy in going to the best schools, for instance, of the city or the nations. And it is might not well represent intelligence as as we said earlier. So these are our examples of measurement bias as my is that can occur when you apply a given measurement process. From the theory point of view, it can occur in one in one of these situations. When you are using a proxy of some capacity. That is relevant for the prediction attend and the proxy itself is relevant is dependent on some sensitive characteristics. So you are using a proxy. For a given capacity relevant to the target and the proxy is connected to the protected attributes. Or you don't you cannot access well, the your prediction, the construct of your prediction you use a proxy. Which is a weather that is connected to a protected attributes. And this is another situation. And here you have represented other possible situations where you have a proxy of a protected attribute that is directly connected to the target variable. And then then discrimination cannot occur because you're using a proxy of a protected attributes. If you're using zip code, this is a well common proxy for such economic status. Many times we have seen the map of Torino. The map of was just an example of different of incomes in different administrative units. So if for some reason you are using the code of the proscription of the administrative unit that you know that is correlated with such economic status. Which directly with a protected attribute or with a proxy of a protected attribute so combinations are there are more than these that are represented. But these are exemplar possibilities. That's show you when measurement bias cannot call. We will now make two examples. Short examples to show you the difference between historical bias and measurement bias. I make these short focus because these are the two most common biases occurring in discrimination. In algorithms. For example, first, first case, a college admission a university uses machine learning program to determine which candidates are more suitable for a given degree program. So to enroll, let's say to the master degree in computer science or the bachelor degree in in civil engineering. And say it's such scores that are scores of the. That's all in the states like a United States or UK, Australian song. These are scores test that are obligatory to make in order to then to enroll to a given university. If I'm not wrong, you make them at the last year of the high school if I'm not wrong. And they are a variable that are using the wall in the in the model and they are relevant variable means that this variable has a predicted a high predictive power. And make we can analyze the sources of biases in true with two hypotheses. The first hypothesis is that we assume that such scores as a good representation of the skills and the competencies of of a student. In Italy, the invals test, the results of the investee test can be considered as a good indicator, a real indicator of what are your skills and competencies. This is the first scenario. The second scenario is when you can you don't consider the score as a good as the major of your skills and competencies. So, so, so such scores notes, they are not a representation of the applicants skills competencies. It means that, sorry, such scores are are a good representation, the real representation or your skills and competencies. It means that the such scores are your capacity. And you can you use it to make some prediction together with other variables, any other variable that might be relevant or not. So the target variable is a function of the competencies the skills given measured by the such score and some other variable. If you have a discrimination, then the discrimination of course because the capacity of the such score is directly connected to the protected attribute. So if there is a discrimination, it occurs mostly because of that, given that other variables are proven to be not independent from the protected attribute. Otherwise, you have a different situation in which the such scores, they are a proxy, as you can see here of a given competence. They are not so the competence competence is the construct the skills are the construct and you're using a proxy for them. So this proxy. Then is the one that is relevant for the, for the prediction together with some other variables. In this case, you have that if you ever when you have a discrimination, you have it not because of historical bias but because of a measurement bias because it occurs on the pr 

- * - *  - * - * - *  - * END CHUNK 3 - * - *  - * - * - *  - * - * - *  - *



Chunk #4    1079 tokens
-------------------------

4.

oxy. The relation is with the proxy, not with the competence. If we have a situation in which some countries, we know that let's take it because just because it is a well known case of Afghanistan. You know that is very difficult to have even impossible in certain in certain periods of the history of this country for females to go to very high levels of instruction education. So you know that there is an historical bias a structural inequality that. So you are in this case that's from the potential space to the construct space that's for bees that do not enables female to to go to the highest level of education. So you will have as a consequence then a data set that is highly in balance. In any case the discrimination the explanation of the discrimination is in the historical in the historical bias. But this is an example a practical example of the first scenario. Another example for the second scenario is when I imagine you have a let's say that you have a situation in which the the measure that you have chosen. Let's change for a while the context number of works works. We know that because of maternity because of maternity leave because of oblique obligations cares care. Usually women work less or slightly less amount of time on average of course on average. This is if you use that measure to is a measure of experience. So you are using a proxy for experience. The proxy the number amount of time worked the number of hours worked might be highly correlated to gender because of the reasons just said because especially in a given age range. Children have more care obligations children for example elderly or they have maternity leave or they do not work for a for some years to take care of children. This is how to say a well known fact in even in most advanced companies economies and this is an example of measurement bias when you have this situation. Questions I hope that is clear now. If not we have a second. For example, well a third one. A second is long applications. A bank uses a machine learning model to determine credit worthiness. What is your reliability when you ask some money to borrow some money. And then the next question is whether a loan should be approved or not. In the past, these work was done mostly with decision was done mostly by a human person that was trying to understand the person in front of him or trying to understand the person on him or via third parties and to get a qualitative let's say idea of here him her her his credit worthiness. This process is mostly automated in banks. So the decisions to give you a loan or not. In many times is given by a set of criteria in the most in the banks that are most technological advanced, you might have some predictive optimization. You might have just a set of criteria requirements that are flagged and then a simple decision model that decides whether you can, you can get the loan or not. Practical example in Italy. If you don't have a limited time contract work contract. Most of the times, you cannot get more sketch so you cannot borrow money to get to buy a house. This is an example of criteria that is then used in their decision model is well known criteria that decision model is a black box, but this is this criteria is quite well known. Unfortunately, in many of the automations that are that have been analyzed in several studies one. One variable that was often used there was repair repayment rates. The weather. You have a good rate of successful repayments whether you repay. In time. So how many time how many times you you you you was not you were not late in paying back money. It's possible the discrimination here cannot call with respect to gender. We assume that there is a discrimination with respect to gender. Again, we have two scenarios. The one that is when you get an historical bias on why, where why is the watch you want to predict the credit worthiness. So you have some structural reason. You have some structural inequality for which women cannot repay on time. Or you can have a measure and bias on why if you assume that and you can explain also that repayment rates is not a good measure of credit worthiness is a proxy for credit worthiness, but it is not represent well the construct the construct is the credit worthiness. So if you assume that the repayment rates is the variable that correctly measure the credit worthiness that means construct space and observed space are basically the same. So you are really measuring credit worthiness. Then the bias is key the discrimination is explained by a structural discrimination. For, for example, the income disparities between women and women. And we commented for quite a good amount of time the gender gap. Last time. So this is a well known inequalities. And if you assume that the repayment rate is the measure of is the real the measure credit worthiness then when at the discrimination of course it's because of the gender gap, because you your income is lower as a women. And so you might find probably more difficulties in paying back your debt. So we can have the target variable is a function of some capacity, some other variable and protected attributes. Please notice that CNQ are independent from a is directly the target variable that is dependent correlated with a because of the structure discrimination. You could also assume that you could also reasons in terms of measurement bias. And as I said, y 

- * - *  - * - * - *  - * END CHUNK 4 - * - *  - * - * - *  - * - * - *  - *



Chunk #5    1038 tokens
-------------------------

5.

ou're not using the real credit worthiness. You're using a proxy of it. And the proxy is correlated. So capability to repay is a pop up proxy of credit worthiness. And this correlated the proxy with the protected attributes. The result is the same. You will get a discrimination explanation can be different. So you can assume whether you think that the variable used is a good measure or not. There are some cases in which in which it is quite obvious whether it is an historical bias or a measurement bias. There are some cases in which the two things can be accepted if correctly explained. Okay, we now move on with the different types of bias. The following biases are biases that are important that occur less frequently than the previous ones. Representation bias where that are not representative of the actual population is very close to measurement bias. And so when you get you have a course when you have a sampling method that does not reach well all the population. You know that the usage of the smartphone is very different between young people and old people on average. So if you use any measure related to the usage of smartphone you might get a different very different representation of your population or usage of social networks. If you want to predict something with data from social networks you might not have a total data from a portion of the population. But you might have also representation bias when you have changes in the population that are not detected. The presentation bias explains alone is sufficient to create a discrimination. You might have a total because of the dimension and the mechanism you might have that the target variable is directly connected to a protected attribute or via a certain capacity or some other variable in turn connected to a protected attribute. This is how you represent it. You don't have of course to learn by heart by heart the graphical notations the graphical notations are proposed in order to help your study and the differences between the different biases. I already made some examples I'm going to read the following training with images taken from specific geographic locations. It's obvious that if I use images only from people from East Asia, the argument won't work well with African faces. Also this is quite was observed in many algorithms that the behavior of the people totally changed during the pandemic. Most of the recommendations advertising recommendation algorithms didn't work during the pandemic because our habits were completely disrupted. And so prediction models were not working well. You might have also the case in which your data does not contain a very, very important variable that is really relevant for the prediction that really explains the prediction. So you might have a situation in which a certain capacity for example is not tracked at all. And as but it has some effects on on the target variable. And this was the last type of bias from data to algorithm. So for example, let's see quickly the last types of bias that are the ones from the decision space to the potential space. Or I omitted it you might also have an arrow here that goes to the construct space. You apply or either directly the classification or the prediction or your algorithm, or you you apply it. It's indirectly taking into account other other variables. Okay, the first is the deployment by bias. It seems to me happens when, for example, when you're you use your algorithm in a way that changes dramatically the behavior of people is a deployment bias, or an algorithm that's yes that have important consequences on on people and we have made the example of predicting a policy. We don't have three types of sub bias that are called algorithm bias that can be aggregation or learning and learning bias and evaluation bias aggregation learning bias are very similar. And of course, when you you're your application directly, exactly, directly have a disparate have a direct disparate impact on protected attributes. So you find the definitions when aggregation bias when sub groups are so different that you should have used different models. For example, you cannot get you cannot use the same model for skin cancer prediction on a population of white skin and on a population of black skin, the model should be different because the skins are very are very different. Very similarly, you have learning bias is when you have some, some design choices of your of your algorithm that they do not fit well specific sub groups. For example, as well I already told you about the predictive skin cancer algorithms, or any other clinical tools that might be should be tailored to different sub groups of populations slants is lengths in social network. And you should be used by specific groups of people. So you should train a model or fine tuning a model with specific sub languages or slangs. And finally, an example on review rates that when you review a restaurant or a product about if you see the previous reviews, you might be somehow somehow impacted. So you should treat the reviews separately in terms of time ranges, for instance, or. Okay, this is, I think that evaluation bias is something that you already seen in a technical courses on neural networks or machine learning when you, well, you train your model, you evaluate it, you test it. And then you use a totally different context. And this is of course a bias that you int 

- * - *  - * - * - *  - * END CHUNK 5 - * - *  - * - * - *  - * - * - *  - *



Chunk #6    1054 tokens
-------------------------

6.

roduce because it might not work and might impact directly with the decisions of the, the persons. This was last type of bias. Yes, model trained with purchases record on America's supermarkets and then evaluated with data from African supermarkets are an example. These are the most operational biases biases are many, many more. We take into consideration only these few biases and especially the first two that I described you historical and measurement bias. And doing some exercises you should be able to recognize them. I think quite easily. And just for your own curiosity. The recent standard of the NIST NIST is the national US national organization for technical standardization. Recently released a standard for the identifying and managing biases in artificial intelligence. It listed a lot of bias, computational statistical biases, human biases with we, I mentioned a new machine bias when you, you trust the decision on a machine more than the decision of a man or a person, just because it comes from a human bias and then systemic bias, including historical bias that we have already commented. We now make a 15 minutes break. And we will resume the lecture with the first statistical formalizations of algorithmic fairness. Okay. So we will move on and we will analyze different criteria for formalizing quantifying algorithmic fairness. So that means to be to have an impact that is the same on different people on different groups of people. And from now on when I say groups of people, I will mostly refer to groups of people that share the same protected attributes. So we want the aim is to get a measure of the impact of a classification prediction system so that different groups of people are not treated are not impacted. Sorry, differently. This is the goal of computing algorithmic fairness. There are several several metrics that can be used to access to assess and evaluate the impact of an algorithm. We use only three of them that the same that you will find in the fair mail book. But just, it is important that you know that the metrics are many, many more. A recent study in the literature identified the more than 7070 proposals of measures. We use we will see that the most important ones, the rationale as I say that beyond the statistical formalization of fairness, it to have is to have a quantification of the impact of the classification of different social groups. And if you remember the very first lecture in which I introduced the different types of of ethics. This is connected to consequentialism. That is, what is the consequence of applying a predictive optimization. And fairness is one of the possible approaches that belong to the two consequentialism. We will see two others examples, one for each of the two other types of ethics that we have in which we that we have defined that is virtue ethics and the ontology. Now let's focus only on on consequentialism and on the computation of a great difference. One of the very first, the side characteristics of an algorithm in order to be fair is that it classifies given social groups in the same way. For example, an algorithm that should select curricula should give we might want that the algorithm classifies as a good curricula in the same way, may be in females, for instance, or the probability of predicting a good curricula should be the same for whites for black, etc. for any group. So given a protected attributes, we might want to require that an algorithm has the same, the same probability of giving either an opportunity in the case of assigning an opportunity in the case of job application, or giving a penalty, a risk score is a potential penalty. And this first one criteria is called independence. Then we might require is we might go farther and we might require that our algorithms. Not only make give assigned good predictions and well, not good predictions terms that assign the same level of positive predictions to all social groups, but that the predictions are right. Are correct. For the same, at the same rate for any instance of the social groups or the same rate for mates and the same rate for female, for example. So we might require that the errors made by the algorithms are the same for any instance of the social group. So if the algorithm makes classification errors, shouldn't make more errors for male and less errors for female. I'll share with you the fact that face recognition algorithms have a very bad performances with black women faces and this is an example of different error rates for different social groups. Black women is a particular case of social group is an intersectional attribute that is the insert the intersection of two product attributes, but that's not attributes of ethnic group and the product attribute of gender. The second criteria is called separation. And so, in the first one, we require more than independence independence is just that we want to say our class if positive classification to be the same. Here, we are we go and check the quality of the classifications. The big difference between independence and separation. And third, we want that our algorithm is well calibrated. The thing is a concept that you already seen in other courses means that for for a given level of the classification. So if the class if the classification is positive we expect a higher presence of actually positive instances, then in the group of negative classific 

- * - *  - * - * - *  - * END CHUNK 6 - * - *  - * - * - *  - * - * - *  - *



Chunk #7    1064 tokens
-------------------------

7.

ations. And we will now, we will now analyze together the exact statistical formalizations. The only criteria that we will take into account are these three. The possible cases of the exam might require you to compute these one of these criteria and to explain them so to check whether there is a discrimination occurring and explain it, for example, or to choose one algorithm or the other one. And with respect to a given criteria of fairness of your choice. So you choose a criteria you explain why you choose it, and then you say, okay, I get one is preferable instead of algorithm two. You will find additional information on these three fairness criteria on the chapter classification of the book of the usual book fairness and machine learning, and each of the criteria is fully explained. Most of the slides come from the book. The examples are mine, but I reflected I respected all the structure of the chapter of the book. We have ready met this formalization that is the same for for the previous part of the sources of bias or we'll go faster here. And we already also repeated what are the protected attributes. Let me go. Okay. You will know, you know, well from other courses that you can compute several measure of performances of your classifier. And we will use the notation coming from the conditional probabilities the bias theorem, we use a lot of bias theorem, and the formal notation is the following one the probability of our classification given a condition. This is our abstraction of the problem. The bias theorem and we know that the classification given a condition is the probability of the classification and the core of the condition together. So given classification, let's say the positive classification, divided by the probability of a given condition that's now move on with the formalization. This is these are errors that I'm sure that you will you know well, whenever you have a classification positive. We use, by the way, we use only binary cases for the sake of simplicity. All our, all our analysis will be on a binary classification 01. And concepts apply also with multiple levels of the classification. So whenever you have a classification that is one positive and the condition that is positive so the target variable is actually true. You have the right inference the true positives. The extreme of the table when the classification is negative and the condition was negative, then you have a true negative is always the right inference, where are the errors are in the middle. When you have either a classification that was negative and the condition positive a false negative and the classification that was positive and the condition instead was negative and this is the false positive case. So far, so good. I think nothing new for you. As I said, we have a free furnace criteria. The first one is independence. Independence requires that the classification is independent from the protected attributes. And I want that the same. The same opportunities are given to, for example, male and females, or the same possible risk of penalties are given to any social group. The classification is the one that you see at the bottom and the statistical requirement is that the probability of our classification to be one given that a the protected attribute is equal to small a to a given value is equal to the probability of a positive and are equal to one given a equal to small b. So now the other value of the protected attributes, as I said, we stay into the binary case for the sake of simplicity. So the classification is binary and the target variable and sorry the target variable is also binary and the protected attribute as only two levels. So we have computations at multiple levels of the protected attributes. So here we might have. Let me. Continue the question and get that the classification, the probability of classifying are equal to one given that a protected attribute. Is equal to see. And so on for every, every level of a. But this is a case where you can have a so I'm white Caucasian African American Asian, etc. ethnic group is usually not on a binary level. Even gender is not at a binary level. Because we can have a multiple construct levels of gender gender, however, is used a lot in historical data sets with only two levels so we will stick to this conventional representation of gender, knowing well that it has a limitation. That one thing is the biological gender and that is different from the more generic gender. It is a different name that now I don't remember. When you might relax a little bit the constraint you might have that you might introduce an epsilon tolerance. It is expressed in this way if you have the same probabilities of R equal to one for the two levels of of the protected attributes. And show you can say that it should be higher or equal to one minus a given epsilon. This is the relaxation of independence. Or you might express the same thing introducing here plus minus epsilon is the same thing. So you require that the two terms, the two probabilities are the same with a given tolerance to percent five percent depends on the context depends on the decision maker or on the legal obligate on the legal obligation. You might have a legal requirement that's as cute to have same probabilities. Plus or less 5%. Here an example of independence not respected. I wanted to make step by step the computation so that you 

- * - *  - * - * - *  - * END CHUNK 7 - * - *  - * - * - *  - * - * - *  - *



Chunk #8    1117 tokens
-------------------------

8.

 know well how they should be made and you feel confident with the tables that you might get at the exam. Here an example of independence. Let's assume that we have the following situation. This example will be an example of independence not respected. We have our cases that is our binary levels of gender. For female. We might we have two cases classification equal to one. Yes, you can say okay or classification equal to zero for females and for males. The three symbols are the correctly classified elements. People the wrong classifications are read the dark red. In this case we have that five out of six positively classified females are correct because we have a five correct classifications and one not correct classification. This is a verse when we have four instances one two three four of female classified negatively so not predicted as suitable for a job application for example two of them were correct and two of them were wrong. And then we assume and then we can build from from these graphical elements the confusion matrix. That is the number of female women classified with are equal to one is six. One, two, three, four, five, six. The number of female classified negatively is four and me male is seven three. We are not remember that independence the first one does not take into account to the quality of the classification so we don't track for independence, the errors. This is a limitation of independence. But it is quite also largely used criteria is the very first criteria that you can put in place for a requirement requiring a fair algorithm classified with the positive instance in the same way, all the social or the social groups. So what is the probability independence requires that the probability of classifying positively females be the same of that of of maze we compute the probability of R equal to one given that a is equal to F as the joint probability of R equal to one and a equal to F six. Six over 20. And the only the probability of a equal to F. There are 10 female in the whole database. We are using the bias theorem. 20 and 20 goes away and six divided by 10 is 060. Are these steps computation steps clear. I am aware of the time that it is required to give to make some even even some simple computation so usually at exam I ask only for a given fairness criteria or I ask you to select the fairness criteria. It is very, very important that you don't show me only the result that you show me at least for a computation the steps. Why, because if you make only a computation error. I don't count it. But if you make a conceptual error, I will count it. And I cannot I cannot know whether you made just a computation error or a conceptual error if I see only the final results. So 060 show me that you understood the concept. Because the denominator will be the same in this formula, you can, as a rule of thumb, you can take whenever you have a table like this, you take this cell, this cell. This is the practical derivation of making this difference. And the same is for males. So, well, seven divided by 10. This is a example of independence that is not respected because there is a difference of 10%. Unless I require I give you a tolerance percentage. I cannot assume that independence independence is a is respected with a 10% difference. 10% might be a lot too many, many domains. So unless I tell you in the exam. I give you a variation of up to 10%. Please consider such a percentage of as in this example not respected. So let's try it forward. An example of independence respected is the following one you have six classifications for female and six classification for men. It's quite simple. So six divided by 10 60%. Six divided by 10 is 60%. This is respected. And you might observe, thanks to the graphical notation that here we have a lot of wrongly classified people. But independence does not take into account the quality of the classification. Sorry, if I repeat it, but this is something that's often I see as is an error in the exams. And here we have actually much less but still it's 50%. Here is even more is five over six might be around the 90%. Let's ask you, take independence select which algorithm is preferable. And you will choose, of course, the one with that is with lowest differences in terms of independence, and then I might ask you, which limitation you still observe in the algorithm that you selected as most preferable. The possible limitation could be this one that even though if algorithm, let's say one has the lowest difference between probability of R equal one for female and males but still expose very higher rates. This is a common limitation that you might find. Even when you select an algorithm as preferable. You will find a competition that should be quite straightforward. This course, if you go back to the official sheet speaks only about risk, quantifying risk, identifying also qualitatively risk of an over discrimination risk for algorithms impact of an algorithm does not require you to mitigate the risk. To improve, at least not from the operation point of view that the the algorithm is not always possible. You might have some mitigation strategies. Just for the sake of your curiosity or whenever in your future you will go back to these slides because they might be useful for your work. You might achieve independence at any level is very simple as a as a criteria so you might see the independenc 

- * - *  - * - * - *  - * END CHUNK 8 - * - *  - * - * - *  - * - * - *  - *



Chunk #9    1082 tokens
-------------------------

9.

e at preprocessing you adjust your just sorry to change the future space. Optimize it let's say with respect to the correlation with sensitive attributes with protected attributes. You might go also training time. So requiring going directly into the optimization process optimization function or post processing. You just adjust directly the outcomes the classifications. Putting a further threshold for instance. It is not the goal of this course but just mention as the book does possible ways of achieving independence. As I told you, it's very simple. This is an advantage can be applied at every stage of the process but it forgets the quality of the of the classifications. That means ignore the possible correlation between why and a this is the meaning also of another way of expressing that the quality of the classification with respect to the levels of the protected attribute is not taken into account. If you want to take care of the quality of the predictions or qualifications we need to take into account separation or any other similar criteria. The paradesion requires that the given a certain level of the target attributes. The classification is independent on the protected attributes. The T is the probability of our to be equal to one given that why is equal to one and the forest specific level of the protected attributes should be the same as the probability of our equal to one given the why is equal to one target variable is equal to one and given the second level of the protected attributes. As before, here we might have a plus me no so given tolerance. The separation criteria as two requirements. This was the first one. The second one is that the probability of our equal to one given that the target variable is equal to zero should be the same for both levels of the protected attributes. Again, plus or minus. Tolerance level. If you look carefully to this statistical formalization you will notice that these is the true positive rate. R equal to one given that why the target variables equal to one is the true positive rate. R equal to one given them why was equal to zero. This is the false positive rate. I remember you the table that I showed you at the very beginning and well here I forgot to update to our slide. So these are the two positives and as I said these are the false positives. This requires that you correctly predict the positive instances at the same rate. You will notice that there are more to male and females and that you make errors in terms of false positives. At the same rate, they both need to be respected otherwise you don't achieve separation. We have many, we have other types of errors, the false negatives. Please do not place into your computation false negatives when computing separation. This is an error. Separation requires only true positive equalization or true positive and false positives. In the last example I saw computations based on false negatives. This is wrong. It could be reasonable if I ask you further limitation once you computed separation which further limitation you still observe then you could compute the false negatives and say okay there is a high rate of false negatives as a further limitation but not as a requirement of separation. Again, one example of separation not respected one example of separation partially respected and then one example of separation respected that just to show you the steps of the computation that are should be quite easy but to have confidence with the type of data that you will find these are the full confusion matrix in which you have also information on the target variable. The graphical notation is the same. But this time we take track also of the levels of the positive instances so the code of the positive instances correctly classified positive instances not correctly classified and so on. Negative instances correctly classified negative instances wrongly classified. So, in the case of looking at this picture in the table, the result is that the probability of R equal to one given that Y is equal to one will be needs to be checked in this portion of the table. So, of R equal to one given the Y is equal to one is free this cell. So, it's this part free divided by 20. The main at the denominator is the condition. Y equal to one and a equal to F is different from independence. This is a five. Because why is the total number of why equal to one. So these ones. Plus these ones. Yeah, why was equal to one here is why equal to one in the table is located here. Be aware that this is a standard shape of the table. So, this is the first where are is on the top and why is on the bottom. And if you choose to compute separation you need to perform some some translation. This is 60% free divided by five and for males instead is 33% is one over three. So, these first requirement of separation is not respected. Even independence is not respected here. Because independence is different. Well, no, it's the same is three plus three divided six and six. I'm wrong. It's the same. The other requirement is that the target variable is it was equal to zero, but the classification was equal to one so it is a false positive. The computation is free. So one given at why is zero is free is this free. The buy by five. Because one, two, three, four, five the total population of female target variable zero. And here you find the formula. The male is seven, five divided by sev 

- * - *  - * - * - *  - * END CHUNK 9 - * - *  - * - * - *  - * - * - *  - *



Chunk #10    1119 tokens
-------------------------

10.

en, following the same steps. Partly respected when, you know, in such a case when we have this case we have two females correctly classified over the total number of actually positive instances. So, 50% and the same is for male is one divided by two. So two divided by four is 50% one divided by two is 50% but then you might have that the false positive rate is different. The five divided by six is 033 and the five divided by eight is 063. So we whenever one only one criteria is respected is is respected we say the separation is partially respect. Whenever you will get the solutions of the exams, I will not repeat again all these steps of computations to use the slides to understand the computation steps. These are provided on purpose so that you have a running examples. So we go quick separation is respected. In this case, 60% is three divided by six by five, both for female and males and post positive rate one divided by two and two divided by four. It's important that you also know how to build a table such that the separation criteria or the dependency criteria will be respected. This is also possible to ask how would you how should you change the table so that the criteria is respected. So you should be able to double check that you fully fully aware of the meaning of the other computations. And here you just have a reminder of the possible all the free examples with separation. Again, it's not the purpose of this course but you might achieve a separation is much more difficult to achieve separation. Then independence you need to do a doc training you need to optimize. You need to do a post elaboration. So you look at the rock curve and you need to check the intersection here of the two positive rate and the false positive rate, because only here separation, you know that is respected. So, of course, the difference. Of course, separation is more is much more strict requires also. And look at the quality of the other of the predictions, not all errors are taken into account the false negative rate is not taken into account and in certain applications. The false negative rate might be more important than the false positive rate depends on the on the context of application. So receiving an error as a false positive rate might be in influence for certain classifications. As I say, there's more much more difficult to achieve it. The last criteria is sufficiency that is equivalent to calibration sufficiency is the reverse of the separation so we put as an event. So we switch target variable and classification. So we switch y and r. So the probability requires the probability of y equal to one given a certain level of the classification is the same for all the levels of the productive attributes. So the classification criteria and the day whenever you have here are. So it can be let me more explicit with respect to the book formalization are can be in our simplified case, either one or zero is the two levels of the classifications. So we can make sure that we have four possible to two possible cases because we fix y equal to one as in the separation we fixed are equal to one here we fixed y equal to one so we look only at the positive actually real positive instances. So the condition is the positive predictive value and the second condition that is positive instances, but the classification given that was zero is the false omission rate. So the classification required by the sufficiency criteria is a equivalence of positive predictive value and the false omission rate in practice. Okay, you hear you find both the levels. That's already told you. We need to change our graphical notation and to get differently the organization of the color so we need whenever the our event is why equal to one we have the two condition given that are was equal to one, or given that are was equal to zero so we take this and we put here. So we see these are the same, but with respect to the previous graphical notation we take this and we put them here. Whenever the event is, but we're not interested but whenever the event to just to complete the table the event is why equal to zero. So it's these are equal to one as it's this are equal to one, are equal to zero. So this table is equal to zero. And the table then is reversed should the table, the specific table for sufficiency as on the top are. So just be careful which table you get this important. And I leave to you just is just a matter of placing the right cells in the right position very quickly how to compute sufficiency. It is very similar to is basically the same switching, but switching why and are with respect to separation probability of the target variable equal to one given that the total classification was one for females. So you put all the conditions and you get you track whenever all the conditions are satisfied so is this cell is free divide by six, the totality of positive classification are equal to one. Two, three, four, five, six, 50%. The same for male. We have only one male that was, it was classified one in the group of actually positive men. The total number is six article to one because here we have a lot of errors. Yeah, so, so one and then two, three, four, five and six, and you get these six. One step after the other because if you just get the graphical notation, I'm sure that you have at least some examples and you are able to to answer without in a very simple way witho 

- * - *  - * - * - *  - * END CHUNK 10 - * - *  - * - * - *  - * - * - *  - *



Chunk #11    1152 tokens
-------------------------

11.

ut building the table. Very similarly is when the classification is are equal to zeros you switch the part or you go to the second part of the table. And this is true divided by four. Why is equal to one the classification is are equal to zero so one and the two. And then you take the totality of classified are equal to zero so for three and four. So, 050 050 also for males. Any question. I'm aware that this might be a bit boring but it's important to me to show you a fully computed example step by step because unfortunately at the exams. I still see many errors conceptual errors not computation errors. So I prefer to show every single steps with you. So a bit quicker here an example of a sufficiency that is not respected 029 and 020 and 017 for the first part of the question and very different for the second part of the question for the question. Here's the third example that I leave to you. And what here you find the summary of the three examples. And you find the explanation of the equivalence between sufficiency and calibration and don't require you to explain why just you need to know that if I say that the classifier is calibrated means that sufficiency is respected. This is the takeaway or this light. The sample of very bad clarity class calibration is here. So this is the case. So this algorithm is not well calibrated by ethnic group or at least by race, considering only white and black because here we have multiple levels of the classification from one to 10. We have only the binary case. And here we had the rate of positive outcomes. So these are the white the green line and the black is quite different for in this area. This is a calibration plot. So for every level of the classification. So this is the rate of positive outcomes. This is the sufficiency. And should they should be the same. Ideally, they should be the, the dotted line and should be the same. There is a portion of the calibration proton which the lines diverge. Instead, the same data set this taken from the book and it is even also openly available is quite good calibrated by gender. And this is a plot of good calibration. So where the two lines are almost overlapping. So we just have critical values are here and here. Okay, this concludes the, the part on statistical formalizations of fairness will make starting from next Tuesday will make some practical examples taken from the past exams. I will now show you how this free criteria perform on the compass case. Before I go ahead, I make a poll. Do you prefer I go ahead. And as usual, I, I finish a bit earlier. I prefer to take five minutes of fresh air, 510 minutes and then we finish at seven sharp up to you. So votes for going ahead now and finishing as usual a bit earlier. Okay, democracy works like this and I will continue. Of course, as usual, I'm here for any question at the end of the lecture. I won't stay here if nobody stays. This is. Okay, give me the time to take the other presentation, then I will then upload. Okay. Okay. I will wait a second because I also to open a link. Okay. Let's start with a little game. I will just track the URL because he's in the slides. And I will upload the slides either after the lecture or tomorrow. You get an article of a few years ago that explains the problem of the compass case. And that I will quickly review with you just to remind what was the compass case and to move a step forward toward the, the, the, the impact of compass according to the fairness criteria. I will not read the article I will just guide you through the graphical representations that simplifies the problem. This is a article by the mid technology review. This is the, the website of MIT. They represented each individual as a dot. So each individual each person is, is, is a dot. So the risk score in compass was from one to 10. So we could show the distribution of people where classified according to the, to the different risk score. And you might get, you might get that there is a much more a higher frequency of people. Can you still hear me? Yes. You get a higher frequency of people with very low scores. And the distribution of high scores is, is made by less, less people roughly. Okay. And this is the first fact. Then one thing that the, the journal researchers and journalists let us reflect is on the threshold, which threshold to use. And this is something that is in this, this part of the process when we make a decision based on the classification. When we use the classification with some other, some other criteria. You can also think to the measurement process that I show you. And the very last step is the interpretation of a given a number that is the output of the algorithm. So which threshold to use in compass the threshold for high risk is eight. That means that whatever is on the right should be jailed, whatever is on the left should be released in simple terms, but we know that compass stays for alternative sanctions. So released with some sanction under payment or some free hours, etc. As a professional, you should not just give the classifications without reflecting on the distribution that you have on the right. If you use such a threshold, you have a huge number of people that are going to be jailed. Okay, your ugly says out gives a given out but then you should, you should reflect on the impact of your classification algorithm on the population because if you put to jail. 90 

- * - *  - * - * - *  - * END CHUNK 11 - * - *  - * - * - *  - * - * - *  - *



Chunk #12    1155 tokens
-------------------------

12.

% of people is not a good idea, perhaps. We might have a totally different population that is cured towards the right. And then we should at least raise the warning that we are putting theoretically into prison will live to into into the jail. So this is the very first consideration to be made after analyzing the distributions of your of your classification. Then the situation is a bit becomes a bit more complicated when you start to check the quality of your classification. The quality of your classification might be computed in simple terms in terms of accuracy. So you might track the people who are re arrested. And the people who are not arrested. If they are re arrested, they are the true positive, because it's a, the score the risk score is a residue, the recidivism risk score. If it's not re arrested is a probably a false positive, mostly a false positive. So, if you want to achieve 100% accuracy, at least in your after the evaluation of your algorithm. You need to move back the thresholds. You have a requirement or you want to show to the to the situations that ask you to test a predictive algorithm that your algorithm has 100% accuracy you needed to move back the thresholds. And these again another consideration so that you don't have a horse anymore. So, you might be in the real situation but actually reality is as much different. The reality is that, well, you don't have perfect calibration and you don't have perfect separation. So, by putting the threshold on on a like the case of compass, you might get you get a accuracy of about 63%. Because you might you have a mix. If you look at the people that are classified with an eight, you will have a certain proportion of people that was actually re arrested so was correctly predicted as risk individuals and then a portion of people that was not really rested. If you have a good calibration. You should have a situation where the majority here is of full dots and the minority of white dots and the proportion of white dots should decrease when the risk or increases because is well calibrated. And the last part that you have very few not to be arrested. Well, things get more complicated because if your requirement is that you need to optimize accuracy, then you need to find a different threshold. Well, this is a game where you should empirically try to understand the worries that the highest threshold and 65 years 69 here 66 and then accuracy decreases you see. So 69 was here. So if your goal is a data scientist is to deliver a model that optimize accuracy you might select a threshold. So this is a six is this a meaning a meaningful threshold. It is right in the middle. Well, a bit above the middle, but you're saying that whatever is more or equal to six needs to be to jail. This is quite a strong a session that you make with your algorithm. So if your your boss says, you need to optimize accuracy situation is this. You have the responsibility to make this kind of reflections. And also to your stakeholders. Then let's go ahead the still ahead. And because situation is even more complex is if we look at the released the but then re arrested. These are the false. These are the false positives or the needless jailed. These are the false positives so the two types of errors. We're looking at the same distribution if we put the threshold that's used was used in compass by a parent. We need that we have a percentage of 60 67% of the false negative and 11% of false positive. What do you prefer. difficult to say. Well, since 67 is too much as an error. So we need to decrease this percentage. Or you might say, no, I want to optimize this. I want to be to pursue a criteria of caution caution. So I don't want to. I don't want to put in jail people who is in innocent. So I prefer to have a higher rate of false negative the important thing is that this percentage is as low as possible. If this is the situation, then you have to find the lowest possible percentage is zero, but it's not meaningful. The lowest, the lowest for supposing rate is here. So you move forward that the threshold of risk. If instead you don't want criminals in society, you say, no, this is too dangerous. You cannot run the risk of having people who have a high very high probability of making a crime. So I want to optimize this percentage. So we go back to our dilemma that was said earlier. So you should be here not but you should be even here. If this is an optimization goal, or you may say I want to balance the situation. The situation is the one where the percentages are as close as possible. And this is the right level 35 and 34%. Wonderful. But is it correct. Is it responsible to release an algorithm with such a rates 35 and 34. This is also a concern to be raised. It's even more complicated than when you put inside the problem, the productive attributes. In this case, attention was made on white and black defendants. And you might just so you might have a two, you will have two types of distributions. The distribution for white defendants, the yellow one distribution for black defendants. And if we use the same threshold as compass, we have this situation, 81% of false negative for white 60% for black. In the sample that was retrieved for making this case. We know that the percentages are a bit very different but in this toy example, these are the same data. These are the percentages. False negative are quite low 

- * - *  - * - * - *  - * END CHUNK 12 - * - *  - * - * - *  - * - * - *  - *



Chunk #13    1147 tokens
-------------------------

13.

 and there is a difference of 9%. Could be okay or could be not depending on the requirements. We could do we could apply different thresholds in order for instance to get a better a better level equal level here. This is a good level where we have 65 and 60. I think is the best one. Let me check. Yes. This is the best we can achieve in terms of false negative. And I think let me check if this is the best we can achieve in terms of false positive so he is five. This is better. Only 3% of difference and lower levels of false positive. And I think that situation will only get worse. Yes. Okay. Well, these are these the level the absolute value of the errors is very, very bad. But we know let's say put his eyes that this is okay for the institution that will use the algorithm. And this is a very good percentage of false positive. Very, very similar black and white. And I think that we we finally found the solution. No, it's illegal to make different thresholds in the in case of classifications of with this kind of of of gold, the kind of gold of predicting such a social outcome, you cannot buy law this is justice is a protected and it's not a law of sensitive attributes, you cannot do direct discrimination that means you cannot apply different thresholds to different groups is illegal. And so, then you can, I think there is. Yes, and then you, you need to use a unique thresholds. And then trying to find that what you can achieve best this is a quite good difference in terms of false positive, but the false negative are really huge. You cannot actually trust negative classification at all with these such percentages. And worse if I put the threshold here, and here we can have better performances in terms of false negative, but very high performances in terms of false positives. This is a conundrum. This was the conundrum and the base of the debate the legal debate. Well, no, this was not legal debate but the debate between propublica who made the journalistic investigation and equivalent, the company who released the software, because the company said, hey, we calibrated perfectly the software. Like I said, separation was not respected. And this is one of the open challenges in the field of fairness in machine learning and artificial intelligence because whenever you choose a fairness criteria, and you optimize for that fairness criteria, you won't achieve any other fairness criteria or only a subset of the others. So if you want to optimize for for independence, you will get you will not respect separation. If you want to have a very good calibration, you will end up and this is mathematically proved not respecting either independence or separation or both of them. Let's look at the numbers. Should make. Wait. This. Yes. Okay, in the slides you find another game that you play can play at home, just if you wanted to get some simulation on dynamic simulation on the same problem. You connect to this URL and you play a little bit. You can play in this game with different thresholds or low risk, medium risk and high risk and see what happens to the false positive and the false negative. Just to get the fully awareness of the problem. The problem. Just recall some specific cases so discrimination for black people. And this is what we just did independence in compass was not satisfied. We got to at least in the data set was retrieved by propublica and was confirmed by the company. And the 58% of the probability of getting a positive classification given that a person was African America and and 33% for Caucasian. Some is not under percent because the whole computations take care, include also the other ethnic groups, but they are much less present in the database. Separation was not satisfied because the two positive rate for Caucasian was 50% while for African American was 72%. Post positive rate for Caucasian was instead 22% and much, much more for African American. Instead, so this was a disadvantage for African American advantage for Caucasian instead the sufficiency that this calibration was if relaxed was respected with a tolerance level of five, six percent predicate a positive predictive value of 59% for Caucasian American 6565% here there's an error is not a negative predictive value is fossil mission rate 29 and 35. We correct. And so the key facts written as I told you, propublica found the separation not not respected equivalent on North Point replied no we have a very good calibration. And we could go on on the reflections, because if you do the same analysis on age, we can find that among the recidivist. The percentage was 25 years or less. And then the majority was between 25 and 45. Among the low risk. Most of you was between 25 and 45 30% more than 45% among the high risk look 35% is 20 years old or less. And so we do when we have an argument like this. Should we just use the plane. In our decision the plane risk that is given by the agreement or should we consider also other protected attributes or other context variables like the age, because if you put in prison a person. At that age, what is his or life is ruined. And of course he needs to go to prison if he made a crime, but he or she has she shouldn't stay in prison while there is a pending judgment because an algorithm decided that is a her right score risk score is high. So there are a lot of reflections to be made. Once you have, you get your, your classification 

- * - *  - * - * - *  - * END CHUNK 13 - * - *  - * - * - *  - * - * - *  - *



Chunk #14    289 tokens
-------------------------

14.

s of predictions. So the good my final goal. Well, of course, is that you pass in a very excellent way the exam but is that in your career then you will start to reflect on the impact of your classification you will start to check your classifications predictions by different subset of protected attributes. And these are some examples that I put here for this call but also for the sake of the exam because I might ask you, given this percentages please reflect on on some social social and ethical issues related to such such predictions. I will say a couple of things and then I will finish because I need to keep my promise. I will finish next time. Consider that there were some studies in which that showed that the accuracy of compass was not so better than the accuracy of using human judges. There are some results were very similar and here is the link to the whole study you might you might reason whether you need a classification or or not. There was another study that proved that even using a simple linear classifier with only two features. The performances were actually you are better or at the same level of compass. So if you really need to, to use all the possible data from people, we will see next time which type of data was used in compass and we will comment it so we will, we will open criminal records and we will make a few consideration. So, nice evening and if there are any questions just come here. Thank you. 

- * - *  - * - * - *  - * END CHUNK 14 - * - *  - * - * - *  - * - * - *  - *



